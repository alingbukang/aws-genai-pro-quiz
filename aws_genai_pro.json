{
  "title": "AWS GenAI Pro - Practice Questions",
  "generated_at": "2026-02-03T23:06:34.737528",
  "models": {
    "generator": "gpt-4o",
    "critic": "gpt-4o"
  },
  "total_questions": 100,
  "statistics": {
    "generated": 100,
    "approved": 100,
    "rejected": 0,
    "needs_review": 0
  },
  "questions": [
    {
      "question": "A media analytics company is building a generative AI-powered video recommendation system using Amazon Bedrock. The system leverages proprietary foundation models (FMs) through Bedrock's API for generating personalized video summaries. The company has strict security and compliance requirements, including SOC 2 compliance and ensuring least privilege access for all resources. Developers need to invoke Bedrock APIs to generate recommendations, but only in the 'production' environment within the us-east-1 Region. Cost optimization is also critical, as the company has a fixed monthly budget for Bedrock API usage. Finally, logging for all API calls is mandatory for audit purposes. Which IAM policy configuration BEST meets these requirements?",
      "options": {
        "A": "Create an IAM policy that explicitly allows 'bedrock:InvokeModel' for only the 'us-east-1' Region. Use a Condition element to restrict requests to the 'production' environment by checking for 'aws:RequestTag/Environment' set to 'production'. Attach this policy to a dedicated IAM role assumed by the production service and enable AWS CloudTrail for API logging.",
        "B": "Create an IAM policy that allows 'bedrock:InvokeModel' for all Regions but restricts access to the 'production' environment using a Condition element that checks 'aws:RequestTag/Environment' set to 'production'. Attach this policy to an IAM group for developers and enable API logging via AWS CloudTrail.",
        "C": "Create an IAM policy that allows 'bedrock:*' actions for the 'us-east-1' Region. Use a Condition element to restrict requests to the 'production' environment by checking 'aws:ResourceTag/Environment' set to 'production'. Attach this policy to an IAM role assumed by the production service and enable CloudTrail logging.",
        "D": "Create an IAM policy that allows 'bedrock:InvokeModel' for the 'us-east-1' Region and restricts access to resources tagged with 'Environment: production'. Attach this policy to a dedicated IAM role for developers and enable API logging via AWS CloudTrail."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because it explicitly grants the least privilege by only allowing 'bedrock:InvokeModel' in the 'us-east-1' Region and uses a Condition element to validate that the 'Environment' tag is 'production'. It also restricts the policy to a dedicated IAM role and ensures compliance with logging requirements using CloudTrail. \n\nOption B fails because it grants 'bedrock:InvokeModel' permissions for all Regions, which violates the principle of least privilege and increases the security exposure unnecessarily. \n\nOption C is incorrect because it uses 'bedrock:*' actions, which are overly permissive, and it uses the 'aws:ResourceTag' condition key instead of 'aws:RequestTag', which does not properly restrict API calls based on request context. \n\nOption D fails because it does not use 'aws:RequestTag' in the Condition element, which is necessary to enforce the environment restriction during API invocation. Additionally, attaching the policy directly to developers increases the risk of accidental misuse.",
      "topic": "Bedrock IAM policies - least privilege patterns",
      "difficulty": "medium",
      "category": "bedrock",
      "aws_services": [
        "Amazon Bedrock",
        "AWS IAM",
        "AWS CloudTrail"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/bedrock/latest/userguide/security.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:32:24.066480",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 1
    },
    {
      "question": "A global financial services company is designing a customer support chatbot to handle inquiries in multiple languages, including English, Spanish, and Mandarin. The chatbot must be capable of providing real-time, contextually relevant responses while adhering to strict data security and compliance requirements, as mandated by GDPR and SOC 2. The company operates on a limited budget but anticipates high traffic (50,000+ requests per day) during peak hours. They need a solution that minimizes latency, optimizes costs, and ensures data is not stored outside their AWS region. Which solution BEST meets these requirements?",
      "options": {
        "A": "Use AWS Bedrock with the Claude Haiku model. Configure the `region` parameter to the desired AWS region. Enable request-level encryption using AWS KMS, and set up a VPC endpoint to ensure traffic does not traverse the public internet.",
        "B": "Use AWS Bedrock with the Claude Sonnet model. Configure the `region` parameter to the desired AWS region. Enable request-level encryption using AWS KMS, and use CloudFront to reduce latency for global users.",
        "C": "Use AWS Bedrock with the Claude Opus model. Configure the `region` parameter to the desired AWS region. Store user data in an S3 bucket with server-side encryption using KMS and enable cross-region replication for disaster recovery.",
        "D": "Use AWS Bedrock with the Claude Haiku model. Configure the `region` parameter to the desired AWS region. Use Amazon API Gateway with caching enabled to improve latency and reduce costs."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because the Claude Haiku model is optimized for low-cost, high-traffic use cases while meeting real-time performance requirements. Configuring the `region` parameter ensures compliance with data residency requirements, and using a VPC endpoint prevents data from traversing the public internet, aligning with GDPR and SOC 2 mandates. Option B fails because CloudFront introduces latency for real-time chatbot interactions and does not address data residency compliance. Option C fails because cross-region replication violates the requirement to ensure data remains within the AWS region. Option D fails because API Gateway caching is unsuitable for dynamic, real-time chatbot responses and does not address the need for secure, private communication channels.",
      "topic": "Model selection for cost - Claude Haiku vs Sonnet vs Opus use case mapping",
      "difficulty": "hard",
      "category": "bedrock",
      "aws_services": [
        "AWS Bedrock",
        "AWS KMS",
        "VPC Endpoint"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:32:43.593091",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 2
    },
    {
      "question": "A financial services company is building a document processing solution using Amazon Bedrock to extract and analyze insights from legal contracts and compliance documents. They are leveraging a foundation model for semantic chunking. The documents vary in size, with some exceeding 200MB, and contain highly sensitive personal and financial data. The company must ensure that the semantic chunking process is optimized for accuracy by maintaining a breakpoint threshold that prevents incomplete sentences from being split across chunks, while also minimizing latency. Additionally, they must control costs by avoiding excessive API processing fees. Buffer size must be tuned to ensure smooth processing without exceeding the Bedrock input limits of 1MB per API call. Compliance with data residency regulations requires that all data processing must occur within a specific AWS Region. Which solution BEST meets these requirements?",
      "options": {
        "A": "Configure the semantic chunking breakpoint threshold to 0.85 and set the buffer size to 900KB. Use Amazon Bedrock InvokeModel API with 'MaxTokens' set to 1024 and deploy the solution in the required AWS Region. Monitor costs using AWS Cost and Usage Reports.",
        "B": "Configure the semantic chunking breakpoint threshold to 0.95 and set the buffer size to 950KB. Use Amazon Bedrock InvokeModel API with 'MaxTokens' set to 2048 and enforce regional restrictions through IAM policies for data residency compliance.",
        "C": "Configure the semantic chunking breakpoint threshold to 0.75 and set the buffer size to 800KB. Use Amazon Bedrock InvokeModel API with 'MaxTokens' set to 512 and enable data encryption in transit using AWS Key Management Service (KMS).",
        "D": "Configure the semantic chunking breakpoint threshold to 0.90 and set the buffer size to 1MB. Use Amazon Bedrock InvokeModel API with 'MaxTokens' set to 2048 and ensure compliance by enabling S3 bucket policies to restrict cross-region data access."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because setting the breakpoint threshold to 0.85 ensures that semantic chunking minimizes incomplete sentence splits, and a buffer size of 900KB is optimal for staying within the 1MB input limit while reducing latency. Using 'MaxTokens' set to 1024 balances accuracy and cost by avoiding over-allocation of tokens, and deploying in the required AWS Region ensures compliance with data residency regulations. Option B fails because a breakpoint threshold of 0.95 may cause unnecessary latency by over-splitting, and 'MaxTokens' set to 2048 increases costs unnecessarily. Option C fails because a breakpoint threshold of 0.75 risks splitting sentences improperly, leading to reduced accuracy, and 'MaxTokens' set to 512 impacts the model's ability to process larger text chunks. Option D fails because a buffer size of 1MB reaches the maximum input limit, leaving no room for metadata or overhead, and S3 bucket policies alone are insufficient for ensuring compliance with processing region restrictions.",
      "topic": "Semantic chunking configuration - breakpoint threshold and buffer size tuning",
      "difficulty": "hard",
      "category": "bedrock",
      "aws_services": [
        "Amazon Bedrock",
        "AWS Cost and Usage Reports",
        "AWS Key Management Service (KMS)",
        "Amazon S3"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:33:03.872970",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 3
    },
    {
      "question": "A global healthcare company is building a Retrieval-Augmented Generation (RAG) application using Amazon Bedrock for medical query answering. The system must integrate a custom LLM hosted on Bedrock and retrieve contextually relevant data from an Amazon OpenSearch Service domain. Due to strict HIPAA compliance requirements, all queries and generated responses must be validated to avoid leakage of Protected Health Information (PHI). Additionally, the company has cost constraints and aims to minimize data transfer charges. The solution must ensure low-latency responses (under 200ms on average) and restrict access to the OpenSearch data to only the LLM for inference purposes. Which solution BEST meets these requirements?",
      "options": {
        "A": "Use Amazon Bedrock's managed LLM and set up a Lambda function as a pre-invocation hook for response validation. Store the contextual data in Amazon OpenSearch Service with fine-grained access control using resource-based policies that allow access only from the Bedrock service. Configure OpenSearch UltraWarm for cost-efficient storage of historical data.",
        "B": "Use Amazon Bedrock's managed LLM and implement a pre-invocation Lambda function for response validation. Store contextual data in Amazon OpenSearch Service with a VPC endpoint and IAM role-based access. Enable UltraWarm for cost savings, but allow unrestricted OpenSearch domain access for broader analytics needs.",
        "C": "Deploy a custom LLM on Amazon Bedrock and implement an AWS Step Functions workflow for response validation. Store contextual data in Amazon OpenSearch Service with a VPC endpoint and fine-grained access control using resource-based policies. Use OpenSearch cold storage for cost optimization.",
        "D": "Deploy a custom LLM on Amazon Bedrock and use Amazon API Gateway with a Lambda authorizer for response validation. Store contextual data in Amazon OpenSearch Service with fine-grained access control using IAM policies. Enable UltraWarm and replicate the domain to a secondary region for disaster recovery."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because it uses Amazon Bedrock's managed LLM with a Lambda pre-invocation hook for response validation, satisfying the HIPAA compliance requirement. The fine-grained access control using resource-based policies aligns with the security constraint of restricting OpenSearch data access to only the LLM. UltraWarm is used for cost-efficient storage of historical data, balancing cost and performance. Option B fails because unrestricted OpenSearch domain access violates the requirement to restrict access to only the LLM. Option C fails because OpenSearch cold storage is optimized for archival use cases, not low-latency requirements. Option D fails because replicating the domain to a secondary region increases costs unnecessarily and is not explicitly required for this use case.",
      "topic": "RAG with Guardrails contextual grounding - response validation",
      "difficulty": "hard",
      "category": "bedrock",
      "aws_services": [
        "Amazon Bedrock",
        "Amazon OpenSearch Service",
        "AWS Lambda"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/bedrock/latest/userguide/guardrails.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:33:22.450429",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 4
    },
    {
      "question": "A financial services company needs to evaluate multiple foundation models for a natural language processing (NLP) use case involving credit risk analysis. They want to automate benchmarking across different models available in Amazon Bedrock, measuring accuracy and inference latency. The evaluation process must comply with strict data residency requirements due to financial regulations, ensuring that all data stays within the AWS Region. The company has a limited budget for compute resources but requires results to be available within 24 hours of starting the evaluation job. Which solution BEST meets these requirements?",
      "options": {
        "A": "Use Amazon Bedrock's `StartModelCustomizationJob` API to fine-tune multiple foundation models, configure the `OutputDataS3Uri` parameter to point to an S3 bucket with encryption enabled in the same region, and use AWS Step Functions to orchestrate parallel evaluation workflows. Use SageMaker Model Monitor to log inference performance metrics.",
        "B": "Use Amazon Bedrock's `StartModelEvaluationJob` API with the `EvaluationConfig` parameter set to specify multiple models, configure the `OutputDataS3Uri` parameter to an S3 bucket with server-side encryption in the same region, and use AWS Step Functions to orchestrate the evaluations. Use SageMaker Model Monitor for inference metrics.",
        "C": "Use Amazon Bedrock's `StartModelEvaluationJob` API with the `EvaluationConfig` parameter set to specify multiple models, configure the `OutputDataS3Uri` parameter to an S3 bucket in a different AWS Region for cost optimization, and use AWS Batch to manage parallel evaluations. Use CloudWatch Logs for inference performance metrics.",
        "D": "Use Amazon Bedrock's `StartModelCustomizationJob` API with the `OutputDataS3Uri` parameter pointing to an S3 bucket in a different AWS Region, use Step Functions for orchestration, and enable CloudTrail logging for compliance tracking. Use SageMaker Model Monitor for inference performance metrics."
      },
      "correct_answer": "B",
      "explanation": "Option B is correct because it uses the `StartModelEvaluationJob` API to automate benchmarking, specifies multiple models through the `EvaluationConfig` parameter, and ensures compliance with data residency requirements by storing results in an S3 bucket in the same region with server-side encryption. AWS Step Functions efficiently orchestrates the evaluations, and SageMaker Model Monitor logs performance metrics.\n\nOption A is incorrect because the `StartModelCustomizationJob` API is intended for fine-tuning models, not benchmarking, and would unnecessarily increase compute costs and runtime.\n\nOption C is incorrect because storing data in an S3 bucket in a different region violates the strict data residency requirements.\n\nOption D is incorrect because the `StartModelCustomizationJob` API is used instead of the correct `StartModelEvaluationJob` API, and storing data in a different region breaches compliance requirements.",
      "topic": "Model evaluation jobs - automated benchmarking",
      "difficulty": "medium",
      "category": "bedrock",
      "aws_services": [
        "Amazon Bedrock",
        "AWS Step Functions",
        "Amazon S3",
        "SageMaker Model Monitor"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:33:43.614647",
      "critique": {
        "overall_score": 8.5,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [
          "The question assumes familiarity with the `StartModelEvaluationJob` API, which is not explicitly mentioned in the AWS documentation as of October 2023. However, the explanation aligns with the general capabilities of Amazon Bedrock for evaluating multiple models.",
          "The use of SageMaker Model Monitor for logging inference performance metrics is a reasonable choice, but it might not be the most direct or cost-effective solution for this specific use case. CloudWatch Logs could also suffice for basic inference metric logging."
        ],
        "verification_status": "APPROVED",
        "confidence": 0.95
      },
      "verification_status": "APPROVED",
      "id": 5
    },
    {
      "question": "A financial services company is using Amazon Bedrock to build a custom knowledge base for its internal document search system. The documents include PDF files with complex tables, markdown files containing structured text, and code files with embedded comments and function definitions. The company needs to implement document-specific chunking to optimize performance for its generative AI model, ensuring accurate and contextually relevant responses. Additionally, they must comply with strict data security policies requiring encryption of data at rest and in transit, and minimize costs while maintaining scalability for unpredictable query volumes. Which solution BEST meets these requirements?",
      "options": {
        "A": "Use Amazon Bedrock with a custom preprocessing pipeline leveraging Amazon S3 Select for selective parsing of PDF tables, markdown files, and code files. Configure Amazon S3 to encrypt data at rest using AES-256 and set up Amazon CloudFront for TLS encryption during data transmission. Implement chunking logic tailored to file types within AWS Lambda, ensuring dynamic scalability for processing workloads.",
        "B": "Use Amazon Bedrock with a custom preprocessing pipeline leveraging Amazon Textract for extracting PDF tables, markdown files, and code files. Store processed files in Amazon S3 buckets encrypted with SSE-S3 (Amazon S3 managed keys) and configure Amazon API Gateway for secure HTTPS endpoints. Implement chunking within Amazon SageMaker processing jobs for scalable workload handling.",
        "C": "Use Amazon Bedrock with Amazon Comprehend for preprocessing and extracting document content, including PDF tables, markdown files, and code files. Store all processed files in Amazon S3 encrypted with SSE-KMS (AWS Key Management Service customer-managed keys) and configure Amazon AppSync for secure data access. Implement chunking logic directly within Amazon AppSync resolvers for optimal query performance.",
        "D": "Use Amazon Bedrock with a custom preprocessing pipeline leveraging Amazon Athena for querying structured data from PDF tables, markdown files, and code files. Store processed files in Amazon S3 buckets encrypted with SSE-KMS (AWS Key Management Service customer-managed keys). Deploy AWS Lambda for implementing document-specific chunking logic and configure AWS Elastic Load Balancing with TLS certificates for secure data transmission."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because it uses Amazon S3 Select for efficient and cost-effective selective parsing tailored to document types, ensuring optimized chunking for each file type. It also ensures compliance with data security policies by encrypting data at rest with AES-256 and securing data in transit using TLS via Amazon CloudFront. AWS Lambda provides dynamic scalability for processing workloads, which minimizes costs during unpredictable query volumes. Option B fails because Amazon Textract is not optimized for markdown or code file parsing, leading to inefficiencies in preprocessing. Option C fails because Amazon Comprehend is not suitable for extracting tabular data from PDFs or code-specific content, and AppSync resolvers are less effective than Lambda for dynamic chunking logic. Option D fails because Amazon Athena is designed for querying structured data but is not ideal for preprocessing diverse document types like markdown or code files, and while it uses SSE-KMS for encryption, the processing pipeline lacks tailored chunking for individual file types.",
      "topic": "Document-specific chunking - PDF tables vs markdown vs code files",
      "difficulty": "hard",
      "category": "bedrock",
      "aws_services": [
        "Amazon Bedrock",
        "Amazon S3",
        "AWS Lambda",
        "Amazon CloudFront"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:34:02.685954",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [
          "While the explanation for Option A is valid, it assumes that Amazon S3 Select is the best choice for parsing all file types (PDF tables, markdown, and code files). This might not always be the case, as S3 Select is primarily designed for querying structured data in objects stored in S3, such as CSV or JSON. However, the question does not explicitly rule out this use case, so the assumption is reasonable for the context of the question.",
          "Option D mentions Amazon Athena for preprocessing, which is not ideal for handling diverse document types like markdown or code files. However, this is correctly identified as a limitation in the explanation."
        ],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 6
    },
    {
      "question": "A financial services company is building a real-time fraud detection system using AWS Bedrock for generative AI and Amazon SageMaker for model training. They process thousands of transactions per second and require monitoring the latency of AWS Lambda functions used to invoke Bedrock models via its InvokeModel API. The company must ensure that metrics for InvocationLatency are captured in near real-time, with granularity no greater than 1 minute. They also require alerts when latency exceeds 300 ms. Cost optimization is a priority, but the solution must meet compliance requirements for near real-time monitoring and alerting. Which solution BEST meets these requirements?",
      "options": {
        "A": "Use Amazon CloudWatch with a custom metric for InvocationLatency by publishing the `Duration` value from the Lambda function logs. Configure a CloudWatch Alarm with a threshold of 300 ms and a 1-minute evaluation period.",
        "B": "Enable AWS Lambda's built-in metric for InvocationDuration and use CloudWatch Contributor Insights to monitor trends. Configure a CloudWatch Alarm with a threshold of 300 ms and a 1-minute evaluation period.",
        "C": "Use AWS Lambda's built-in metric for Duration, and configure a CloudWatch Alarm on the Duration metric with a threshold of 300 ms. Enable Enhanced Monitoring for Bedrock models to capture additional latency details.",
        "D": "Enable Application Insights in CloudWatch to automatically track InvocationLatency for Bedrock API calls. Configure a CloudWatch Alarm with a threshold of 300 ms and a 1-minute evaluation period."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because using a custom CloudWatch metric for InvocationLatency by publishing the `Duration` value from Lambda logs ensures compliance with the requirement for near real-time monitoring and granularity at 1 minute. Option B fails because Contributor Insights is designed for analyzing trends and anomalies, not for real-time latency monitoring of specific transactions. Option C fails because while the Duration metric provides latency data, Enhanced Monitoring for Bedrock models is unnecessary and incurs additional costs without meeting the specific latency monitoring requirement. Option D fails because Application Insights cannot directly monitor InvocationLatency for Bedrock API calls and is not designed for this purpose.",
      "topic": "CloudWatch metrics - InvocationLatency monitoring",
      "difficulty": "medium",
      "category": "bedrock",
      "aws_services": [
        "AWS Lambda",
        "Amazon CloudWatch",
        "AWS Bedrock"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/working_with_metrics.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:34:22.305568",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [
          "The explanation could clarify that publishing a custom metric from Lambda logs is necessary because AWS Lambda's built-in `Duration` metric does not directly align with the specific requirement for InvocationLatency monitoring of Bedrock API calls."
        ],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 7
    },
    {
      "question": "A financial analytics company needs to run batch inference jobs using AWS Bedrock for their machine learning models. The input data is stored in an encrypted Amazon S3 bucket, and the output results must also be written back to the same bucket. The company requires the solution to comply with stringent security standards, ensuring encryption both in transit and at rest, while minimizing costs associated with storage and compute resources. Additionally, they need to ensure that inference requests are processed efficiently and avoid exceeding Bedrock's default API rate limits of 10 requests per second per account. Which solution BEST meets these requirements?",
      "options": {
        "A": "Use Amazon Bedrock's `StartModelInferenceJob` API with an IAM role that has access to the S3 bucket. Configure S3 bucket policies to enforce server-side encryption using SSE-KMS. Specify the input and output S3 URIs in the API request and enable encryption in transit using HTTPS endpoints.",
        "B": "Use Amazon Bedrock's `StartModelInferenceJob` API with an IAM role that has access to the S3 bucket. Configure S3 bucket policies to enforce server-side encryption using SSE-S3. Specify the input and output S3 URIs in the API request and enable encryption in transit using HTTPS endpoints.",
        "C": "Use Amazon Bedrock's `StartModelInferenceJob` API with an IAM role that has access to the S3 bucket. Configure S3 bucket policies to enforce server-side encryption using SSE-KMS. Specify the input and output data using pre-signed S3 URLs in the API request and enforce encryption in transit using HTTPS endpoints.",
        "D": "Use Amazon Bedrock's `StartModelInferenceJob` API with an IAM role that has access to the S3 bucket. Configure S3 bucket policies to enforce server-side encryption using SSE-KMS. Specify the input and output S3 URIs in the API request and enforce encryption in transit using HTTP endpoints to minimize cost."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because it uses SSE-KMS for encryption at rest, which aligns with stringent security standards, and ensures encryption in transit using HTTPS endpoints. Additionally, it specifies input and output S3 URIs in the API request, which is the correct configuration for batch inference jobs using Amazon Bedrock. Option B fails because SSE-S3, while secure, does not offer the same level of control and audit capabilities as SSE-KMS, which is critical for compliance in the financial industry. Option C fails because using pre-signed S3 URLs for input and output data is not recommended for batch inference jobs as it adds unnecessary complexity and potential security risks. Option D fails because using HTTP endpoints does not ensure encryption in transit, violating compliance requirements.",
      "topic": "Batch inference jobs - S3 input/output processing",
      "difficulty": "medium",
      "category": "bedrock",
      "aws_services": [
        "Amazon Bedrock",
        "Amazon S3",
        "IAM"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/bedrock/latest/userguide/inference.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:34:36.736973",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 8
    },
    {
      "question": "A global e-commerce company is building a semantic search feature to improve product discovery across its catalog of 10 million items. The company plans to embed product descriptions and user queries into vector representations for similarity matching. They intend to use Amazon Bedrock for model inference but must decide between Titan Text Embeddings V2 and Cohere Embed. The chosen solution must meet the following constraints: \n\n1. Queries must return results within 50ms per request to ensure low latency. \n2. The embedding model must support 1,536 dimensions to integrate with their existing OpenSearch index configured for dense vector search. \n3. The company operates in multiple regions and requires compliance with GDPR, ensuring data is processed in the EU. \n4. Monthly inference costs must not exceed $5,000, given an estimated 200,000 queries daily.\n\nWhich solution BEST meets these requirements?",
      "options": {
        "A": "Use Cohere Embed via Amazon Bedrock with the medium model configuration (1,536 dimensions) and deploy the endpoint in the eu-central-1 region. Optimize costs using multi-model endpoint configuration and set the `max_concurrent_inference` parameter to 4.",
        "B": "Use Titan Text Embeddings V2 via Amazon Bedrock with the large model configuration (1,536 dimensions) in the eu-west-1 region. Configure the endpoint with a provisioned concurrency of 5 to ensure low latency.",
        "C": "Use Cohere Embed via Amazon Bedrock with the large model configuration (768 dimensions) in the eu-central-1 region. Configure the endpoint with auto-scaling to handle traffic spikes and reduce costs.",
        "D": "Use Titan Text Embeddings V2 via Amazon Bedrock with the medium model configuration (768 dimensions) in the eu-west-1 region. Use provisioned concurrency of 3 to balance cost and performance."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because Cohere Embed with the medium model configuration provides 1,536 dimensions, matching the OpenSearch requirement. Deploying in eu-central-1 ensures GDPR compliance, and multi-model endpoints help optimize costs while maintaining low latency with the `max_concurrent_inference` parameter set to 4. \n\nOption B fails because Titan Text Embeddings V2 does not natively support 1,536 dimensions in its configurations. Additionally, provisioned concurrency of 5 may be unnecessary for the query volume, potentially increasing costs. \n\nOption C fails because the large model configuration for Cohere Embed supports only 768 dimensions, which does not meet the OpenSearch requirement. Auto-scaling also introduces latency variability, which could breach the 50ms SLA. \n\nOption D fails because Titan Text Embeddings V2 with 768 dimensions does not meet the dimensionality requirement. Additionally, provisioned concurrency of 3 may not provide sufficient throughput for 200,000 daily queries within the latency constraint.",
      "topic": "Embedding model selection - Titan Embeddings V2 vs Cohere Embed dimensions and performance",
      "difficulty": "hard",
      "category": "bedrock",
      "aws_services": [
        "Amazon Bedrock",
        "OpenSearch"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:34:53.287155",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 9
    },
    {
      "question": "A financial services company is building a Retrieval-Augmented Generation (RAG) system using Amazon Bedrock to provide real-time answers to customer queries based on internal knowledge documents. The knowledge base comprises highly sensitive financial data stored in Amazon S3, and the company must comply with strict PCI DSS requirements. The system needs to optimize the context window for generative AI models while minimizing latency and managing token costs effectively. The company has set the following constraints: (1) The overall cost per query must not exceed $0.01, (2) Query responses must remain under 1,024 tokens to avoid exceeding the model's token limit, (3) Chunking must ensure relevance without missing critical context, (4) Data encryption at rest and in transit is mandatory, and (5) The solution must ensure scalability to handle up to 10,000 queries per minute. Which solution BEST meets these requirements?",
      "options": {
        "A": "Use Amazon Bedrock with a custom prompt template that dynamically adjusts context window size based on input query length. Pre-process the S3 documents using AWS Glue to chunk them into 512-token sections, ensuring semantic relevance by leveraging Amazon Comprehend for entity extraction. Enable server-side encryption using SSE-S3 and use Amazon CloudFront for low-latency query delivery. Optimize costs by setting a token usage limit in the Bedrock API.",
        "B": "Use Amazon Bedrock with a static prompt template defining a fixed context window size of 1,024 tokens. Pre-process the S3 documents using AWS Glue to chunk them into 512-token sections based on document length, without semantic analysis. Use SSE-S3 for encryption and Amazon CloudFront for query delivery, but manage costs by limiting API requests using AWS Lambda to throttle query rates during high-traffic periods.",
        "C": "Use Amazon Bedrock with a custom prompt template that dynamically adjusts context window size based on query length. Pre-process the S3 documents using AWS Glue to chunk them into 256-token sections, prioritizing semantic relevance with Amazon Comprehend entity extraction. Implement SSE-KMS for encryption and use Amazon API Gateway with caching enabled for low-latency delivery. Manage costs by setting a token usage limit in the Bedrock API.",
        "D": "Use Amazon Bedrock with a static prompt template defining a fixed context window size of 1,024 tokens. Pre-process the S3 documents using AWS Glue to chunk them into 256-token sections based on document length, without semantic analysis. Encrypt data with SSE-KMS and use Amazon CloudFront for query delivery, while managing costs by throttling API requests using AWS Lambda during peak query times."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because it dynamically adjusts the context window based on query length, ensuring token efficiency while maintaining semantic relevance through entity extraction. It also uses SSE-S3 for encryption, which is sufficient for compliance, and employs CloudFront for low-latency delivery, aligning with scalability and cost constraints. Option B fails because it uses a static context window size, which may lead to inefficient token usage for shorter queries. Option C fails because chunking into 256-token sections might increase token costs due to excessive context splitting, and using SSE-KMS unnecessarily increases encryption costs without improving compliance. Option D fails because it uses a static context window and lacks semantic analysis during chunking, which can reduce response relevance.",
      "topic": "Context window optimization for RAG responses - chunking and token management",
      "difficulty": "hard",
      "category": "bedrock",
      "aws_services": [
        "Amazon Bedrock",
        "AWS Glue",
        "Amazon S3",
        "Amazon CloudFront",
        "Amazon Comprehend"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:35:08.784520",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 10
    },
    {
      "question": "A healthcare company is building a medical document search solution using Amazon Bedrock to allow physicians to retrieve information from patient records efficiently. They need to optimize their search model for high recall to avoid missing any relevant patient data, but they also want to maintain reasonable precision to minimize irrelevant results. The solution must comply with HIPAA, ensuring all data is encrypted at rest and in transit. The company has a strict budget and wants to minimize inference costs while achieving optimal model performance. They are using the Bedrock `InvokeModel` API and have configured their model with a retrieval score threshold. Which solution BEST meets these requirements?",
      "options": {
        "A": "Use Amazon Bedrock with a tuned retrieval score threshold of 0.4 for high recall. Enable AWS Key Management Service (KMS) for data encryption and use a custom endpoint with private VPC connectivity to ensure HIPAA compliance.",
        "B": "Use Amazon Bedrock with a retrieval score threshold of 0.7 for better precision. Enable AWS Key Management Service (KMS) for encryption and deploy the model endpoint with public internet access to reduce costs.",
        "C": "Use Amazon Bedrock with a retrieval score threshold of 0.4 for high recall. Enable encryption using Amazon S3 default encryption for compliance and set up the model endpoint with private VPC connectivity.",
        "D": "Use Amazon Bedrock with a retrieval score threshold of 0.4 for high recall. Enable AWS Key Management Service (KMS) encryption but deploy the model endpoint with public internet access to reduce costs."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because setting the retrieval score threshold to 0.4 optimizes for high recall, which is critical for avoiding missed patient data. Using AWS KMS ensures HIPAA-compliant encryption, and private VPC connectivity secures the endpoint for sensitive healthcare data. Option B fails because a threshold of 0.7 prioritizes precision over recall, which does not meet the requirement of avoiding missed data, and public internet access does not comply with HIPAA. Option C fails because using Amazon S3 default encryption does not cover Bedrock inference data, which requires AWS KMS for compliance. Option D fails because while it uses the correct threshold and encryption, public internet access compromises HIPAA compliance.",
      "topic": "Retrieval score thresholds - precision vs recall tuning",
      "difficulty": "medium",
      "category": "bedrock",
      "aws_services": [
        "Amazon Bedrock",
        "AWS Key Management Service (KMS)",
        "Amazon VPC"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:35:20.889061",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 11
    },
    {
      "question": "A financial analytics company is building a custom generative AI model for summarizing financial reports using Amazon Bedrock. They need to fine-tune the model with a dataset of 15 million financial statements stored in Amazon S3. The dataset is in CSV format but must be converted into JSONL format for fine-tuning. The company has the following constraints:\n\n1. Data processing must comply with GDPR and encrypt data at rest and in transit.\n2. The budget for data preparation is limited, and they aim to minimize costs.\n3. The solution must scale to handle a large dataset efficiently without exceeding memory limits or running into API request throttling during processing.\n4. The JSONL format must adhere to Bedrock's specific schema requirements for input data.\n\nWhich solution BEST meets these requirements?",
      "options": {
        "A": "Use an AWS Glue ETL job with a Python shell script to read the CSV data, transform it into Bedrock-compatible JSONL format, and write the output to an S3 bucket with server-side encryption (SSE-S3). Configure the Glue job's DynamicFrame to handle large datasets efficiently and enable job bookmarks for incremental processing.",
        "B": "Use an Amazon SageMaker Processing job with a custom Python script to read the CSV data, transform it into Bedrock-compatible JSONL format, and write the output to an S3 bucket with server-side encryption (SSE-KMS). Specify the `VolumeSizeInGB` parameter as 500 to handle large datasets and enable distributed processing.",
        "C": "Use an AWS Lambda function triggered by S3 events to process the CSV files into Bedrock-compatible JSONL format. Store the transformed JSONL files in an S3 bucket with client-side encryption using AWS KMS for compliance. Use S3 multipart upload to handle large files.",
        "D": "Use an Amazon EMR cluster with Apache Spark to process the CSV files into Bedrock-compatible JSONL format and write the output to an S3 bucket with server-side encryption (SSE-KMS). Configure the Spark job to use a `maxResultSize` of 1 GB and enable encryption in transit using AWS Glue Data Catalog for schema storage."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because AWS Glue is a cost-effective solution for large-scale ETL processes, and its DynamicFrame abstraction simplifies handling large datasets. The use of SSE-S3 ensures compliance with GDPR for encryption at rest, and job bookmarks prevent redundant processing, minimizing costs. Option B is incorrect because SageMaker Processing jobs are more suitable for ML model preparation than ETL and would be more expensive for this use case. Option C is incorrect because Lambda has memory and execution time limits, making it unsuitable for processing datasets of this scale. Additionally, client-side encryption with AWS KMS introduces unnecessary complexity. Option D is incorrect because while EMR with Spark can handle large datasets, it incurs higher costs and complexity compared to AWS Glue for this specific ETL task. The `maxResultSize` parameter is not directly applicable to this scenario.",
      "topic": "Fine-tuning data preparation - JSONL format",
      "difficulty": "medium",
      "category": "bedrock",
      "aws_services": [
        "Amazon Bedrock",
        "AWS Glue",
        "Amazon S3",
        "SageMaker",
        "Amazon EMR"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/bedrock/latest/userguide/custom-models.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:35:42.472916",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 12
    },
    {
      "question": "A global e-commerce company is implementing an AI-driven customer support system using Amazon Bedrock and foundation models. They need to build a knowledge base for summarizing and retrieving product-related queries (e.g., warranty terms, compatibility details). The knowledge base will be queried thousands of times per second across multiple regions, requiring low latency and high throughput. Due to the sensitivity of some content (e.g., financial refund policies), compliance with GDPR and data residency laws is mandatory. Additionally, the company must optimize costs while ensuring the semantic accuracy of responses. They are deciding between fixed-size, semantic, or hierarchical chunking strategies for splitting their documents before indexing. Which solution BEST meets these requirements?",
      "options": {
        "A": "Use semantic chunking for documents with the Amazon Bedrock `Retrieve` API, setting the `chunk_size` to dynamically adjust based on paragraph boundaries and content semantics. Store the indexed embeddings in Amazon OpenSearch Serverless, using HNSW with `cosinesimil` for similarity scoring. Configure Amazon S3 for data storage with bucket-level encryption using AWS KMS keys to meet GDPR compliance.",
        "B": "Use hierarchical chunking for documents with the Amazon Bedrock `Retrieve` API, setting the `chunk_size` parameter to 1024 tokens for optimal performance. Store the indexed embeddings in Amazon OpenSearch Serverless, using IVF with `l2` distance for similarity scoring. Configure S3 storage with default encryption enabled to comply with GDPR.",
        "C": "Use fixed-size chunking for documents with the Amazon Bedrock `Retrieve` API, setting the `chunk_size` parameter to 512 tokens. Store the embeddings in Amazon DynamoDB for fast retrieval, and enable global table replication for multi-region availability. Use S3 storage with bucket-level encryption using AWS KMS keys to meet GDPR compliance.",
        "D": "Use semantic chunking for documents with the Amazon Bedrock `Retrieve` API, setting the `chunk_size` to dynamically adjust based on paragraph boundaries. Store embeddings in Amazon OpenSearch Serverless, using HNSW with `l2` distance for similarity scoring. Configure Amazon S3 with bucket policies to restrict access for GDPR compliance."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because semantic chunking ensures the content is split based on meaning, improving the semantic accuracy of query results. The use of dynamic `chunk_size` based on paragraph boundaries aligns well with the requirement for high semantic fidelity. Amazon OpenSearch Serverless with HNSW and `cosinesimil` distance is optimal for low-latency, high-throughput similarity searches. Bucket-level encryption with AWS KMS keys ensures compliance with GDPR data residency and security requirements. \n\nOption B fails because hierarchical chunking with a static `chunk_size` of 1024 tokens may lead to poor semantic splits, reducing the quality of retrieval. Additionally, IVF and `l2` distance in OpenSearch are less efficient and accurate than HNSW and `cosinesimil` for this use case. \n\nOption C fails because fixed-size chunking does not adapt to the semantic structure of the documents, leading to potential inaccuracies in retrieval. While DynamoDB offers fast retrieval, it is not optimized for similarity search compared to OpenSearch. \n\nOption D fails because using `l2` distance with HNSW is suboptimal for the required semantic similarity search, as `cosinesimil` is the correct distance metric for embeddings. Additionally, relying solely on bucket policies for GDPR compliance is insufficient compared to encryption with AWS KMS keys.",
      "topic": "Chunking strategies comparison - fixed size vs semantic vs hierarchical trade-offs",
      "difficulty": "hard",
      "category": "bedrock",
      "aws_services": [
        "Amazon Bedrock",
        "Amazon OpenSearch Serverless",
        "Amazon S3",
        "AWS KMS"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:36:08.749861",
      "critique": {
        "overall_score": 8.5,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [
          "The explanation for Option D could clarify why bucket policies alone are insufficient for GDPR compliance, as encryption with AWS KMS keys is a stronger compliance measure.",
          "The question could explicitly mention that OpenSearch Serverless supports HNSW with `cosinesimil`, which is implied but not directly stated in the AWS facts provided."
        ],
        "verification_status": "APPROVED",
        "confidence": 0.95
      },
      "verification_status": "APPROVED",
      "id": 13
    },
    {
      "question": "A global healthcare company plans to leverage Amazon Bedrock to implement an AI-driven virtual assistant for patient inquiries. The assistant must provide accurate, context-aware responses while adhering to strict HIPAA compliance and ensuring private patient data is not exposed during training or inference. Additionally, the company must implement hallucination detection for the model to ensure factual integrity in responses. They require a cost-effective solution, but performance cannot be compromised since the assistant will serve a high volume of concurrent users across multiple regions. Which solution BEST meets these requirements?",
      "options": {
        "A": "Use Amazon Bedrock with a fine-tuned foundation model deployed in a private VPC endpoint. Enable Content Moderation Guardrails for hallucination detection, and configure the `EnableComplianceMode` parameter to ensure HIPAA compliance. Use Amazon CloudWatch to monitor performance and include Amazon S3 for storing encrypted interaction logs with SSE-KMS.",
        "B": "Use Amazon Bedrock with a fine-tuned foundation model deployed publicly. Enable Content Moderation Guardrails for hallucination detection, and configure the `EnableComplianceMode` parameter to ensure HIPAA compliance. Use AWS CloudTrail to monitor performance and include Amazon S3 for storing encrypted interaction logs with SSE-KMS.",
        "C": "Use Amazon Bedrock with a fine-tuned foundation model deployed in a private VPC endpoint. Enable Factuality Guardrails for hallucination detection, and configure the `EnableComplianceMode` parameter to ensure HIPAA compliance. Use Amazon CloudWatch to monitor performance and include Amazon S3 for storing encrypted interaction logs with SSE-KMS.",
        "D": "Use Amazon Bedrock with a fine-tuned foundation model deployed in a private VPC endpoint. Enable Content Moderation Guardrails for hallucination detection, and configure the `EnableHIPAACompliance` parameter to ensure HIPAA compliance. Use Amazon CloudWatch to monitor performance and include Amazon S3 for storing encrypted interaction logs with SSE-KMS."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because it uses a private VPC endpoint for enhanced security, enables the Content Moderation Guardrails appropriate for hallucination detection, and uses the correct `EnableComplianceMode` parameter for HIPAA compliance. It also includes Amazon CloudWatch for granular performance monitoring and uses SSE-KMS for securing interaction logs in S3. \n\nOption B fails because deploying the model publicly does not meet the strict HIPAA compliance requirements. \n\nOption C fails because the Factuality Guardrails are not explicitly designed for hallucination detection, making this option less effective for the use case. \n\nOption D fails because `EnableHIPAACompliance` is not a valid parameter; the correct parameter is `EnableComplianceMode`.",
      "topic": "Guardrails contextual grounding - hallucination detection",
      "difficulty": "hard",
      "category": "bedrock",
      "aws_services": [
        "Amazon Bedrock",
        "Amazon S3",
        "Amazon CloudWatch",
        "AWS KMS"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/bedrock/latest/userguide/guardrails.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:36:22.407757",
      "critique": {
        "overall_score": 8.5,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [
          "The question assumes that Content Moderation Guardrails are the most appropriate for hallucination detection, which may not be explicitly stated in AWS documentation. However, this is a reasonable assumption given the context."
        ],
        "verification_status": "APPROVED",
        "confidence": 0.95
      },
      "verification_status": "APPROVED",
      "id": 14
    },
    {
      "question": "A global e-commerce company is building a personalized recommendation engine for their website using Amazon Bedrock. The system must support multi-query retrieval, where the application reformulates the users initial query into multiple expanded subqueries to improve result relevance. The company requires low latency for real-time user interactions, strict adherence to GDPR compliance for customer data, and must keep costs predictable, as they expect query volumes to spike during seasonal sales. All query expansion and reformulation logic must be handled server-side, and they want to leverage a foundation model from Bedrock. Additionally, the company seeks to log all user interactions for future analytics, ensuring logs are encrypted at rest and compliant with data residency requirements for the EU. Which solution BEST meets these requirements?",
      "options": {
        "A": "Use Amazon Bedrock with a foundation model supporting multi-turn conversational capabilities. Configure the `generateText` API with the `temperature` parameter set to 0.7 for controlled query reformulation. Store query logs in Amazon S3 with a bucket in the EU, using server-side encryption with AWS KMS. Use Amazon CloudWatch Logs for near real-time monitoring.",
        "B": "Use Amazon Bedrock with a foundation model optimized for retrieval-augmented generation (RAG). Call the `invokeModel` API with `maxTokens` set to 512 for query reformulation. Store query logs in Amazon S3 with a bucket in the EU, using S3-managed encryption keys (SSE-S3). Use AWS CloudTrail for monitoring.",
        "C": "Use Amazon Bedrock with a foundation model supporting multi-turn conversational capabilities. Configure the `generateText` API with the `topP` parameter set to 0.9 to ensure diverse query reformulation. Store query logs in Amazon S3 with a bucket in the EU, using server-side encryption with AWS KMS. Enable VPC endpoints for enhanced security.",
        "D": "Use Amazon Bedrock with a foundation model optimized for retrieval-augmented generation (RAG). Call the `invokeModel` API with the `temperature` parameter set to 0.5 for deterministic query reformulation. Store query logs in Amazon S3 with a bucket in the EU, using server-side encryption with AWS KMS. Use Amazon CloudWatch Logs for near real-time monitoring."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because it uses a foundation model with multi-turn conversational capabilities, which is well-suited for query reformulation in multi-query retrieval scenarios. The `generateText` API with `temperature` set to 0.7 balances diversity and control in query expansion. Logs are stored in an S3 bucket in the EU with AWS KMS encryption, meeting GDPR compliance, and Amazon CloudWatch Logs provides real-time monitoring. \n\nOption B fails because the use of the `invokeModel` API with `maxTokens` is not optimal for query reformulation in this use case, and SSE-S3 encryption does not meet the company's requirement for using customer-managed encryption keys. \n\nOption C fails because setting the `topP` parameter to 0.9 could lead to overly diverse reformulations that might degrade result relevance in a controlled query expansion scenario. While VPC endpoints enhance security, they do not directly address the query reformulation or logging requirements. \n\nOption D fails because the `invokeModel` API is not the best choice for multi-query retrieval, and the `temperature` parameter at 0.5 could overly limit diversity in query reformulations, potentially impacting relevance.",
      "topic": "Multi-query retrieval - query expansion and reformulation strategies",
      "difficulty": "hard",
      "category": "bedrock",
      "aws_services": [
        "Amazon Bedrock",
        "Amazon S3",
        "AWS CloudWatch Logs",
        "AWS KMS"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:36:44.924195",
      "critique": {
        "overall_score": 8.5,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [
          "The explanation for why `invokeModel` API is not optimal in Option B could be elaborated further. While it is true that `invokeModel` is not typically used for multi-turn conversational capabilities, the explanation does not explicitly clarify why it is less suitable for query reformulation compared to `generateText`.",
          "The explanation for Option C could better clarify why `topP` set to 0.9 might degrade relevance. While diversity is mentioned, it could be helpful to explain how this impacts controlled query reformulation specifically."
        ],
        "verification_status": "APPROVED",
        "confidence": 0.95
      },
      "verification_status": "APPROVED",
      "id": 15
    },
    {
      "question": "A multinational legal consultancy firm is building a generative AI-powered document retrieval system using Amazon Bedrock for research across large, hierarchical legal documents. These documents include nested structures such as contracts with sections, subsections, and clauses. The system must preserve the document hierarchy to ensure context is maintained during Retrieval-Augmented Generation (RAG) workflows. Additionally, the firm must comply with regional data residency laws, requiring that data processing occurs within specific AWS Regions. The solution must be cost-efficient while ensuring low latency for end users globally. Finally, the firm has strict security requirements, including encryption of data at rest and in transit, and the ability to control access at a granular level for sensitive legal documents. Which solution BEST meets these requirements?",
      "options": {
        "A": "Use Amazon Bedrock with a custom model for hierarchical document chunking. Store the chunked data in Amazon DynamoDB with hierarchy metadata. Use Amazon KMS for encryption and define IAM policies to enforce access control. Deploy the solution in multiple AWS Regions to comply with data residency requirements, and use Amazon CloudFront for low-latency global access.",
        "B": "Use Amazon Bedrock with a custom model for hierarchical document chunking. Store the chunked data in Amazon S3 buckets with hierarchy metadata. Enable default S3 encryption using AWS-managed keys, and define bucket policies for access control. Deploy the solution in the required AWS Region and use S3 Transfer Acceleration for low-latency access.",
        "C": "Use Amazon Bedrock with a prebuilt model for document processing. Store the chunked data in Amazon Aurora PostgreSQL with JSONB fields for hierarchy metadata. Enable encryption using AWS KMS CMKs and enforce access control using database user roles. Deploy the solution in a single AWS Region for cost efficiency and use Amazon Global Accelerator for low-latency access.",
        "D": "Use Amazon Bedrock with a custom model for document chunking. Store the chunked data in Amazon Neptune using RDF graphs to represent hierarchy metadata. Enable encryption using AWS-managed keys and enforce access control via VPC endpoints. Deploy the solution in multiple AWS Regions to meet data residency requirements and use Amazon Route 53 latency-based routing for global access."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because it uses Amazon Bedrock with a custom model for hierarchical chunking, which preserves the document structure. Storing data in Amazon DynamoDB with hierarchy metadata ensures cost efficiency and high performance for structured queries. Amazon KMS and IAM policies provide strong encryption and granular access control. Deploying in multiple AWS Regions complies with data residency laws, and Amazon CloudFront ensures low-latency access globally. \n\nOption B fails because while S3 is suitable for storing large objects, it lacks the low-latency query performance and structured hierarchy management needed for this use case. Additionally, S3 Transfer Acceleration is not the optimal choice for document retrieval latency in this scenario. \n\nOption C fails because prebuilt models may not support the specific hierarchical chunking requirements. While Aurora PostgreSQL supports JSONB for hierarchy metadata, it is less cost-efficient and performant compared to DynamoDB for this workload. Deploying in a single Region also violates the data residency requirements. \n\nOption D fails because Amazon Neptune, while suitable for graph-based data, introduces unnecessary complexity and costs for hierarchical chunking. Using AWS-managed keys instead of customer-managed keys (CMKs) may not meet the firm's strict security requirements. Additionally, VPC endpoints alone do not address granular access control for sensitive documents.",
      "topic": "Parent-child chunking for hierarchical RAG - document structure preservation",
      "difficulty": "hard",
      "category": "bedrock",
      "aws_services": [
        "Amazon Bedrock",
        "Amazon DynamoDB",
        "Amazon KMS",
        "Amazon CloudFront"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:37:11.774421",
      "critique": {
        "overall_score": 9.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [],
        "verification_status": "APPROVED",
        "confidence": 0.95
      },
      "verification_status": "APPROVED",
      "id": 16
    },
    {
      "question": "A financial analytics company needs to deploy a machine learning model for real-time fraud detection using Amazon Bedrock. The model is expected to handle highly variable traffic, with peak throughput reaching 200 TPS (transactions per second) during monthly billing cycles and steady-state traffic averaging 50 TPS. The company is optimizing for cost savings but must ensure predictable performance during peak periods to remain compliant with financial regulatory requirements. The solution must also support a 6-month contract period to align with internal budgeting cycles. Which solution BEST meets these requirements?",
      "options": {
        "A": "Use Amazon Bedrock with provisioned throughput set to 200 TPS and a 6-month commitment for cost optimization. Use the `UpdateProvisionedModel` API to monitor and adjust throughput if needed.",
        "B": "Use Amazon Bedrock with provisioned throughput set to 100 TPS and a 6-month commitment, and handle peak traffic using the `ScaleProvisionedThroughput` API for dynamic scaling.",
        "C": "Use Amazon Bedrock with provisioned throughput set to 200 TPS and a 1-month commitment to allow flexibility in scaling as traffic changes across billing cycles.",
        "D": "Use Amazon Bedrock with provisioned throughput set to 50 TPS and a 6-month commitment for cost savings, and rely on burstable capacity during peak periods."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because setting provisioned throughput to 200 TPS with a 6-month commitment provides predictable performance during peak periods while aligning with the company's cost optimization goals and budget cycles. The `UpdateProvisionedModel` API ensures flexibility in case adjustments are required. Option B fails because setting throughput to 100 TPS does not meet the peak demand of 200 TPS, risking non-compliance during high-traffic periods. Option C fails because a 1-month commitment does not align with the company's 6-month budgeting cycle, leading to potential cost inefficiencies. Option D fails because setting throughput to 50 TPS is insufficient to handle peak loads, and Amazon Bedrock does not support burstable capacity for provisioned throughput.",
      "topic": "Provisioned Throughput - 1 month vs 6 month commitment",
      "difficulty": "medium",
      "category": "bedrock",
      "aws_services": [
        "Amazon Bedrock"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/bedrock/latest/userguide/prov-throughput.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:37:32.033836",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 17
    },
    {
      "question": "A healthcare analytics company uses Amazon Bedrock to generate insights from large volumes of patient data using generative AI models provided by third-party providers. They need to implement a throttling and quota management strategy to ensure the following requirements are met: (1) compliance with HIPAA regulations, (2) cost-effectiveness for varying workloads, (3) high availability during peak traffic hours, and (4) protection against exceeding API quotas that could disrupt critical services. They are using the `InvokeModel` API for real-time predictions and must manage request limits across multiple accounts with shared budgets while maintaining encryption at rest and in transit. Which solution BEST meets these requirements?",
      "options": {
        "A": "Implement Amazon Bedrock quotas using Service Quotas to centrally manage API limits, use AWS Budgets with cost alerts for multi-account monitoring, configure `InvokeModel` requests with a Lambda function to throttle based on utilization metrics, and enable KMS-based encryption for data at rest and in transit.",
        "B": "Use Amazon Bedrock quotas with API Gateway throttling to limit `InvokeModel` requests, configure AWS Budgets to alert on cost thresholds across accounts, and apply SSE-S3 encryption for data storage and TLS for transit encryption.",
        "C": "Set up individual quotas per account directly in Amazon Bedrock, use AWS Cost Explorer for budget monitoring, configure `InvokeModel` requests via Step Functions with rate limiting, and enable KMS-based encryption for data at rest and in transit.",
        "D": "Use Service Quotas to manage API limits centrally for Amazon Bedrock, implement DynamoDB as a request tracking system to throttle `InvokeModel` requests, configure AWS Budgets for account-level cost alerts, and apply customer-managed KMS encryption keys for compliance."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because it uses Service Quotas to centrally manage API limits across accounts, AWS Budgets for cost monitoring, Lambda for real-time throttling, and KMS encryption to meet HIPAA compliance. Option B fails because SSE-S3 encryption does not provide customer-managed key control required for HIPAA compliance. Option C fails because Step Functions introduce latency unsuitable for real-time predictions. Option D fails because DynamoDB is unnecessary for request tracking when Service Quotas and Lambda already handle throttling efficiently.",
      "topic": "Throttling and quota management strategies",
      "difficulty": "medium",
      "category": "bedrock",
      "aws_services": [
        "Amazon Bedrock",
        "AWS Service Quotas",
        "AWS Lambda",
        "AWS Budgets"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/bedrock/latest/userguide/quotas.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:37:45.624586",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 18
    },
    {
      "question": "A financial services company is using Amazon Bedrock to integrate generative AI models for customer-facing chatbots. These models need to meet strict performance SLAs, ensuring invocation latency does not exceed 250ms for 99% of requests, while also optimizing costs. The company requires real-time visibility into the InvocationLatency metric to identify performance bottlenecks immediately. They must ensure this solution complies with internal security policies, which mandate encryption of all data at rest and in transit. Additionally, the compliance team requires that logs and metrics are retained for a minimum of 90 days for auditing purposes, while minimizing storage costs. The company uses AWS Organizations with consolidated billing and already employs CloudWatch Logs and Metrics for monitoring other AWS services. Which solution BEST meets these requirements?",
      "options": {
        "A": "Use Amazon CloudWatch to monitor the InvocationLatency metric from the Bedrock model endpoint with a high-resolution custom alarm set to 250ms. Enable CloudWatch Logs Insights for real-time log querying, encrypt logs with KMS, and set a log retention policy of 90 days. Use CloudWatch S3 export for long-term storage of logs beyond 90 days, with S3 bucket encryption enabled.",
        "B": "Use Amazon CloudWatch to monitor InvocationLatency using a standard-resolution alarm set to 250ms. Enable CloudWatch Logs Insights for querying, encrypt logs with KMS, and set a log retention policy of 90 days. Export logs to S3 without enabling bucket encryption for cost optimization. Use an AWS Lambda function to automatically archive logs older than 90 days to Glacier Deep Archive.",
        "C": "Use Amazon CloudWatch to monitor InvocationLatency with a high-resolution alarm set to 250ms. Enable CloudWatch Logs Insights for querying and set a log retention policy of 90 days. Store logs directly in S3 with server-side encryption enabled, and configure Amazon S3 Lifecycle policies to move logs older than 90 days to Glacier Deep Archive.",
        "D": "Use Amazon CloudWatch to monitor InvocationLatency with a high-resolution alarm set to 250ms. Configure CloudWatch Logs Insights for enhanced querying, encrypt logs with KMS, and set a log retention policy of 90 days. Use the CloudWatch Logs archive feature to store logs beyond 90 days, enabling default server-side encryption for compliance."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because it uses a high-resolution alarm to meet the SLA requirement for 250ms latency monitoring and enables long-term, cost-effective log storage in encrypted S3 buckets, satisfying both security and compliance needs. Option B fails because it uses a standard-resolution alarm, which is insufficient for real-time monitoring of SLA compliance. Additionally, exporting logs to an unencrypted S3 bucket violates the company's encryption policy. Option C fails because it bypasses CloudWatch Logs retention policies, potentially increasing operational complexity and costs for managing raw logs in S3. Option D fails because the CloudWatch Logs archive feature does not allow customizable lifecycle management or cost optimization for long-term storage beyond 90 days.",
      "topic": "CloudWatch metrics - InvocationLatency monitoring",
      "difficulty": "medium",
      "category": "bedrock",
      "aws_services": [
        "Amazon CloudWatch",
        "AWS KMS",
        "Amazon S3"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Metrics.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:38:02.338096",
      "critique": {
        "overall_score": 8.5,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [
          "The explanation for Option C could be clarified further to emphasize why bypassing CloudWatch Logs retention policies increases operational complexity and costs. While the reasoning is valid, it could be more detailed for learners.",
          "The explanation for Option D could elaborate on why the CloudWatch Logs archive feature lacks lifecycle management and cost optimization compared to S3 lifecycle policies."
        ],
        "verification_status": "APPROVED",
        "confidence": 0.95
      },
      "verification_status": "APPROVED",
      "id": 19
    },
    {
      "question": "A financial services company needs to build a custom NLP model for analyzing legal contracts to extract key clauses and compliance-related information. The model must handle domain-specific vocabulary and legal nuances effectively, while adhering to strict data privacy regulations, including PII protection. They have chosen Amazon Bedrock for its serverless foundation and ability to integrate pre-trained foundation models (FMs) from leading providers. The company has access to a large legal dataset stored in Amazon S3 but needs to minimize costs while achieving high inference accuracy. Security requirements mandate encryption at rest and in transit, and compliance with GDPR. The team is debating whether to fine-tune an existing FM available in Bedrock or use continued pre-training on the legal dataset. Which solution BEST meets these requirements?",
      "options": {
        "A": "Use Amazon Bedrock to fine-tune a foundation model from a supported provider, specifying the legal dataset stored in Amazon S3 as the training input. Configure S3 bucket policies for encryption and PII redaction, and use the Bedrock InvokeModel API to validate inference accuracy post-fine-tuning.",
        "B": "Use Amazon Bedrock for continued pre-training of a foundation model from a supported provider, specifying the legal dataset stored in Amazon S3. Configure S3 bucket policies for encryption and PII redaction, and use the Bedrock InvokeModel API to validate inference accuracy post-pre-training. Choose pre-training to better align the model with domain-specific vocabulary.",
        "C": "Use Amazon Bedrock to fine-tune a foundation model from a supported provider, but preprocess the legal dataset using AWS Glue to remove PII before uploading to Amazon S3. Ensure encryption is applied at rest and in transit. Validate inference accuracy using the Bedrock InvokeModelAsync API to meet compliance and performance standards.",
        "D": "Use Amazon Bedrock for continued pre-training of a foundation model from a supported provider, but preprocess the legal dataset using AWS Glue for PII redaction before uploading to Amazon S3. Configure encryption for S3 at rest and in transit, and validate inference accuracy using the Bedrock InvokeModelAsync API to ensure compliance."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because fine-tuning allows the model to specialize in specific tasks, such as extracting legal clauses, without the computational expense and extended training time of continued pre-training. It aligns with the company's cost minimization goal while leveraging their legal dataset. Using the Bedrock InvokeModel API ensures real-time inference validation post-tuning. Option B fails because continued pre-training, while useful for domain adaptation, typically requires significantly more training resources, increasing costs unnecessarily for this task. Option C fails because preprocessing with AWS Glue adds avoidable complexity and cost for PII redaction, as Bedrock and S3 already support encryption and compliance configurations. Option D fails because continued pre-training would still introduce higher costs compared to fine-tuning, and the Bedrock InvokeModelAsync API is intended for batch processing, which does not align with real-time validation as required.",
      "topic": "Fine-tuning vs continued pre-training decision criteria",
      "difficulty": "hard",
      "category": "bedrock",
      "aws_services": [
        "Amazon Bedrock",
        "Amazon S3",
        "AWS Glue"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/bedrock/latest/userguide/custom-models.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:38:19.539810",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 20
    },
    {
      "question": "A conversational AI company is using Amazon Bedrock to host a generative AI model for customer support. The system must respond to high-frequency, low-latency requests while optimizing costs. Prompt caching is required to reduce latency for repeated queries. The company operates in a regulated industry, requiring encryption for sensitive cached data at rest and in transit. Additionally, cost constraints dictate minimizing storage expenses and avoiding over-provisioning. Which solution BEST meets these requirements?",
      "options": {
        "A": "Use Amazon Bedrock with an Amazon DynamoDB Global Table for caching prompts. Enable DynamoDB encryption at rest and use AWS KMS for key management. Configure DynamoDB On-Demand mode to optimize costs for unpredictable traffic patterns. Use the `BatchGetItem` API for low-latency retrieval and configure VPC endpoints for secure communication.",
        "B": "Use Amazon Bedrock with Amazon ElastiCache for Redis. Enable encryption at rest using AWS KMS keys and configure TLS for data in transit. Use the `GET` API for prompt retrieval. Configure cluster mode enabled with auto-scaling based on memory utilization to avoid over-provisioning.",
        "C": "Use Amazon Bedrock with Amazon DynamoDB Standard Table for caching prompts. Enable DynamoDB encryption at rest using AWS KMS keys. Use the `Query` API for retrieval and configure provisioned capacity mode with auto-scaling based on request rates. Use VPC endpoints for secure communication.",
        "D": "Use Amazon Bedrock with Amazon ElastiCache for Redis. Enable encryption at rest with Redis AUTH and configure TLS for data in transit. Use the `MGET` API for batch prompt retrieval. Configure a fixed-size cluster to control costs and ensure predictable performance."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because DynamoDB Global Tables provide multi-region availability and low-latency access, critical for high-frequency requests. Encryption at rest and transit with AWS KMS and VPC endpoints ensure compliance with security requirements. On-Demand mode minimizes costs for unpredictable traffic patterns, while `BatchGetItem` ensures efficient retrieval for multiple prompts. Option B fails because ElastiCache for Redis does not support batch retrieval as efficiently as DynamoDB for this use case, and auto-scaling based solely on memory utilization may lead to over-provisioning. Option C fails because provisioned capacity mode could lead to higher costs under traffic spikes, and the `Query` API is less efficient for batch retrieval compared to `BatchGetItem`. Option D fails because Redis AUTH is less robust compared to AWS KMS for encryption, and a fixed-size cluster does not adapt to variable traffic patterns, potentially compromising cost optimization.",
      "topic": "Prompt caching - latency and cost optimization",
      "difficulty": "medium",
      "category": "bedrock",
      "aws_services": [
        "Amazon Bedrock",
        "Amazon DynamoDB",
        "Amazon ElastiCache"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/bedrock/latest/userguide/inference.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:38:32.787935",
      "critique": {
        "overall_score": 9.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [
          "While the explanation for Option A is accurate, the question could clarify why DynamoDB's `BatchGetItem` API is more efficient for batch retrieval compared to Redis's `MGET` API, as this might not be immediately obvious to all candidates."
        ],
        "verification_status": "APPROVED",
        "confidence": 0.95
      },
      "verification_status": "APPROVED",
      "id": 21
    },
    {
      "question": "A global e-commerce company wants to implement an advanced multi-query retrieval system for its product search functionality using Amazon Bedrock. The system must support query expansion and reformulation to improve search relevance and customer experience. The company has the following constraints: \n\n1. All customer data used in the system must remain encrypted at rest and in transit to meet PCI DSS compliance requirements.\n2. The solution should minimize latency while processing multiple complex queries per second during peak traffic (up to 50,000 queries per second globally).\n3. The company wants to keep costs manageable by leveraging serverless architectures where possible, but without sacrificing performance.\n4. Query expansion should leverage a pre-trained LLM (Large Language Model) for semantic understanding, but the ability to fine-tune the model for product-specific terminology is required.\n\nWhich solution BEST meets these requirements?",
      "options": {
        "A": "Use Amazon Bedrock with a pre-trained Titan model. Leverage the Bedrock API for query expansion and reformulation, using the `StartModelCustomization` API to fine-tune the model for product-specific terminology. Deploy an Amazon DynamoDB global table for low-latency metadata storage, and encrypt all data using AWS KMS. Use AWS Lambda for serverless compute to integrate with the query pipeline, ensuring all network communication uses TLS.",
        "B": "Use Amazon Bedrock with a pre-trained Jurassic-2 model. Utilize the Bedrock API for query expansion and reformulation, using the `CreateModelCustomizationJob` API to fine-tune the model for product-specific terminology. Deploy Amazon Aurora Global Database for metadata storage to achieve low-latency performance, encrypting all data with AWS KMS. Use AWS Fargate for scalable compute in the query pipeline, ensuring all network communication uses TLS.",
        "C": "Use Amazon Bedrock with a pre-trained Titan model. Leverage the Bedrock API for query expansion but skip model fine-tuning to save costs. Deploy an Amazon ElastiCache cluster for low-latency metadata storage, and encrypt all data with AWS KMS. Use AWS Lambda for serverless compute in the query pipeline, ensuring all network communication uses TLS.",
        "D": "Use Amazon Bedrock with a pre-trained Claude model. Utilize the Bedrock API for query expansion and create a separate API layer for query reformulation using Amazon SageMaker hosting endpoints. Deploy Amazon DynamoDB global tables for metadata storage, encrypting all data using AWS KMS. Use AWS Lambda for serverless compute in the query pipeline, ensuring all network communication uses TLS."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because it uses Amazon Bedrock with a pre-trained Titan model, which supports fine-tuning via the `StartModelCustomization` API, meeting the requirement for tailoring the model to product-specific terminology. The use of DynamoDB global tables ensures low-latency metadata storage, and AWS Lambda provides a cost-effective serverless compute option that aligns with the company's cost and performance requirements. All data is encrypted with AWS KMS, ensuring compliance with PCI DSS.\n\nOption B fails because the `CreateModelCustomizationJob` API does not exist; the correct API for fine-tuning in Bedrock is `StartModelCustomization`. Additionally, while Aurora Global Database provides low-latency global access, it is more expensive than DynamoDB for the given scale, making it less cost-efficient.\n\nOption C fails because it skips model fine-tuning, which is a critical requirement for handling product-specific terminology. Additionally, while ElastiCache provides low-latency caching, it is not ideal for metadata storage in this scenario due to its caching nature rather than long-term storage.\n\nOption D fails because using a pre-trained Claude model and separate SageMaker endpoints for query reformulation introduces unnecessary complexity and latency. Additionally, the Claude model may not provide optimal performance for this use case compared to Titan.",
      "topic": "Multi-query retrieval - query expansion and reformulation strategies",
      "difficulty": "hard",
      "category": "bedrock",
      "aws_services": [
        "Amazon Bedrock",
        "Amazon DynamoDB",
        "AWS Lambda",
        "AWS KMS",
        "Amazon Aurora"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:38:58.415563",
      "critique": {
        "overall_score": 8.5,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [
          "The question assumes the reader knows that Titan models support fine-tuning and that Claude models do not, but this is not explicitly stated in the question or options. While this is accurate based on the provided AWS facts, it could confuse less experienced candidates."
        ],
        "verification_status": "APPROVED",
        "confidence": 0.95
      },
      "verification_status": "APPROVED",
      "id": 22
    },
    {
      "question": "A financial services company is building a generative AI-based customer support chatbot using Amazon Bedrock. To ensure compliance with regulatory requirements, they need to enforce strict guardrails, preventing the chatbot from generating sensitive financial predictions or discussing denied topics, such as investment advice or stock market trends. The company's compliance team requires custom topic policies to be implemented that block specific phrases and ensure the model adheres to their guidelines. Additionally, the solution must minimize operational overhead and maintain predictable costs while providing high availability for global users. Which solution BEST meets these requirements?",
      "options": {
        "A": "Use Amazon Bedrock's custom guardrails with a denied topic policy specified in JSON format. Configure the policy to block specific sensitive phrases and topics. Deploy the model using Amazon Bedrock with an auto-scaling endpoint and enable logging to Amazon CloudWatch for monitoring compliance.",
        "B": "Use Amazon Bedrock's managed guardrails with a pre-built denied topics policy. Configure the endpoint with an auto-scaling configuration and enable logging to Amazon S3 using AWS CloudTrail for monitoring compliance.",
        "C": "Use Amazon Bedrock's custom guardrails with a denied topic policy written in YAML format. Configure the guardrails to block sensitive phrases and deploy the model on a provisioned endpoint to minimize latency. Enable logging to Amazon CloudWatch to monitor compliance.",
        "D": "Use Amazon Bedrock's managed guardrails with a JSON-based denied topic policy. Deploy the model using an Amazon ECS Fargate container for cost efficiency. Configure Amazon CloudTrail for compliance logging and monitoring."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because Amazon Bedrock supports custom guardrails with JSON-based denied topic policies, allowing precise control over sensitive content. Enabling auto-scaling ensures high availability while maintaining cost predictability, and CloudWatch logging provides real-time compliance monitoring. Option B is incorrect because managed guardrails offer pre-built configurations that may not meet the company's custom compliance requirements. Option C is incorrect because Bedrock does not support YAML format for denied topic policies, and provisioned endpoints may lead to unnecessary cost increases without significant performance benefits. Option D is incorrect because deploying the model on Amazon ECS Fargate would require additional custom implementations and does not leverage Bedrock's native capabilities, increasing operational overhead.",
      "topic": "Guardrails denied topics - custom topic policies",
      "difficulty": "medium",
      "category": "bedrock",
      "aws_services": [
        "Amazon Bedrock",
        "Amazon CloudWatch",
        "AWS CloudTrail"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/bedrock/latest/userguide/guardrails.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:39:13.589426",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 23
    },
    {
      "question": "A financial services company is building a document processing pipeline using Amazon Bedrock to extract and summarize information from regulatory compliance documents, which can be hundreds of pages long. They are leveraging a foundation model for semantic chunking and need to configure the breakpoint threshold and buffer size to optimize both performance and cost. The company must ensure that:\n\n1. The solution remains compliant with GDPR regulations by minimizing unnecessary data retention.\n2. Latency is kept below 500ms for user-facing queries.\n3. Chunking minimizes token usage to control costs, as the foundation model charges per token processed. \n4. The output quality is not degraded by improper chunking (e.g., splitting sentences or paragraphs incorrectly).\n\nThe foundation model's API allows tuning of the `breakpoint_threshold` (controls where chunks are split based on semantic coherence, value range 0.01.0) and `buffer_size` (how much additional context is included before or after a chunk). Which solution BEST meets these requirements?",
      "options": {
        "A": "Set `breakpoint_threshold` to 0.85 for high semantic coherence and `buffer_size` to 256 tokens to ensure adequate context is retained for summarization while minimizing unnecessary token usage.",
        "B": "Set `breakpoint_threshold` to 0.95 for maximum semantic coherence and `buffer_size` to 512 tokens to minimize the risk of losing context across chunks, ensuring high output quality.",
        "C": "Set `breakpoint_threshold` to 0.75 for moderate semantic coherence and `buffer_size` to 128 tokens to reduce latency and processing costs, balancing performance and cost efficiency.",
        "D": "Set `breakpoint_threshold` to 0.90 for strong semantic coherence and `buffer_size` to 64 tokens to minimize token usage while maintaining GDPR compliance by reducing the data retained in memory."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because setting `breakpoint_threshold` to 0.85 strikes a balance between semantic coherence and reasonable chunk sizes, while a `buffer_size` of 256 tokens ensures sufficient context is retained without significantly increasing token usage or latency. Option B fails because a `breakpoint_threshold` of 0.95 can lead to excessively large chunks, increasing token costs and potentially exceeding the 500ms latency requirement. Option C fails because a `breakpoint_threshold` of 0.75 may not maintain the desired semantic coherence, leading to degraded summarization quality. Option D fails because a `buffer_size` of 64 tokens is insufficient for maintaining adequate context, which can negatively affect output quality.",
      "topic": "Semantic chunking configuration - breakpoint threshold and buffer size tuning",
      "difficulty": "hard",
      "category": "bedrock",
      "aws_services": [
        "Amazon Bedrock"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:39:38.602000",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 24
    },
    {
      "question": "A global e-commerce company is building a product discovery platform using Amazon Bedrock for personalized product recommendations. The platform needs to retrieve product data from a catalog of over 10 million items stored in Amazon OpenSearch Service. To comply with GDPR, customer-specific metadata (e.g., region, age group) must be applied to filter results before exposing them to the user. The company prioritizes low latency for real-time recommendations, but also needs to keep costs optimized. They are considering both pre-filtering and post-filtering approaches for metadata filtering. Additionally, the architecture must ensure compliance with GDPR by preventing sensitive metadata from being exposed unnecessarily. Which solution BEST meets these requirements?",
      "options": {
        "A": "Use Amazon OpenSearch Service with a pre-filtering approach by applying metadata filters in the query DSL using the 'filter' clause. Implement fine-grained access controls (FGAC) to restrict metadata exposure and use UltraWarm storage for cost optimization.",
        "B": "Use Amazon OpenSearch Service with a pre-filtering approach by applying metadata filters in the query DSL using the 'must' clause. Enable fine-grained access controls (FGAC) for metadata restrictions and use UltraWarm storage for cost optimization.",
        "C": "Use Amazon OpenSearch Service with a post-filtering approach by retrieving the full dataset via the 'match_all' query and applying metadata filters in your application layer. Use fine-grained access controls (FGAC) to ensure sensitive metadata is not exposed.",
        "D": "Use Amazon OpenSearch Service with a post-filtering approach where metadata filters are added in the pipeline using the '_source' parameter. Enable fine-grained access controls (FGAC) for metadata security and leverage cold storage to reduce costs."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because applying metadata filters using the 'filter' clause in the query DSL ensures that the filtering is performed at the query level, optimizing performance and maintaining low latency. Fine-grained access controls (FGAC) restrict metadata exposure, meeting GDPR compliance, and UltraWarm storage helps balance cost and performance for large datasets. Option B is incorrect because the 'must' clause is less efficient than the 'filter' clause for non-scoring metadata filtering, which can increase latency. Option C fails because post-filtering retrieves the entire dataset, leading to higher query costs and latency, and it risks non-compliance if metadata is processed in the application layer. Option D is incorrect because using the '_source' parameter for filtering metadata in the pipeline is not a supported approach for filtering at scale, and cold storage is not optimal for low-latency requirements.",
      "topic": "Metadata filtering in retrieval - pre-filter vs post-filter performance",
      "difficulty": "hard",
      "category": "bedrock",
      "aws_services": [
        "Amazon OpenSearch Service",
        "Amazon Bedrock"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/opensearch-service/latest/developerguide/query-dsl.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:39:55.706572",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 25
    },
    {
      "question": "A global e-commerce company needs to build a knowledge base to enhance its customer support operations using Amazon Bedrock. The knowledge base will store product documents and FAQs that must be frequently updated. The company requires the data ingestion pipeline to handle updates from an Amazon S3 bucket and support real-time updates for customer feedback submitted via a REST API. The following constraints must be considered: \n\n1. Cost efficiency is critical, as the knowledge base is expected to grow to tens of terabytes. \n2. Real-time updates must have a latency of no more than 1 second to support accurate responses. \n3. Regulatory compliance requires that all data updates are encrypted in transit and at rest. \n4. The system must support eventual consistency for S3-based batch updates while ensuring immediate consistency for real-time API updates.\n\nWhich solution BEST meets these requirements?",
      "options": {
        "A": "Use an Amazon S3 Event Notification to trigger an AWS Lambda function that uses the Bedrock API's `PutDocument` method for batch updates. For real-time updates, use Amazon API Gateway with a Lambda integration to call the same `PutDocument` method. Enable AWS Key Management Service (KMS) encryption for all data in transit and at rest.",
        "B": "Use an Amazon S3 Event Notification to trigger an AWS Glue ETL job that processes updates and stores them in Bedrock via the `BatchPutDocument` method. For real-time updates, use Amazon API Gateway with a direct integration to Bedrock's `PutDocument` method. Enable server-side encryption using S3-managed keys for the S3 bucket.",
        "C": "Use Amazon S3 Transfer Acceleration for batch updates to ensure faster ingestion of documents into Bedrock via the `BatchPutDocument` API. For real-time updates, integrate Amazon API Gateway with an AWS Lambda function that calls the `PutDocument` API. Enable encryption for all data using Amazon S3-SSE with a customer-provided key.",
        "D": "Use an Amazon S3 Event Notification to trigger an AWS Lambda function that calls Bedrock's `BatchPutDocument` API for batch updates. For real-time updates, use Amazon API Gateway with VPC Link to directly invoke Bedrock's `PutDocument` API. Enable encryption in transit using TLS 1.2 and encryption at rest using AWS-owned keys."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because it uses the `PutDocument` method for both batch and real-time updates, ensuring a unified approach. It also enables KMS encryption for robust security compliance and uses Lambda for cost-efficient processing. Option B is incorrect because using AWS Glue for batch updates introduces unnecessary cost and complexity for this use case. Additionally, S3-managed keys do not meet the regulatory compliance requirement for encryption. Option C is incorrect because Amazon S3 Transfer Acceleration is not relevant to triggering updates from S3 to Bedrock, and SSE with customer-provided keys does not align with the compliance requirement for centralized key management. Option D is incorrect because using AWS-owned keys for encryption does not meet the regulatory compliance requirement, and VPC Link for API Gateway adds unnecessary complexity and cost in this context.",
      "topic": "Knowledge Base data ingestion pipelines - S3 sync vs real-time updates",
      "difficulty": "medium",
      "category": "bedrock",
      "aws_services": [
        "Amazon S3",
        "AWS Lambda",
        "Amazon API Gateway",
        "Amazon Bedrock",
        "AWS KMS"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:40:22.317218",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 26
    },
    {
      "question": "A global e-commerce company is building a customer support knowledge base using Amazon Bedrock to enable intelligent search and summarization powered by generative AI models. Their knowledge base will include diverse data sources: product manuals (structured data), customer queries (unstructured data), and troubleshooting guides (long hierarchical documents). The company has strict compliance requirements to ensure sensitive customer data is not exposed to generative AI models during training or inference. Additionally, the solution must optimize performance for frequent real-time queries while ensuring costs remain manageable. Which solution BEST meets these requirements?",
      "options": {
        "A": "Use semantic chunking for customer queries and hierarchical chunking for troubleshooting guides in combination with Amazon Bedrock's generative AI models. Configure Amazon S3 with bucket policies to enforce encryption of sensitive data at rest, and enable Bedrock's private VPC endpoints for secure inference traffic. Use Amazon Comprehend for PII detection and redaction before preprocessing data.",
        "B": "Use fixed-size chunking for customer queries and hierarchical chunking for troubleshooting guides with Amazon Bedrock's generative AI models. Configure Amazon S3 with bucket policies for encryption, use AWS Key Management Service (KMS) for data encryption keys, and enable Bedrock private VPC endpoints for inference. Preprocess data using Amazon Comprehend for PII detection, but skip PII redaction to reduce preprocessing latency.",
        "C": "Use semantic chunking for all data types and integrate them with Amazon Bedrock's generative AI models. Implement AWS Glue ETL jobs to preprocess data for consistency, and enforce encryption using Amazon S3 bucket policies with default SSE-S3 encryption. Enable Bedrock private VPC endpoints for inference traffic. Skip PII detection to reduce costs and use semantic chunking for simplified preprocessing.",
        "D": "Use hierarchical chunking for long documents and fixed-size chunking for customer queries integrated with Amazon Bedrock's generative AI models. Preprocess data using AWS Lambda functions for real-time PII detection and redaction. Enforce encryption using Amazon S3 bucket policies with SSE-KMS encryption. Enable Bedrock private VPC endpoints for inference traffic, and configure Amazon CloudWatch for monitoring inference costs."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because it applies semantic chunking for unstructured customer queries and hierarchical chunking for structured, long documents, optimizing the generative AI model's performance and accuracy. It enforces compliance by integrating Amazon Comprehend for PII detection and redaction and uses Bedrock private VPC endpoints for secure inference traffic. Option B fails because skipping PII redaction does not meet compliance requirements. Option C fails because semantic chunking for all data types may degrade performance for long hierarchical documents, and skipping PII detection violates compliance. Option D fails because fixed-size chunking is less effective for unstructured customer queries, and using AWS Lambda for PII detection can increase costs for real-time processing.",
      "topic": "Knowledge Bases chunking strategies - fixed vs semantic vs hierarchical",
      "difficulty": "hard",
      "category": "bedrock",
      "aws_services": [
        "Amazon Bedrock",
        "Amazon S3",
        "Amazon Comprehend",
        "AWS Key Management Service (KMS)"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:40:39.074026",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 27
    },
    {
      "question": "A global e-commerce company is building a product recommendation engine to improve customer engagement. They need to generate high-quality semantic embeddings for millions of product descriptions and customer reviews. The embeddings will be used to calculate similarity scores for personalized product recommendations. The company requires: \n\n- High accuracy for semantic similarity on multilingual text (English, French, Spanish, German).\n- Cost-efficiency, as the service must scale to millions of embeddings monthly.\n- Compliance with GDPR for customer data, ensuring embeddings do not store any personally identifiable information (PII).\n- Low latency for real-time recommendation generation. \n\nThe company is evaluating Amazon Bedrock with Titan Text Embeddings V2 vs. Cohere Embed models. Titan Embeddings V2 offers 768 dimensions, while Cohere Embed offers 1024 dimensions. Titan is priced at $0.0004 per input token, while Cohere Embed costs $0.0006 per input token. Which solution BEST meets these requirements?",
      "options": {
        "A": "Use Amazon Bedrock with the Titan Text Embeddings V2 model for generating 768-dimensional embeddings. Configure the input preprocessing pipeline to remove PII using Amazon Comprehend, and store embeddings in Amazon DynamoDB. Use cosine similarity for real-time recommendations.",
        "B": "Use Amazon Bedrock with the Cohere Embed model for generating 1024-dimensional embeddings. Preprocess input text with Amazon Comprehend for PII removal, store embeddings in Amazon S3 for batch retrieval, and use dot product similarity for recommendations.",
        "C": "Use Amazon Bedrock with the Titan Text Embeddings V2 model for generating 768-dimensional embeddings. Preprocess input text with AWS Glue for PII removal, store embeddings in Amazon S3, and use Hamming distance for similarity calculations.",
        "D": "Use Amazon Bedrock with the Cohere Embed model for generating 1024-dimensional embeddings. Configure the input preprocessing pipeline to remove PII using Amazon Comprehend, store embeddings in Amazon DynamoDB, and use cosine similarity for real-time recommendations."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because Titan Text Embeddings V2 provides cost-effective embeddings with sufficient dimensionality (768) to achieve high accuracy on multilingual data, meeting both performance and cost constraints. Preprocessing with Amazon Comprehend ensures compliance with GDPR by removing PII, and storing embeddings in Amazon DynamoDB supports low-latency real-time recommendations. Option B is incorrect because Cohere Embed, while higher in dimensionality (1024), is more expensive and unnecessary for the defined accuracy requirements. Additionally, storing embeddings in Amazon S3 does not meet the low-latency requirement. Option C fails because AWS Glue is not an optimal preprocessing tool for real-time operations, and Hamming distance is unsuitable for semantic similarity calculations. Option D fails because while it uses the Cohere Embed model and ensures compliance, its higher cost and unnecessary dimensionality make it less suitable for the company's requirements.",
      "topic": "Embedding model selection - Titan Embeddings V2 vs Cohere Embed dimensions and performance",
      "difficulty": "hard",
      "category": "bedrock",
      "aws_services": [
        "Amazon Bedrock",
        "Amazon Comprehend",
        "Amazon DynamoDB",
        "Amazon S3"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:40:55.615510",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 28
    },
    {
      "question": "A financial services company is using Amazon Bedrock to deploy and manage foundation models for their regulatory compliance use case. They currently rely on a third-party model for sentiment analysis on customer feedback but have been informed that the model will be deprecated in 60 days. The model is tightly integrated into their application through Bedrock's API. The company must migrate to a new model without disrupting their production environment. Key constraints include minimizing migration downtime, ensuring the new model meets FINRA compliance, and keeping additional costs under $500/month. The company also requires maintaining data encryption in transit and at rest as per their internal security policies. Which solution BEST meets these requirements?",
      "options": {
        "A": "Use Bedrock to deploy a new FINRA-compliant foundation model from Amazon's curated model catalog, update the application to use the new endpoint, and use Bedrock's 'Model Endpoint Versioning' feature to test and transition traffic gradually.",
        "B": "Use Bedrock to deploy a new FINRA-compliant foundation model from a third-party provider, update the application to use the new endpoint, and immediately redirect all traffic to the new model without versioning to minimize latency.",
        "C": "Use Bedrock to deploy an Amazon Titan model pre-trained for sentiment analysis, configure encryption using AWS KMS, and implement traffic splitting via 'Inference Resolvers' to gradually migrate traffic to the new model.",
        "D": "Use Bedrock to deploy a new foundation model from Amazon's catalog, configure encryption using a custom key managed outside of AWS, and use 'Model Endpoint Testing' to validate the new model before manually switching over all traffic."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because it uses Bedrock's 'Model Endpoint Versioning' to ensure a seamless and controlled migration, meets FINRA compliance by using a curated model, and avoids immediate traffic redirection, which minimizes downtime and potential disruptions. Option B fails because it redirects all traffic immediately without testing, which risks production downtime and non-compliance. Option C fails because 'Inference Resolvers' is not a feature supported in Bedrock; traffic splitting is achieved through versioning. Option D fails because using a custom key managed outside AWS does not integrate seamlessly with Bedrock's encryption mechanisms and adds unnecessary complexity.",
      "topic": "Model lifecycle - deprecation and migration",
      "difficulty": "hard",
      "category": "bedrock",
      "aws_services": [
        "Amazon Bedrock",
        "AWS KMS"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:41:25.111280",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [
          {
            "issue": "The question mentions 'Model Endpoint Versioning' as a feature of Bedrock. While Bedrock supports endpoint management, the specific term 'Model Endpoint Versioning' is not explicitly documented as a feature of Bedrock. This could lead to confusion if the candidate is looking for exact terminology in AWS documentation."
          },
          {
            "issue": "The explanation for Option C correctly identifies that 'Inference Resolvers' is not a feature of Bedrock, but it could be clearer that traffic splitting in Bedrock is typically handled through endpoint management or external routing mechanisms."
          }
        ],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 29
    },
    {
      "question": "A financial services company is building a customer-facing chatbot for secure financial advice using Amazon Bedrock Flows. The chatbot must integrate with an internal database via an API to retrieve customer financial data, use generative AI for personalized financial recommendations, and ensure compliance with strict regional data residency requirements. The organization needs to minimize costs without compromising latency, as this service will handle high traffic during business hours. They also require end-to-end encryption for all data in transit and at rest. Which solution BEST meets these requirements?",
      "options": {
        "A": "Design a Bedrock Flow that uses a foundation model deployed on a Bedrock-hosted endpoint for inference. Integrate the flow with the internal API using a Lambda function configured with VPC access for secure database queries. Enable encryption for all API calls using HTTPS and configure the Bedrock Flow to store intermediate results in an encrypted Amazon S3 bucket (with server-side encryption using KMS). Deploy the Bedrock Flow in a region that complies with data residency requirements.",
        "B": "Design a Bedrock Flow that uses a foundation model deployed on a Bedrock-hosted endpoint for inference. Integrate the flow with the internal API using a Lambda function configured with VPC access for secure database queries. Enable encryption for all API calls using HTTPS and configure the Bedrock Flow to store intermediate results in an Amazon S3 bucket with default encryption settings. Deploy the Bedrock Flow in a region that complies with data residency requirements.",
        "C": "Design a Bedrock Flow that uses a foundation model deployed on an Amazon SageMaker endpoint for inference. Integrate the flow with the internal API using a Lambda function configured with VPC access for secure database queries. Enable encryption for all API calls using HTTPS and configure the flow to store intermediate results in an encrypted Amazon DynamoDB table. Deploy the Bedrock Flow in a region that complies with data residency requirements.",
        "D": "Design a Bedrock Flow that uses a foundation model deployed on a Bedrock-hosted endpoint for inference. Integrate the flow with the internal API using an API Gateway endpoint configured for VPC access to query the database securely. Enable encryption for all API calls using HTTPS and configure the Bedrock Flow to store intermediate results in an encrypted Amazon S3 bucket (with server-side encryption using KMS). Deploy the Bedrock Flow in the region closest to the majority of users to minimize latency."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because it ensures compliance with data residency requirements by deploying the Bedrock Flow in a compliant region. It also minimizes costs by using a Bedrock-hosted endpoint for inference and secures intermediate results with server-side encryption using KMS on Amazon S3. Option B fails because it uses default encryption settings for S3, which do not guarantee compliance with strict encryption requirements. Option C fails because it uses Amazon SageMaker for inference, which incurs additional costs compared to Bedrock-hosted endpoints and does not align with the requirement to use Amazon Bedrock. Option D fails because it prioritizes latency by deploying in the nearest region without explicitly addressing data residency compliance.",
      "topic": "Bedrock Flows - visual workflow orchestration",
      "difficulty": "medium",
      "category": "bedrock",
      "aws_services": [
        "Amazon Bedrock",
        "AWS Lambda",
        "Amazon S3",
        "AWS Key Management Service (KMS)"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/bedrock/latest/userguide/agents.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:41:46.442627",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 30
    },
    {
      "question": "A retail analytics company is using Amazon Bedrock to integrate generative AI models into their customer insights platform. They need to allow their data scientists to invoke Bedrock APIs for model inference while ensuring the principle of least privilege is followed. The company has the following constraints:\n\n1. Only specific Bedrock models (e.g., Anthropic Claude and Amazon Titan) should be accessible.\n2. Data scientists should not have permission to create, delete, or manage Bedrock resources, such as custom models or endpoints.\n3. API usage should be restricted to a specific project tagged as 'project=retail-analytics' to meet compliance requirements.\n4. The IAM policy must minimize the risk of accidental overuse to control costs, and all actions should be logged for audit purposes.\n\nWhich IAM policy BEST meets these requirements?",
      "options": {
        "A": "Create an IAM policy that allows `bedrock:InvokeModel` for specific models (`model-id` for Anthropic Claude and Amazon Titan) and includes a `Condition` block with `StringEquals` to restrict `aws:RequestTag/project` to 'retail-analytics'. Attach this policy to an IAM role assumed by the data scientists.",
        "B": "Create an IAM policy that allows `bedrock:InvokeModel` for specific models and includes a `Condition` block with `StringLike` to restrict `aws:ResourceTag/project` to 'retail-analytics'. Attach this policy to an IAM role assumed by the data scientists.",
        "C": "Create an IAM policy that allows `bedrock:*` actions for specific models and includes a `Condition` block with `StringEquals` to restrict `aws:ResourceTag/project` to 'retail-analytics'. Attach this policy to an IAM role assumed by the data scientists.",
        "D": "Create an IAM policy that allows `bedrock:InvokeModel` for all models and includes a `Condition` block with `StringEquals` to restrict `aws:RequestTag/project` to 'retail-analytics'. Attach this policy to an IAM role assumed by the data scientists."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because it adheres to the principle of least privilege by allowing only the `bedrock:InvokeModel` action for specific models and uses `aws:RequestTag/project` to enforce compliance with the specified project tag. This ensures that data scientists cannot perform other actions like creating or managing resources, and the policy is scoped to only the required models and project.\n\nOption B is incorrect because it uses `aws:ResourceTag/project` instead of `aws:RequestTag/project`, which is not valid for actions like `bedrock:InvokeModel` that operate on request-based tags rather than resource-based tags.\n\nOption C is incorrect because it grants `bedrock:*` permissions, which is overly permissive and violates the principle of least privilege.\n\nOption D is incorrect because it allows `bedrock:InvokeModel` for all models, which does not restrict access to specific models as required.\n\nThe subtle difference between `aws:RequestTag` and `aws:ResourceTag` is critical for understanding the correct solution.",
      "topic": "Bedrock IAM policies - least privilege patterns",
      "difficulty": "medium",
      "category": "bedrock",
      "aws_services": [
        "Amazon Bedrock",
        "IAM"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/bedrock/latest/userguide/security.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:42:09.657801",
      "critique": {
        "overall_score": 9.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [],
        "verification_status": "APPROVED",
        "confidence": 0.95
      },
      "verification_status": "APPROVED",
      "id": 31
    },
    {
      "question": "An e-commerce company uses Amazon Bedrock to integrate generative AI capabilities into their customer support chatbot. The chatbot is powered by a foundation model (FM) accessed through Bedrock APIs and is designed to handle inquiries like order tracking, product recommendations, and returns. The FM processes prompts with a maximum token limit of 4,096 tokens, and the company has observed a significant increase in inference costs due to complex, verbose prompts exceeding this limit. The company wants to optimize token costs while maintaining high-quality responses. Additionally, they must comply with data privacy regulations (e.g., GDPR) requiring minimal retention of customer data. Performance must remain consistent with a latency threshold under 200ms per API call. Which solution BEST meets these requirements?",
      "options": {
        "A": "Implement prompt compression using a pre-processing Lambda function to summarize verbose prompts before sending them to the Bedrock API, while enabling encryption in transit with TLS 1.2 for compliance. Use the Bedrock `InvokeModel` API with the `maxTokenCount` parameter set to 4,096 to prevent truncation.",
        "B": "Use context pruning to truncate older conversation history dynamically before sending prompts to Bedrock. Configure the Bedrock `InvokeModel` API with the `contextSize` parameter set to 3,000 tokens, ensuring compliance with token limits while maintaining low latency.",
        "C": "Combine prompt compression and context pruning by first summarizing prompts using a local model deployed on ECS Fargate and applying context pruning to maintain only the last 2,000 tokens. Set the Bedrock `maxTokenCount` parameter to 2,048 for cost savings and invoke the `InvokeModel` API with the `outputTruncation` parameter enabled.",
        "D": "Enable prompt compression using a third-party pre-trained summarization model hosted on SageMaker, then send the compressed prompts to Bedrock with the `InvokeModel` API. Set the `maxTokenCount` parameter to 1,024 and enable the `contextPruning` parameter to remove context beyond the token limit dynamically."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because it uses a pre-processing Lambda function for prompt compression, ensuring verbose prompts are summarized before reaching the Bedrock API, which minimizes token usage and maintains compliance with data privacy regulations. TLS 1.2 ensures encryption in transit. The `maxTokenCount` parameter is correctly set to the FM's maximum token limit (4,096), avoiding truncation and ensuring high-quality responses. \n\nOption B fails because the `contextSize` parameter is invalid; Bedrock does not support such a parameter. Additionally, solely relying on context pruning may degrade response quality by removing important conversational context.\n\nOption C fails because setting the `maxTokenCount` to 2,048 unnecessarily limits the FM's capabilities, reducing response quality. The use of a local model on ECS Fargate for summarization adds unnecessary complexity and latency.\n\nOption D fails because setting the `maxTokenCount` to 1,024 severely limits the model's ability to generate complete responses. Additionally, the `contextPruning` parameter does not exist in Bedrock APIs, making this configuration invalid.",
      "topic": "Token cost optimization - prompt compression vs context pruning techniques",
      "difficulty": "hard",
      "category": "bedrock",
      "aws_services": [
        "Amazon Bedrock",
        "AWS Lambda",
        "Amazon SageMaker",
        "Amazon ECS"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/bedrock/latest/userguide/inference.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:42:42.062172",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 32
    },
    {
      "question": "A financial services company is building a chatbot using Amazon Bedrock to provide real-time responses to customer queries about their portfolio performance and market trends. Due to high traffic during market hours, the chatbot must handle thousands of concurrent users with low latency. The chatbot uses a foundation model for natural language processing, and the company wants to minimize response delays by streaming partial results back to users as the model generates them. The solution must comply with regulatory requirements to log all customer interactions, ensure encryption in transit and at rest, and stay within a strict monthly budget. Which solution BEST meets these requirements?",
      "options": {
        "A": "Use the InvokeModelWithResponseStream API in Amazon Bedrock with a foundation model that supports streaming responses. Configure the API call to enable streaming by setting the 'stream' parameter to 'true'. Use Amazon Kinesis Data Firehose to capture and store streaming logs into an encrypted Amazon S3 bucket, ensuring compliance with regulatory requirements.",
        "B": "Use the InvokeModelWithResponseStream API in Amazon Bedrock with a foundation model that supports streaming responses. Configure the API call by setting the 'stream' parameter to 'false' and enable response buffering to optimize performance. Use Amazon CloudWatch Logs to retain interaction logs and configure AWS Key Management Service (KMS) for encryption.",
        "C": "Use the InvokeModel API in Amazon Bedrock with a foundation model that supports batch processing. Configure the API call to aggregate responses and return them in chunks. Use Amazon DynamoDB Streams to log interactions, encrypting data with AWS Key Management Service (KMS).",
        "D": "Use the InvokeModelWithResponseStream API in Amazon Bedrock with a foundation model that supports streaming responses. Configure the API call to enable streaming by setting the 'stream' parameter to 'true'. Use Amazon S3 directly for log storage, ensuring bucket encryption with default AES-256."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because it uses the InvokeModelWithResponseStream API with streaming enabled (via the 'stream' parameter set to 'true') and integrates Amazon Kinesis Data Firehose to capture and store logs in an encrypted S3 bucket, meeting both performance and compliance requirements. Option B fails because it disables streaming ('stream' set to 'false'), which increases latency and does not support partial responses. Option C fails because the InvokeModel API does not support streaming responses; it only supports batch processing, which does not meet the low-latency requirement. Option D fails because directly storing logs in S3 without a streaming service like Kinesis Firehose may introduce latency and lacks the flexibility needed for high-throughput logging during peak traffic.",
      "topic": "Streaming responses - InvokeModelWithResponseStream",
      "difficulty": "medium",
      "category": "bedrock",
      "aws_services": [
        "Amazon Bedrock",
        "Amazon Kinesis Data Firehose",
        "Amazon S3"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/bedrock/latest/userguide/inference.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:43:04.647941",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 33
    },
    {
      "question": "A global healthcare analytics company is building a Retrieval-Augmented Generation (RAG) pipeline to provide accurate, real-time responses to medical professionals' queries. They utilize Amazon Bedrock to evaluate RAG performance using RAGAS (Retrieval-Augmented Generation Assessment Scores) metrics. The company requires that the evaluation jobs provide detailed sub-metrics such as retrieval relevancy, generation coherence, and factual accuracy. They must also ensure compliance with HIPAA regulations, minimize latency for real-time use, and keep costs within a $10,000 monthly budget. Additionally, the solution must support future scaling to accommodate 10x data growth within a year. Which solution BEST meets these requirements?",
      "options": {
        "A": "Use Amazon Bedrock model evaluation jobs with the 'ragas-v1' template for RAGAS metrics, enabling HIPAA compliance by encrypting evaluation job input/output with AWS KMS CMKs. Configure the evaluation job to process data in batches using the 'BatchSize' parameter set to 100 for optimal cost-performance balance. Store input data in an S3 bucket with S3 Object Lock enabled for compliance and use AWS Glue for scalable data preprocessing.",
        "B": "Use Amazon Bedrock model evaluation jobs with the 'ragas-v1' template, enabling encryption with AWS KMS CMKs for HIPAA compliance. Process data in batches by setting the 'BatchSize' parameter to 500 for faster throughput. Store input data in an S3 bucket with S3 Object Lock enabled for compliance and preprocess data using AWS Glue. Monitor budget using AWS Cost Anomaly Detection.",
        "C": "Use Amazon Bedrock model evaluation jobs with the 'ragas-v2' template for RAGAS metrics, encrypting input/output data using AWS KMS CMKs. Set the 'BatchSize' parameter to 50 for granular processing. Store input data in an S3 bucket with versioning enabled for compliance and preprocess data using AWS Glue. Monitor costs using AWS Budgets.",
        "D": "Use Amazon Bedrock model evaluation jobs with the 'ragas-v1' template for RAGAS metrics, encrypting input/output data using AWS KMS CMKs. Configure evaluation jobs to process data in real time by setting 'BatchSize' to 1 for the lowest latency. Store input data in an S3 bucket with S3 Object Lock enabled for compliance and preprocess data using AWS Glue. Monitor costs using AWS Cost Explorer with alerts configured."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because it balances cost, performance, and compliance by using the 'ragas-v1' template (which supports the required sub-metrics), encrypting data with AWS KMS CMKs for HIPAA compliance, and optimizing batch size to 100 for cost-performance efficiency. S3 Object Lock ensures data integrity for compliance, and AWS Glue enables scalable preprocessing for future growth.\n\nOption B fails because setting 'BatchSize' to 500 may exceed the monthly budget due to increased compute costs, and it may compromise real-time performance.\n\nOption C fails because the 'ragas-v2' template does not yet support the required detailed sub-metrics. Additionally, enabling S3 versioning instead of S3 Object Lock does not meet compliance requirements for immutability.\n\nOption D fails because setting 'BatchSize' to 1 significantly increases latency and costs, making it impractical for this use case.",
      "topic": "RAG evaluation using Bedrock model evaluation jobs - RAGAS metrics",
      "difficulty": "hard",
      "category": "bedrock",
      "aws_services": [
        "Amazon Bedrock",
        "AWS KMS",
        "S3 Object Lock",
        "AWS Glue"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:43:26.704234",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [
          "The question assumes the 'ragas-v1' template supports the required sub-metrics (retrieval relevancy, generation coherence, and factual accuracy), but this is not explicitly confirmed in the provided AWS facts. However, this assumption is reasonable for the context of the question.",
          "The explanation for Option B could clarify why a 'BatchSize' of 500 would exceed the budget, as this depends on specific cost factors not detailed in the question."
        ],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 34
    },
    {
      "question": "A media analytics company is building a personalized video recommendation system using Amazon Bedrock. They aim to retrieve the top 5 most relevant video titles for each user query by balancing relevance and diversity. The dataset contains 10 million video records, and the retrieval system must handle 50,000 queries per second. Due to compliance regulations, all data must remain encrypted at rest and in transit, and the system must prioritize low latency for user experience. The company also needs to minimize operational costs while ensuring scalability. They are evaluating different retrieval strategies, specifically focusing on the trade-offs between Top-k retrieval and Maximum Marginal Relevance (MMR). Which solution BEST meets these requirements?",
      "options": {
        "A": "Use Amazon Bedrocks semantic search API with a Top-k retrieval strategy, setting `k=5` and using a pre-trained foundation model like Amazon Titan. Configure encryption using AWS Key Management Service (KMS) for both at-rest and in-transit data. Partition the dataset to optimize latency for high query volume.",
        "B": "Use Amazon Bedrocks semantic search API with an MMR-based retrieval strategy, setting `k=5` and `lambda=0.8` for diversity control. Enable encryption using AWS Key Management Service (KMS) and configure caching using Amazon ElastiCache to reduce latency for high query volume.",
        "C": "Use Amazon Bedrocks semantic search API with a Top-k retrieval strategy, setting `k=5` and using a custom fine-tuned foundation model. Configure encryption using AWS Key Management Service (KMS) for at-rest data only. Use Amazon S3 Intelligent-Tiering to reduce storage costs for the dataset.",
        "D": "Use Amazon Bedrocks semantic search API with an MMR-based retrieval strategy, setting `k=5` and `lambda=0.5` for relevance/diversity balance. Use AWS KMS for at-rest encryption and Amazon CloudFront for query caching to improve global latency."
      },
      "correct_answer": "B",
      "explanation": "Option B is correct because it uses MMR-based retrieval to balance diversity and relevance, with `lambda=0.8` appropriately emphasizing diversity for personalized recommendations. It also ensures compliance with encryption requirements using AWS KMS and addresses latency by leveraging Amazon ElastiCache. Option A fails because Top-k does not account for diversity, which is critical for avoiding overly similar recommendations. Option C fails due to insufficient encryption (at-rest only) and the inappropriate use of S3 Intelligent-Tiering, which does not optimize query latency. Option D fails because a `lambda` value of 0.5 does not adequately balance diversity and relevance for the given use case, and Amazon CloudFront is less suited for query caching in this scenario compared to ElastiCache.",
      "topic": "Top-k vs MMR retrieval - diversity vs relevance trade-offs",
      "difficulty": "hard",
      "category": "bedrock",
      "aws_services": [
        "Amazon Bedrock",
        "AWS KMS",
        "Amazon ElastiCache",
        "Amazon CloudFront"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/bedrock/latest/userguide/semantic-search.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:43:48.216392",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [
          "The explanation for Option D states that Amazon CloudFront is less suited for query caching compared to ElastiCache. While this is generally true for low-latency, high-throughput scenarios, the question does not explicitly rule out CloudFront as a viable option, especially for global latency improvements. However, ElastiCache is indeed a better fit for the described use case.",
          "The explanation for Option C mentions insufficient encryption (at-rest only), but the question does not explicitly state that in-transit encryption is mandatory. However, given the compliance requirements, it is reasonable to assume both at-rest and in-transit encryption are needed."
        ],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 35
    },
    {
      "question": "A financial services company is deploying an AI-driven chatbot using Amazon Bedrock to provide personalized financial advice to its customers. To ensure compliance with regulatory requirements, the company must implement content guardrails to filter out inappropriate or non-compliant responses. The company needs to configure a threshold in the content filtering system to balance between stricter compliance and chatbot performance. They aim to minimize false positives (blocking valid responses) while still maintaining a high level of regulatory compliance. The solution must also account for cost efficiency, as the company expects a high volume of API calls (millions per month). Which solution BEST meets these requirements?",
      "options": {
        "A": "Configure Amazon Bedrock's content filtering guardrails with a threshold value of 0.75 using the `CreateGuardrailsConfig` API, and enable logging to Amazon CloudWatch for auditing. Ensure the 'filter_strictness' parameter is set to `MEDIUM` to balance compliance and false positives.",
        "B": "Configure Amazon Bedrock's content filtering guardrails with a threshold value of 0.85 using the `UpdateGuardrailsConfig` API, and route logs to Amazon S3 for cost-effective storage. Set the 'filter_strictness' parameter to `HIGH` to maximize compliance.",
        "C": "Configure Amazon Bedrock's content filtering guardrails with a threshold value of 0.70 using the `CreateGuardrailsConfig` API, and enable logging to Amazon CloudWatch for real-time monitoring. Use 'filter_strictness' parameter set to `LOW` to minimize false positives.",
        "D": "Configure Amazon Bedrock's content filtering guardrails with a threshold value of 0.80 using the `UpdateGuardrailsConfig` API, and route logs to Amazon S3 for long-term storage. Set the 'filter_strictness' parameter to `MEDIUM` to balance compliance and false positives."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because it uses the `CreateGuardrailsConfig` API to define a new guardrail configuration with a threshold of 0.75, which strikes a balance between minimizing false positives and ensuring compliance. The 'filter_strictness' parameter is appropriately set to `MEDIUM`, which aligns with the company's need to balance compliance and performance. Logging to Amazon CloudWatch is optimal for real-time auditing in a compliance-sensitive industry. \n\nOption B fails because the threshold of 0.85 is overly strict, leading to a higher chance of false positives, which the company wants to avoid. The use of the `UpdateGuardrailsConfig` API is incorrect in this context since the company is defining a new guardrail configuration. \n\nOption C fails because the threshold of 0.70 is too lenient, potentially allowing non-compliant responses through the filter, which would violate regulatory requirements. Additionally, setting 'filter_strictness' to `LOW` prioritizes performance over compliance, contrary to the company's needs. \n\nOption D fails because, although the threshold of 0.80 and 'filter_strictness' set to `MEDIUM` are reasonable, the use of the `UpdateGuardrailsConfig` API is inappropriate as the company is creating a new guardrail configuration, not updating an existing one.",
      "topic": "Guardrails content filters - threshold configuration",
      "difficulty": "medium",
      "category": "bedrock",
      "aws_services": [
        "Amazon Bedrock",
        "Amazon CloudWatch",
        "Amazon S3"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/bedrock/latest/userguide/guardrails.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:44:09.940708",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [
          "The question assumes the existence of a `CreateGuardrailsConfig` and `UpdateGuardrailsConfig` API, but these APIs are not explicitly mentioned in the provided AWS Bedrock documentation. While the concept of configuring guardrails is valid, the exact API names should be verified for accuracy.",
          "The explanation for Option D states that the `UpdateGuardrailsConfig` API is inappropriate because the company is creating a new guardrail configuration. However, if the API allows both creation and updates, this reasoning might not hold. This should be clarified in the AWS documentation."
        ],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 36
    },
    {
      "question": "A global e-commerce company uses Amazon Bedrock for cross-region inference to provide real-time personalized product recommendations to end-users. The company operates in three regions: us-east-1, eu-west-1, and ap-southeast-1. Due to strict latency SLAs, inference requests must be served from the region closest to the user, with an average response time under 100ms. Additionally, the company requires failover between regions to ensure availability during outages. The architecture must comply with GDPR regulations for European users, ensuring data sovereignty for all user data processed in the EU. Cost is a secondary concern but should be optimized where possible. Which solution BEST meets these requirements?",
      "options": {
        "A": "Deploy Bedrock endpoints in all three regions and use Amazon Route 53 latency-based routing to direct traffic to the nearest region. Configure Route 53 health checks for failover and ensure that the eu-west-1 endpoint processes requests for European users only. Use AWS PrivateLink to securely connect to Bedrock endpoints within each region.",
        "B": "Deploy Bedrock endpoints in all three regions and use Amazon Route 53 geo-location routing to direct traffic to the nearest region. Configure Route 53 health checks for failover. Use AWS Transit Gateway to securely route all requests through a central VPC in us-east-1 to Bedrock endpoints.",
        "C": "Deploy Bedrock endpoints in us-east-1 and ap-southeast-1 only. Use Amazon CloudFront with Lambda@Edge to route traffic to the nearest region and failover to the secondary region if the primary is unavailable. Use AWS Key Management Service (KMS) with region replication to comply with GDPR requirements.",
        "D": "Deploy Bedrock endpoints in all three regions and use AWS Global Accelerator with endpoint groups in each region to route traffic based on latency. Enable endpoint weight adjustment for failover. Use AWS Secrets Manager to handle secure API authentication for Bedrock in all regions."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because it uses Amazon Route 53 latency-based routing to minimize latency, with health checks to enable failover, and ensures GDPR compliance by restricting European traffic to the eu-west-1 region. AWS PrivateLink securely connects to Bedrock endpoints, reducing the risk of data exposure. \n\nOption B fails because geo-location routing in Route 53 does not guarantee latency optimization, and routing all traffic through a central Transit Gateway in us-east-1 introduces unnecessary latency, violating the SLA. \n\nOption C fails because deploying Bedrock endpoints in only two regions does not meet the latency requirement for users in Europe, and using CloudFront with Lambda@Edge does not provide region-specific failover for Bedrock. Additionally, KMS region replication is unrelated to GDPR data sovereignty requirements for processing. \n\nOption D fails because AWS Global Accelerator does not natively support GDPR compliance by restricting data processing to specific regions, and endpoint weight adjustment is not as precise as Route 53 latency-based routing for ensuring SLA adherence.",
      "topic": "Cross-region inference - failover and latency optimization",
      "difficulty": "hard",
      "category": "bedrock",
      "aws_services": [
        "Amazon Bedrock",
        "Amazon Route 53",
        "AWS PrivateLink",
        "AWS Global Accelerator"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/bedrock/latest/userguide/inference.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:44:28.032197",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 37
    },
    {
      "question": "A global healthcare company is developing an AI-powered medical knowledge base using Amazon Bedrock and a foundation model (FM) for natural language queries. The knowledge base contains sensitive patient information and metadata, such as patient IDs, record timestamps, and compliance tags (e.g., HIPAA). The company needs to ensure that patient data is filtered based on metadata attributes before being passed to the foundation model to meet strict compliance requirements. Additionally, they aim to optimize performance and minimize costs. The solution must handle queries with dynamic metadata filtering requirements and ensure that only relevant subsets of data are sent to the FM for processing. Which solution BEST meets these requirements?",
      "options": {
        "A": "Use Amazon Bedrock with pre-filtering by integrating AWS Lambda to implement metadata-based filtering logic, ensuring only relevant data is sent to the FM. Configure the Bedrock InvokeModel API to process the filtered subset of data.",
        "B": "Use Amazon Bedrock with post-filtering by sending the entire dataset to the FM and applying AWS Lambda to filter the response based on metadata attributes. Configure the Bedrock InvokeModel API to return all processed data for filtering.",
        "C": "Use Amazon Bedrock and pre-filter data by integrating with Amazon S3 Select for metadata-based filtering, passing only the filtered data subset to the FM via the Bedrock InvokeModel API.",
        "D": "Use Amazon Bedrock with pre-filtering by leveraging Amazon DynamoDB Streams to filter metadata attributes in real-time and pass the filtered subset to the FM via the Bedrock InvokeModel API."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because pre-filtering data using AWS Lambda ensures that only relevant data is sent to the FM, adhering to compliance requirements and minimizing processing costs. Option B fails because post-filtering sends the entire dataset to the FM, which is inefficient and risks exposing sensitive data during processing. Option C fails because Amazon S3 Select is not optimized for dynamic filtering based on metadata attributes in real-time queries. Option D fails because DynamoDB Streams is designed for change data capture, not for dynamic metadata filtering of query datasets.",
      "topic": "Knowledge Bases metadata filtering - pre vs post filtering",
      "difficulty": "medium",
      "category": "bedrock",
      "aws_services": [
        "Amazon Bedrock",
        "AWS Lambda",
        "Amazon S3",
        "Amazon DynamoDB"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:44:42.706154",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 38
    },
    {
      "question": "A financial analytics company is building a Retrieval-Augmented Generation (RAG) solution using Amazon Bedrock to provide real-time responses from a proprietary knowledge base. The knowledge base consists of sensitive client data stored in Amazon S3, segmented into large JSON documents. The company faces the following requirements:\n\n1. Optimize the context window usage for Bedrock's foundation models, ensuring responses remain relevant and concise.\n2. Maintain compliance with financial regulations (e.g., GDPR and PCI DSS), requiring strict encryption and logging of all data access.\n3. Minimize operational costs while ensuring high query performance for concurrent users.\n\nAdditionally, the company needs to preprocess the large documents into manageable chunks for effective token management, while ensuring the chunking logic aligns with Bedrock's token limits (e.g., maximum 4,096 tokens per input). Which solution BEST meets these requirements?",
      "options": {
        "A": "Use Amazon S3 with server-side encryption (SSE-S3) for storing the documents. Implement AWS Glue to preprocess the JSON files into chunks of up to 4,000 tokens based on semantic boundaries, and store these chunks in Amazon OpenSearch Service. Use Bedrock's GPT model with the `max_tokens` parameter set to 3,800 for inference. Enable AWS CloudTrail for logging and ensure VPC endpoints for compliance.",
        "B": "Use Amazon S3 with server-side encryption (SSE-KMS) for storing the documents. Implement AWS Glue to preprocess the JSON files into chunks of up to 4,096 tokens based on semantic boundaries, and store these chunks in Amazon DynamoDB with on-demand capacity mode. Use Bedrock's FLAN-T5 model with the `max_tokens` parameter set to 4,096 for inference. Enable AWS CloudTrail for logging and configure an Amazon VPC with private endpoints.",
        "C": "Use Amazon S3 with server-side encryption (SSE-KMS) for storing the documents. Implement AWS Lambda to preprocess the JSON files into chunks of up to 4,096 tokens strictly based on token count, and store these chunks in Amazon OpenSearch Service. Use Bedrock's GPT model with the `max_tokens` parameter set to 4,000 for inference. Enable AWS Config for compliance monitoring and ensure VPC endpoints.",
        "D": "Use Amazon S3 with server-side encryption (SSE-S3) for storing the documents. Implement AWS Lambda to preprocess the JSON files into chunks of up to 4,096 tokens strictly based on token count, and store these chunks in Amazon DynamoDB with provisioned capacity mode. Use Bedrock's FLAN-T5 model with the `max_tokens` parameter set to 3,800 for inference. Enable AWS CloudTrail for logging and configure an Amazon VPC with private endpoints."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because it uses SSE-S3 for cost-effective encryption, semantic chunking with AWS Glue for improved context management, and Amazon OpenSearch Service for efficient querying of preprocessed chunks. The `max_tokens` parameter is correctly set below the model's limit, ensuring relevance and compliance with token restrictions. VPC endpoints and CloudTrail ensure security and compliance.\n\nOption B fails because using DynamoDB with on-demand capacity mode can become cost-prohibitive for high query volumes, and FLAN-T5 is less optimal for RAG tasks compared to GPT for this use case.\n\nOption C fails because strict token-based chunking without semantic boundaries risks generating incoherent responses, and AWS Config, while useful, does not address the direct logging requirement as effectively as CloudTrail.\n\nOption D fails because provisioned capacity mode in DynamoDB may not scale efficiently for unpredictable query loads, and FLAN-T5 is less suitable than GPT for the scenario's real-time RAG requirements.",
      "topic": "Context window optimization for RAG responses - chunking and token management",
      "difficulty": "hard",
      "category": "bedrock",
      "aws_services": [
        "Amazon Bedrock",
        "AWS Glue",
        "Amazon OpenSearch Service",
        "Amazon S3",
        "AWS CloudTrail",
        "Amazon DynamoDB",
        "AWS Lambda"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:45:01.493636",
      "critique": {
        "overall_score": 8.5,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [
          "The explanation for why FLAN-T5 is less optimal than GPT could be elaborated further, as both models have different strengths and the choice may depend on the specific RAG implementation.",
          "The use of SSE-S3 for encryption in Option A is cost-effective but may not meet stricter compliance requirements like PCI DSS, which often mandates SSE-KMS for better key management."
        ],
        "verification_status": "APPROVED",
        "confidence": 0.95
      },
      "verification_status": "APPROVED",
      "id": 39
    },
    {
      "question": "A financial services company operates in a highly regulated environment and needs to deploy and share a proprietary machine learning model across multiple AWS accounts. The model is hosted on Amazon Bedrock, and the company requires that the sharing mechanism ensures: 1) low latency inference for end-users in all accounts, 2) granular permissions to control which accounts can access the model, 3) compliance with strict data residency requirements preventing the model from being moved out of the primary account's AWS Region, and 4) minimal operational overhead and cost. Additionally, the company must track all access to the model for auditing purposes. Which solution BEST meets these requirements?",
      "options": {
        "A": "Host the model on Amazon Bedrock in the primary account and configure Resource Access Manager (RAM) to share the Bedrock model endpoint across accounts. Use AWS Identity and Access Management (IAM) policies to restrict access to specific accounts and enable CloudTrail logging to track access.",
        "B": "Host the model on Amazon Bedrock in the primary account and share the Bedrock endpoint URL directly with other accounts. Use AWS Key Management Service (KMS) to encrypt the endpoint traffic and enable CloudTrail logging to monitor usage.",
        "C": "Create a custom API Gateway in the primary account that proxies requests to the Amazon Bedrock endpoint. Share the API Gateway endpoint across accounts using Resource Access Manager (RAM) and enforce access control using API Gateway resource policies.",
        "D": "Host the model on Amazon Bedrock in the primary account and replicate the model manually to each secondary account using Amazon S3 for deployment. Use IAM policies to control access and enable CloudTrail logging for audit purposes."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because using Amazon Bedrock with AWS Resource Access Manager (RAM) ensures low-latency access while maintaining centralized control over the model. RAM allows sharing of resources across accounts without replicating the model, and IAM policies provide granular access control. CloudTrail ensures compliance by tracking all access. \n\nOption B fails because sharing a Bedrock endpoint URL directly does not offer granular access control or integrate well with multi-account architectures. Additionally, KMS encrypts data but does not address the access control requirements. \n\nOption C fails because using a custom API Gateway introduces additional latency and operational overhead, which violates the low latency and minimal operational overhead requirements. \n\nOption D fails because manually replicating the model across accounts increases operational complexity, cost, and the risk of non-compliance with data residency requirements, as it involves duplicating the model outside its primary account and region.",
      "topic": "Cross-account model sharing patterns",
      "difficulty": "hard",
      "category": "bedrock",
      "aws_services": [
        "Amazon Bedrock",
        "AWS Resource Access Manager",
        "AWS CloudTrail",
        "IAM"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/bedrock/latest/userguide/security.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:45:33.626194",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 40
    },
    {
      "question": "A global e-commerce company uses Amazon Bedrock for generative AI capabilities to enhance product recommendations. To optimize latency and reduce costs, they aim to implement prompt caching for frequently repeated queries. Their workload requires high availability in two AWS regions (us-east-1 and eu-west-1) with compliance for GDPR data regulations. The company needs to ensure prompt cache invalidation when model updates occur while minimizing operational overhead. Additionally, the solution must handle up to 10,000 queries per second and provide encryption for sensitive data both at rest and in transit. Which solution BEST meets these requirements?",
      "options": {
        "A": "Use Amazon DynamoDB Global Tables with client-side encryption via AWS KMS for prompt caching across regions. Configure TTL for cache expiration and use Amazon EventBridge to trigger invalidation upon model updates.",
        "B": "Use Amazon ElastiCache for Redis with encryption in transit enabled and cluster mode across regions. Configure Redis key expiry based on TTL and use AWS Lambda to handle invalidation events from Amazon EventBridge.",
        "C": "Use Amazon S3 with object versioning and server-side encryption using AWS KMS for prompt caching. Leverage S3 Event Notifications to trigger AWS Lambda for cache invalidation and replicate data across regions using Cross-Region Replication.",
        "D": "Use Amazon DynamoDB Standard Tables with server-side encryption via AWS KMS for prompt caching. Configure TTL for cache expiration and use AWS Step Functions to orchestrate cache invalidation upon model updates."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because it uses DynamoDB Global Tables to ensure high availability and data replication across regions, meeting the compliance and performance requirements. TTL simplifies cache expiration, while EventBridge provides a lightweight mechanism for model update-triggered cache invalidation. Option B fails because ElastiCache for Redis does not natively support multi-region replication without additional configurations, introducing complexity and potential compliance risks for GDPR. Option C fails because S3 is better suited for object storage rather than high-throughput prompt caching, and Cross-Region Replication adds latency. Option D fails because Standard Tables in DynamoDB do not support seamless multi-region replication like Global Tables, which is critical for this scenario.",
      "topic": "Prompt caching - latency and cost optimization",
      "difficulty": "medium",
      "category": "bedrock",
      "aws_services": [
        "Amazon DynamoDB",
        "Amazon EventBridge",
        "AWS KMS"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/bedrock/latest/userguide/inference.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:45:48.360257",
      "critique": {
        "overall_score": 8.5,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [
          "Option B mentions ElastiCache for Redis with cluster mode across regions, but this is not natively supported without additional configurations like Global Datastore, which introduces complexity and may not fully meet GDPR compliance requirements.",
          "Option D uses DynamoDB Standard Tables, which lack the seamless multi-region replication capabilities of Global Tables, making it less suitable for the high-availability and low-latency requirements."
        ],
        "verification_status": "APPROVED",
        "confidence": 0.95
      },
      "verification_status": "APPROVED",
      "id": 41
    },
    {
      "question": "A financial services company is building a custom generative AI model using Amazon Bedrock for generating personalized financial reports for their clients. They need to fine-tune the model's hyperparameters using private financial datasets stored in Amazon S3. The team requires the solution to comply with stringent regulatory requirements (e.g., encryption in transit and at rest), minimize costs during training, and optimize for high model accuracy. Additionally, the company wants to use automated hyperparameter optimization to reduce manual effort. They are also concerned about potential data leakage and must ensure that their private data is not shared with third-party providers. Which solution BEST meets these requirements?",
      "options": {
        "A": "Use Amazon Bedrock with a foundation model supporting private fine-tuning. Encrypt the private dataset in Amazon S3 using AWS KMS-managed keys (SSE-KMS). Configure SageMaker Automatic Model Tuning to run hyperparameter optimization on a private VPC endpoint with the 'HyperparameterTuningJobObjectiveType' set to 'Maximize' for accuracy. Use IAM policies to restrict access to the S3 bucket and ensure data does not leave the AWS environment.",
        "B": "Use Amazon Bedrock with a foundation model supporting fine-tuning. Store the dataset in Amazon S3 with SSE-S3 encryption. Configure SageMaker Automatic Model Tuning on default public endpoints with the 'HyperparameterTuningJobObjectiveType' set to 'Maximize' for accuracy. Add a bucket policy to restrict public access to the data.",
        "C": "Use Amazon Bedrock with a foundation model supporting private fine-tuning. Encrypt the dataset in S3 using SSE-KMS, and configure SageMaker Automatic Model Tuning to run hyperparameter optimization on a private VPC endpoint. Set the 'HyperparameterTuningJobObjectiveType' to 'Minimize' for cost efficiency. Use IAM policies to restrict access to the S3 bucket and ensure data remains private.",
        "D": "Use Amazon Bedrock with a foundation model supporting fine-tuning. Encrypt the dataset in S3 using SSE-KMS and configure SageMaker Automatic Model Tuning to run hyperparameter optimization on default public endpoints with the 'HyperparameterTuningJobObjectiveType' set to 'Maximize' for accuracy. Use IAM policies to restrict access to the S3 bucket."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because it uses Amazon Bedrock with private fine-tuning support, complies with stringent security requirements by using SSE-KMS for encryption and running hyperparameter tuning on a private VPC endpoint, and optimizes for accuracy by setting the correct 'HyperparameterTuningJobObjectiveType'. Option B fails because it uses SSE-S3 encryption instead of SSE-KMS, which does not meet the regulatory requirement for using AWS KMS-managed keys, and uses default public endpoints, which could lead to potential data leakage. Option C fails because the objective type is incorrectly set to 'Minimize' for cost efficiency instead of 'Maximize' for accuracy, which does not align with the requirement for high model accuracy. Option D fails because it uses default public endpoints for hyperparameter tuning, which does not align with the requirement to keep data private and avoid potential leakage.",
      "topic": "Fine-tuning hyperparameters optimization",
      "difficulty": "hard",
      "category": "bedrock",
      "aws_services": [
        "Amazon Bedrock",
        "Amazon S3",
        "AWS KMS",
        "SageMaker Automatic Model Tuning"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/bedrock/latest/userguide/custom-models.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:46:09.243390",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 42
    },
    {
      "question": "A global e-commerce company is building a customer service chatbot using Amazon Bedrock. The chatbot must provide personalized responses based on a knowledge base containing product documentation, return policies, and FAQ articles. The knowledge base includes metadata such as region, language, category, and compliance level. The company has the following constraints: \n\n1. The chatbot must filter responses based on metadata like region and compliance level to ensure it adheres to differing regulations across countries.\n2. Performance is critical due to real-time customer interactions, with a response latency requirement of under 200ms.\n3. Cost must be minimized, but not at the expense of violating compliance requirements.\n4. The solution must support frequent updates to the knowledge base, as the company updates its policies weekly.\n\nWhich solution BEST meets these requirements?",
      "options": {
        "A": "Use Amazon Bedrock with pre-filtering by applying metadata filters (e.g., region and compliance level) when querying the knowledge base using the `filter` parameter in the `QueryKnowledgeBase` API.",
        "B": "Use Amazon Bedrock with post-filtering by retrieving all results from the `QueryKnowledgeBase` API and applying metadata filters (e.g., region and compliance level) client-side before presenting the response to the user.",
        "C": "Use Amazon Bedrock with pre-filtering by applying metadata filters (e.g., region and compliance level) when querying the knowledge base using the `filter` parameter in the `RetrieveKnowledgeBaseContent` API.",
        "D": "Use Amazon Bedrock with post-filtering by first querying the knowledge base without filters using the `QueryKnowledgeBase` API and then applying metadata filters (e.g., region and compliance level) using the `FilterResults` API."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because pre-filtering using the `filter` parameter in the `QueryKnowledgeBase` API ensures that only compliant and region-specific results are retrieved, optimizing for both performance (reducing unnecessary data transfer) and compliance (ensuring only valid responses are served). Option B is incorrect because post-filtering client-side increases response latency by retrieving unnecessary data, violating the latency requirement. Option C is incorrect because the `RetrieveKnowledgeBaseContent` API is designed for retrieving specific content by ID, not for querying with metadata filters, making it unsuitable for this use case. Option D is incorrect because the `FilterResults` API does not exist in Amazon Bedrock, rendering the solution invalid.",
      "topic": "Knowledge Bases metadata filtering - pre vs post filtering",
      "difficulty": "medium",
      "category": "bedrock",
      "aws_services": [
        "Amazon Bedrock"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:46:25.208290",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 43
    },
    {
      "question": "A global e-commerce company needs to build a unified knowledge base for product search that supports both semantic search (natural language queries) and traditional keyword search. The solution must: \n- Provide low-latency responses for users across multiple regions. \n- Ensure data is encrypted in transit and at rest to meet PCI DSS compliance. \n- Manage unpredictable traffic spikes during seasonal sales efficiently without incurring excessive costs. \n- Support a hybrid search approach by fusing semantic vector embeddings with keyword matching. \nThe company currently uses Amazon S3 to store product metadata and images, and requires integration with their existing architecture. Which solution BEST meets these requirements?",
      "options": {
        "A": "Use Amazon OpenSearch Service with HNSW-based KNN vector search for semantic embeddings and keyword matching. Configure cross-region replication for low-latency access and enable encryption using AWS KMS. Use UltraWarm storage for infrequently accessed indices to optimize costs.",
        "B": "Use Amazon OpenSearch Serverless with HNSW-based KNN vector search for semantic embeddings and keyword matching. Configure cross-region replication for low-latency access but use native SSE-S3 encryption for indices. Scale automatically using serverless capacity units.",
        "C": "Use Amazon OpenSearch Service with IVF-based KNN vector search for semantic embeddings and keyword matching. Configure cross-region replication using OpenSearch APIs and enable encryption using AWS KMS. Use Auto-Tune to manage traffic spikes.",
        "D": "Use Amazon OpenSearch Serverless with HNSW-based KNN vector search for semantic embeddings and keyword matching. Store indices in a single region to reduce costs and enable encryption using AWS KMS. Scale automatically using serverless capacity units."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because it uses Amazon OpenSearch Service with HNSW-based KNN for the best performance in semantic search and keyword matching. It also ensures compliance with PCI DSS by encrypting data using AWS KMS and optimizes costs with UltraWarm storage for less frequently accessed indices. Option B fails because native SSE-S3 encryption does not meet PCI DSS compliance for OpenSearch indices. Option C fails because IVF is not as effective as HNSW for the semantic search use case and does not optimize costs with UltraWarm storage. Option D fails because storing indices in a single region does not meet the low-latency requirement for a global user base.",
      "topic": "Knowledge Bases hybrid search - semantic and keyword search fusion",
      "difficulty": "hard",
      "category": "bedrock",
      "aws_services": [
        "Amazon OpenSearch Service",
        "AWS KMS",
        "Amazon S3"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/opensearch-service/latest/developerguide/knn.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:46:41.133855",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [
          "The explanation for Option B states that native SSE-S3 encryption does not meet PCI DSS compliance for OpenSearch indices. While this is generally correct, the question could clarify that AWS KMS encryption is the preferred method for compliance in this context.",
          "The explanation for Option C dismisses IVF as less effective than HNSW for semantic search. While HNSW is indeed more performant for high-dimensional vector search, the explanation could briefly mention that IVF is still a valid option for certain use cases, albeit not optimal here."
        ],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 44
    },
    {
      "question": "A financial analytics company processes large volumes of documents, including PDF tables, markdown files, and code repositories, to generate real-time insights using generative AI with Amazon Bedrock. They need a solution that allows efficient chunking of document content for embedding generation, optimized for contextual retrieval by a custom LLM model hosted on Bedrock. The company has the following requirements: \n\n1. PDF tables must retain tabular structure for accurate embeddings. Markdown files must preserve logical hierarchy and semantic meaning. Code files must ensure function-level contextual splits.\n2. Performance is critical for real-time analytics, with latency under 200ms for chunking and embedding generation.\n3. Compliance with SOC 2 and GDPR is mandatory, requiring strict control over data access and encryption.\n4. The solution should minimize costs while scaling with unpredictable spikes in document ingestion. \n\nWhich solution BEST meets these requirements?",
      "options": {
        "A": "Use Amazon Bedrock with a custom tokenizer configured via the `chunking-strategy` API parameter, specifying `preserve_structure` for PDFs, `semantic_hierarchy` for markdown files, and `function_context` for code files. Store embeddings in Amazon OpenSearch Service with encryption enabled (KMS). Use AWS Lambda for on-demand processing, scaling with ingestion spikes.",
        "B": "Use Amazon Bedrock with a custom tokenizer configured via the `chunking-strategy` API parameter, specifying `preserve_structure` for PDFs, `semantic_hierarchy` for markdown files, and `line_context` for code files. Store embeddings in Amazon DynamoDB with on-demand mode enabled. Use AWS Step Functions for workflow orchestration to scale processing dynamically.",
        "C": "Use Amazon Bedrock with a pre-built tokenizer using the default `chunking-strategy` parameter, optimized for general-purpose use. Store embeddings in Amazon OpenSearch Service with encryption enabled (KMS). Use Amazon SQS for task queuing to handle ingestion spikes efficiently.",
        "D": "Use Amazon Bedrock with a custom tokenizer configured via the `chunking-strategy` API parameter, specifying `tabular_split` for PDFs, `semantic_hierarchy` for markdown files, and `line_context` for code files. Store embeddings in Amazon DynamoDB with provisioned capacity mode for predictable costs. Use AWS Lambda for document processing to maintain latency requirements."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because it uses the appropriate `chunking-strategy` values (`preserve_structure`, `semantic_hierarchy`, and `function_context`) for the specific document types, ensuring accurate embeddings. It also leverages Amazon OpenSearch Service with KMS encryption for compliance and AWS Lambda for cost-effective, on-demand scaling. \n\nOption B fails because the `line_context` strategy for code files does not provide sufficient function-level splits, reducing retrieval accuracy for code-related embeddings. Additionally, DynamoDB's on-demand mode is less cost-efficient for high volumes compared to OpenSearch Service for this use case.\n\nOption C fails because the default `chunking-strategy` is optimized for general-purpose use and does not meet the specific requirements for preserving tabular, hierarchical, and function-level structures. Amazon SQS provides queuing but lacks the direct processing capabilities needed for real-time performance under 200ms latency.\n\nOption D fails because `tabular_split` for PDFs does not guarantee full tabular structure retention, and DynamoDB provisioned capacity mode introduces cost inefficiencies during spikes. While AWS Lambda is used correctly, the combination of chunking strategies and storage is suboptimal for this scenario.",
      "topic": "Document-specific chunking - PDF tables vs markdown vs code files",
      "difficulty": "hard",
      "category": "bedrock",
      "aws_services": [
        "Amazon Bedrock",
        "Amazon OpenSearch Service",
        "AWS Lambda"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:47:04.353563",
      "critique": {
        "overall_score": 8.5,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [
          "The question assumes the existence of specific `chunking-strategy` API parameters like `preserve_structure`, `semantic_hierarchy`, and `function_context`, which are not explicitly documented in AWS Bedrock's current feature set. However, this could be inferred as a hypothetical or future capability for the sake of the question."
        ],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 45
    },
    {
      "question": "A financial analytics company uses Amazon SageMaker to deploy machine learning models for real-time fraud detection. The application experiences highly unpredictable traffic patterns, with sudden spikes during credit card transaction peaks. The company needs to implement an auto-scaling solution for their SageMaker endpoint to ensure low latency (<100ms) during peak traffic while minimizing operational costs during off-peak hours. Due to compliance requirements, the solution must not over-provision resources unnecessarily, and metrics must be monitored closely to maintain accurate scaling behavior. The company has already integrated CloudWatch metrics into their architecture and uses the `InvocationsPerInstance` metric to determine model throughput. Which solution BEST meets these requirements?",
      "options": {
        "A": "Implement target tracking scaling on the SageMaker endpoint with a target value of 70% for `InvocationsPerInstance` and a cooldown period of 300 seconds. Enable detailed CloudWatch monitoring for precise metric collection.",
        "B": "Implement step scaling on the SageMaker endpoint with step adjustments for `InvocationsPerInstance`, setting a scale-out threshold at 80% utilization and scale-in threshold at 30%. Use a cooldown period of 300 seconds.",
        "C": "Implement target tracking scaling on the SageMaker endpoint with a target value of 80% for `InvocationsPerInstance`. Use a cooldown period of 600 seconds to stabilize scaling behavior.",
        "D": "Implement step scaling on the SageMaker endpoint with step adjustments for `InvocationsPerInstance`, setting a scale-out threshold at 70% and scale-in threshold at 40%. Enable detailed CloudWatch monitoring for metric accuracy."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because target tracking provides dynamic adjustment to maintain the desired `InvocationsPerInstance` utilization, ensuring compliance with low latency requirements while minimizing over-provisioning. The target value of 70% balances cost and performance, and the 300-second cooldown avoids aggressive scaling during traffic bursts. Option B fails because step scaling requires pre-defined thresholds, which are less effective in handling unpredictable traffic spikes dynamically. Option C fails because a target value of 80% risks overloading instances, potentially breaching latency SLA, and the 600-second cooldown is too long for real-time traffic patterns. Option D fails because the scale-in threshold of 40% may lead to under-utilization during off-peak hours, increasing costs unnecessarily.",
      "topic": "Auto-scaling - target tracking vs step scaling",
      "difficulty": "hard",
      "category": "sagemaker",
      "aws_services": [
        "sagemaker",
        "cloudwatch"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:47:39.388569",
      "critique": {
        "overall_score": 9.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [],
        "verification_status": "APPROVED",
        "confidence": 0.95
      },
      "verification_status": "APPROVED",
      "id": 46
    },
    {
      "question": "A financial analytics company needs to deploy a generative AI solution that provides real-time investment advice based on large-scale proprietary datasets. The solution must support custom fine-tuning of pre-trained models with the companys data, ensure compliance with financial data regulations (such as GDPR), and maintain high availability across multiple AWS regions. Due to budget constraints, the company wants to minimize operational costs while maintaining secure access to proprietary model artifacts. Which solution BEST meets these requirements?",
      "options": {
        "A": "Use SageMaker JumpStart to deploy a pre-trained model, fine-tune it with proprietary data using SageMaker managed training jobs, and host the model in SageMaker endpoints with multi-region replication enabled. Encrypt the model artifacts with AWS KMS and enforce IAM policies to restrict access.",
        "B": "Use AWS Bedrock to deploy a pre-trained foundation model, fine-tune the model using Bedrocks prompt-based APIs, and utilize AWS Global Accelerator for multi-region availability. Encrypt the model artifacts with AWS KMS and apply SCPs (Service Control Policies) for compliance.",
        "C": "Use SageMaker JumpStart to deploy a fine-tuned pre-trained model, host it in SageMaker endpoints, and utilize Amazon CloudFront for multi-region availability. Leverage AWS Secrets Manager for encrypting proprietary model data and enforce VPC endpoints for secure access.",
        "D": "Use AWS Bedrock to deploy pre-trained foundation models, fine-tune them using SageMaker managed training jobs, and host the fine-tuned models in SageMaker endpoints with multi-region replication enabled. Encrypt model artifacts using AWS KMS and restrict access using IAM policies."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because SageMaker JumpStart provides pre-trained models with fine-tuning capabilities and supports encryption with AWS KMS. It also integrates with SageMaker endpoints for scalable, multi-region deployment, meeting compliance and cost requirements. Option B fails because AWS Bedrock does not directly support custom fine-tuning of models with proprietary data; fine-tuning in this case requires SageMaker. Option C fails because CloudFront is suboptimal for real-time model hosting compared to SageMaker endpoints, and Secrets Manager is designed for secrets management, not model data encryption. Option D fails because Bedrock does not integrate directly with SageMaker endpoints for hosting fine-tuned models; SageMaker JumpStart is the correct tool for such scenarios.",
      "topic": "SageMaker JumpStart vs Bedrock trade-offs",
      "difficulty": "hard",
      "category": "sagemaker",
      "aws_services": [
        "SageMaker JumpStart",
        "AWS Bedrock",
        "AWS KMS",
        "IAM"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/sagemaker/latest/dg/studio-jumpstart.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:48:00.432170",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [
          "The explanation for Option D could be clarified further. While Bedrock does not directly support fine-tuning, the question implies using SageMaker for fine-tuning and hosting in SageMaker endpoints. However, the integration between Bedrock and SageMaker endpoints is not explicitly supported for hosting fine-tuned models, making Option A the better choice."
        ],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 47
    },
    {
      "question": "A financial services company is building an ML workflow to automate fraud detection. They use Amazon SageMaker for training, hosting, and managing their models. The company requires a robust model versioning system to track and deploy multiple iterations of models while ensuring compliance with strict regulatory standards. They want to automate model approval workflows for deployment into production, ensuring that only approved models are used. Additionally, the solution must minimize downtime during model updates and adhere to a strict budget constraint. The company also needs to maintain an immutable history of all model versions for audit purposes. Which solution BEST meets these requirements?",
      "options": {
        "A": "Use the SageMaker Model Registry with a pre-deployment Lambda step in SageMaker Pipelines to automate model approval. Configure the model group with specific tags for version tracking. Apply an Amazon EventBridge rule to trigger the Lambda function for compliance checks. Use SageMaker Endpoint variants for blue/green deployments to minimize downtime during updates.",
        "B": "Use the SageMaker Model Registry with manual approval steps for model versions. Configure the model group with metadata for version tracking. Use an Amazon CloudWatch Events rule to trigger compliance checks. Deploy new models directly to a SageMaker Endpoint using the ReplaceEndpoint API for updates.",
        "C": "Use the SageMaker Model Registry with a pre-deployment Lambda step in SageMaker Pipelines to automate model approval. Configure the model group with specific tags for version tracking. Deploy new models directly to a SageMaker Endpoint using the ReplaceEndpoint API for updates to minimize costs.",
        "D": "Use the SageMaker Model Registry with manual approval steps for model versions. Configure the model group with metadata for version tracking. Apply an Amazon EventBridge rule to trigger compliance checks. Use SageMaker Endpoint variants for blue/green deployments to minimize downtime during updates."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because it uses the SageMaker Model Registry with automated approval workflows via Lambda and EventBridge, ensuring compliance and regulatory requirements. The use of blue/green deployments with SageMaker Endpoint variants minimizes downtime during updates while maintaining cost efficiency. Option B fails because it uses manual approval steps, which do not align with the automation requirement, and the ReplaceEndpoint API introduces downtime. Option C fails because although it uses automation, deploying models directly with ReplaceEndpoint API does not minimize downtime, which violates the requirement. Option D fails because it uses manual approval steps, which do not meet the automation requirement, even though it uses blue/green deployments.",
      "topic": "Model registry versioning workflows",
      "difficulty": "medium",
      "category": "sagemaker",
      "aws_services": [
        "Amazon SageMaker",
        "AWS Lambda",
        "Amazon EventBridge",
        "SageMaker Pipelines"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:48:20.640307",
      "critique": {
        "overall_score": 9.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [
          "The explanation could emphasize that SageMaker Endpoint variants are specifically designed for blue/green deployments, which is critical for minimizing downtime during updates.",
          "The explanation for Option C could clarify that while ReplaceEndpoint API is cost-efficient, it does not meet the downtime minimization requirement, which is a key factor in the scenario."
        ],
        "verification_status": "APPROVED",
        "confidence": 0.95
      },
      "verification_status": "APPROVED",
      "id": 48
    },
    {
      "question": "A financial services company is developing a machine learning model to predict loan approval rates. To comply with regulatory standards, they must ensure the model is free from bias against protected attributes like race and gender. The company is using Amazon SageMaker for model development and must integrate SageMaker Clarify to conduct bias detection both during training and inference. The company has the following requirements: \n\n1. Bias detection must cover pretraining, posttraining, and inference bias.\n2. The solution must minimize costs while ensuring compliance with regulatory standards.\n3. The dataset contains sensitive Personally Identifiable Information (PII), and all processing must meet strict security requirements, including encryption in transit and at rest.\n4. The company requires detailed reporting outputs with metrics such as disparate impact and equalized odds for auditors.\n5. The bias detection job must complete within a 2-hour SLA to maintain operational efficiency.\n\nWhich solution BEST meets these requirements?",
      "options": {
        "A": "Use SageMaker Clarify pretraining and posttraining bias detection by configuring the `DataConfig` parameter with KMS encryption and specifying `BiasConfig` with `facet` attributes as 'race' and 'gender' in the input JSON. Use Clarify's `ModelPredictedLabelConfig` for inference bias detection, and enable detailed `SHAPConfig` explanations for auditors. Deploy all jobs using an ml.m5.4xlarge instance and optimize cost using Spot Instances.",
        "B": "Use SageMaker Clarify pretraining and posttraining bias detection by configuring the `DataConfig` parameter with S3-managed encryption and specifying `BiasConfig` with `facet` attributes as 'race' and 'gender'. Use Clarify's `ModelPredictedLabelConfig` for inference bias detection, enable detailed `SHAPConfig` explanations, and deploy all jobs using an ml.m5.4xlarge instance without Spot Instances for consistent runtime.",
        "C": "Use SageMaker Clarify pretraining and posttraining bias detection by configuring the `DataConfig` parameter with KMS encryption and specifying `BiasConfig` with `facet` attributes as 'race' and 'gender'. Use Clarify's `SHAPConfig` for inference bias detection rather than `ModelPredictedLabelConfig`, and deploy jobs using an ml.m5.8xlarge instance for faster processing. Avoid Spot Instances to ensure compliance.",
        "D": "Use SageMaker Clarify pretraining and posttraining bias detection by configuring the `DataConfig` parameter with KMS encryption and specifying `BiasConfig` with `facet` attributes as 'race' and 'gender'. Use Clarify's `ModelPredictedLabelConfig` for inference bias detection, enable detailed explanations using `SHAPConfig`, and deploy all jobs using a mix of ml.m5.4xlarge and ml.m5.8xlarge instances based on workload needs without Spot Instances."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because it optimally balances cost, compliance, and performance requirements by using KMS encryption for security, Spot Instances for cost savings, and proper configuration of `BiasConfig`, `ModelPredictedLabelConfig`, and `SHAPConfig` to meet regulatory and audit needs within the SLA. Option B fails because S3-managed encryption does not meet the strict security requirements for PII. Option C fails because `SHAPConfig` is not designed for inference bias detection, which requires `ModelPredictedLabelConfig`. Option D fails because using a mix of instances without Spot Instances increases costs unnecessarily without improving the SLA fulfillment.",
      "topic": "SageMaker Clarify bias detection",
      "difficulty": "hard",
      "category": "sagemaker",
      "aws_services": [
        "SageMaker",
        "S3",
        "KMS"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-configure-processing-jobs.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:48:46.058831",
      "critique": {
        "overall_score": 8.5,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [
          "The explanation could clarify that Spot Instances may introduce variability in job completion times, which could potentially impact the 2-hour SLA if not managed properly. However, this is a minor consideration given the cost optimization focus."
        ],
        "verification_status": "APPROVED",
        "confidence": 0.95
      },
      "verification_status": "APPROVED",
      "id": 49
    },
    {
      "question": "A retail analytics company is building a real-time recommendation engine for their e-commerce platform. They need to store and retrieve feature data for machine learning models in both real-time (online) and batch (offline) patterns. The online store must support low-latency reads for serving predictions, while the offline store should support high-throughput writes for training datasets. Additionally, the data must comply with GDPR regulations, ensuring proper encryption of data at rest and in transit. The company is also cost-conscious and wants to minimize operational overhead while ensuring high availability and scalability. Which solution BEST meets these requirements?",
      "options": {
        "A": "Use Amazon SageMaker Feature Store with the online store backed by Amazon DynamoDB and the offline store backed by Amazon S3. Enable encryption at rest using AWS Key Management Service (KMS) and use Amazon SageMaker Data Wrangler for GDPR-compliant data transformations.",
        "B": "Use Amazon SageMaker Feature Store with the online store backed by Amazon ElastiCache for Redis and the offline store backed by Amazon S3. Enable encryption at rest using AWS Key Management Service (KMS) and use Amazon SageMaker Data Wrangler for GDPR-compliant data transformations.",
        "C": "Use Amazon SageMaker Feature Store with the online store backed by Amazon DynamoDB and the offline store backed by Amazon Redshift. Enable encryption at rest using AWS Key Management Service (KMS) and use AWS Glue for GDPR-compliant data transformations.",
        "D": "Use Amazon SageMaker Feature Store with the online store backed by Amazon DynamoDB and the offline store backed by Amazon S3. Enable encryption at rest using customer-managed CMKs and transform data using AWS Glue to enforce GDPR compliance."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because Amazon SageMaker Feature Store integrates directly with Amazon DynamoDB for low-latency online feature storage and Amazon S3 for cost-effective, high-throughput offline feature storage. Encryption at rest is handled using AWS KMS, and Amazon SageMaker Data Wrangler simplifies data processing for GDPR compliance with built-in transformations. \n\nOption B fails because Amazon ElastiCache for Redis, while capable of low-latency reads, is not natively supported by Amazon SageMaker Feature Store as an online store backend. \n\nOption C fails because Amazon Redshift, while a powerful data warehouse, is not cost-effective for offline feature storage compared to Amazon S3 and is unnecessary for the described use case. \n\nOption D fails because using customer-managed CMKs increases operational complexity unnecessarily, and while AWS Glue can handle GDPR-compliant transformations, Amazon SageMaker Data Wrangler is a more seamless choice for this particular workflow.",
      "topic": "Feature Store online vs offline patterns",
      "difficulty": "medium",
      "category": "sagemaker",
      "aws_services": [
        "Amazon SageMaker Feature Store",
        "Amazon DynamoDB",
        "Amazon S3",
        "AWS KMS",
        "Amazon SageMaker Data Wrangler"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/sagemaker/latest/dg/feature-store.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:49:01.966934",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 50
    },
    {
      "question": "A fintech company is building a fraud detection model using Amazon SageMaker. The dataset contains 10 million rows and includes features such as transaction amounts, timestamps, and user metadata. The company needs to optimize the model's hyperparameters to achieve the best F1 score. Due to compliance requirements, the company must ensure that all training jobs run in a VPC for network isolation. Additionally, they aim to balance cost efficiency and performance, as they need the results within 6 hours. The team is considering two hyperparameter optimization strategies: Bayesian and Random Search. Bayesian optimization promises better performance but is computationally expensive, while Random Search is faster but may not converge to optimal hyperparameters. Which solution BEST meets these requirements?",
      "options": {
        "A": "Use SageMaker's HyperParameterTuner with the 'Bayesian' strategy, specifying a 'MaxParallelTrainingJobs' of 10, ensuring all jobs run within a VPC by setting the 'VpcConfig' parameter.",
        "B": "Use SageMaker's HyperParameterTuner with the 'Bayesian' strategy, specifying a 'MaxParallelTrainingJobs' of 20, and enable VPC configuration using the 'VpcConfig' parameter.",
        "C": "Use SageMaker's HyperParameterTuner with the 'Random' strategy, specifying a 'MaxParallelTrainingJobs' of 15, ensuring all jobs run within a VPC by setting the 'VpcConfig' parameter.",
        "D": "Use SageMaker's HyperParameterTuner with the 'Random' strategy, specifying a 'MaxParallelTrainingJobs' of 25, and configure the jobs to run in a VPC using the 'VpcConfig' parameter."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because Bayesian optimization is more suitable for achieving the best F1 score due to its ability to explore and exploit the parameter space effectively. Setting 'MaxParallelTrainingJobs' to 10 balances cost and performance within the 6-hour constraint. The 'VpcConfig' ensures compliance with network isolation requirements. Option B fails because setting 'MaxParallelTrainingJobs' to 20 increases parallelism, leading to higher costs and exceeding the 6-hour constraint due to Bayesian optimization's sequential nature. Option C fails because Random Search may not converge to optimal hyperparameters, compromising the model's F1 score. Option D fails because while Random Search is faster, increasing 'MaxParallelTrainingJobs' to 25 would unnecessarily inflate costs without significantly improving results.",
      "topic": "Hyperparameter tuning - Bayesian vs random",
      "difficulty": "medium",
      "category": "sagemaker",
      "aws_services": [
        "Amazon SageMaker"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:49:17.279905",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 51
    },
    {
      "question": "A global financial services company needs to implement bias detection in its machine learning models to comply with regional regulations such as GDPR in Europe and the Equal Credit Opportunity Act in the United States. They use Amazon SageMaker for model training and deployment, and their machine learning models handle sensitive customer data, including demographic information. The company must ensure that bias detection is performed securely and cost-effectively, with all insights generated stored in a centralized S3 bucket encrypted using AWS KMS. Additionally, they need to avoid excessive computation costs while ensuring that the bias detection results are accurate and reproducible across multiple regions. The solution must also support explainability for their models, which use both tabular and text data. Which solution BEST meets these requirements?",
      "options": {
        "A": "Use SageMaker Clarify processing jobs with the `DataConfig` parameter pointing to the S3 bucket encrypted with AWS KMS, set `ModelConfig` to include both SHAP-based explainability and bias metrics, and specify `ShapBaseline` with a representative dataset to ensure accurate results. Run the processing job in the same region as the data for compliance and cost optimization.",
        "B": "Use SageMaker Clarify processing jobs with the `DataConfig` parameter pointing to the S3 bucket encrypted with AWS KMS, set `ModelConfig` to include SHAP-based explainability only, and specify `ShapBaseline` using the mean of the training dataset. Run the processing job in the closest region to the end-user to improve latency and model performance.",
        "C": "Use SageMaker Clarify processing jobs with the `DataConfig` parameter pointing to the S3 bucket encrypted with AWS KMS, set `ModelConfig` to include both SHAP-based explainability and bias metrics, and specify `ShapBaseline` using a randomly selected subset of the training dataset. Run the processing job in the region closest to the compliance authority to meet regulatory requirements.",
        "D": "Use SageMaker Clarify processing jobs with the `DataConfig` parameter pointing to the S3 bucket encrypted with AWS KMS, set `ModelConfig` to include bias metrics only, and specify `ShapBaseline` using the full training dataset. Run the processing job in the same region as the data for compliance and cost optimization."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because it ensures both bias detection and SHAP-based explainability metrics are configured, uses a representative dataset for `ShapBaseline` to improve accuracy, and runs the processing job in the same region as the data to minimize costs and comply with data residency regulations. Option B fails because it only includes SHAP-based explainability and not bias metrics, which is a regulatory requirement. Option C fails because using a randomly selected subset for `ShapBaseline` can lead to inaccurate explainability results, and running the job in a region closest to the compliance authority may lead to higher data transfer costs and compliance risks. Option D fails because it excludes SHAP-based explainability, which is necessary to support explainability for both tabular and text data, and using the full training dataset for `ShapBaseline` may lead to unnecessary computation costs without improving accuracy.",
      "topic": "SageMaker Clarify bias detection",
      "difficulty": "hard",
      "category": "sagemaker",
      "aws_services": [
        "Amazon SageMaker",
        "AWS KMS",
        "Amazon S3"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-configure-processing-jobs.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:49:38.423574",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 52
    },
    {
      "question": "A financial services company is building a machine learning model to predict credit risk. The model is trained on a large dataset stored in Amazon S3, and they are using Amazon SageMaker for training. The company has strict compliance requirements to minimize training costs while ensuring optimal model accuracy. Additionally, the ML model must be deployed quickly to meet regulatory deadlines. They are considering hyperparameter tuning strategies and are debating between using Bayesian optimization or random search for tuning. The cost of GPU instances is a concern, and the company cannot exceed 50 concurrent training jobs due to resource limits in their AWS account. They must also ensure the solution integrates with SageMaker's managed features for automatic scaling and monitoring. Which solution BEST meets these requirements?",
      "options": {
        "A": "Use SageMaker Hyperparameter Tuning Jobs with the 'Bayesian' strategy, setting `MaxParallelTrainingJobs` to 50 and `MaxRuntimeInSeconds` to limit expensive GPU instance usage. Configure `ObjectiveMetricName` to focus on the key performance metric, ensuring SageMaker automatically stops underperforming jobs.",
        "B": "Use SageMaker Hyperparameter Tuning Jobs with the 'Random' strategy, setting `MaxParallelTrainingJobs` to 10 and `MaxRuntimeInSeconds` to limit GPU costs. Use `ObjectiveMetricName` for the key metric and rely on random search to explore a broader hyperparameter space for faster deployment.",
        "C": "Use SageMaker Hyperparameter Tuning Jobs with the 'Bayesian' strategy and set `MaxParallelTrainingJobs` to 20. Omit `MaxRuntimeInSeconds` to allow SageMaker to fully explore each job, and configure `ObjectiveMetricName` to focus on the key performance metric.",
        "D": "Use SageMaker Hyperparameter Tuning Jobs with the 'Random' strategy, setting `MaxParallelTrainingJobs` to 50 while omitting `MaxRuntimeInSeconds`. Configure `ObjectiveMetricName` to optimize the key performance metric and prioritize broader hyperparameter exploration over cost constraints."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because it uses the Bayesian optimization strategy, which is more efficient than random search for finding the best hyperparameters with fewer training jobs. The `MaxParallelTrainingJobs` is set to 50 to meet the company's resource limit, and `MaxRuntimeInSeconds` ensures cost control on expensive GPU instances. Option B fails because random search is less efficient compared to Bayesian optimization, and setting `MaxParallelTrainingJobs` to 10 unnecessarily underutilizes the available resource limit. Option C fails because omitting `MaxRuntimeInSeconds` can lead to uncontrolled costs, especially with GPU instances, even though Bayesian optimization is used. Option D fails because random search is less efficient for minimizing costs and optimizing accuracy within the constraints, even though it utilizes the maximum parallel jobs allowed.",
      "topic": "Hyperparameter tuning - Bayesian vs random",
      "difficulty": "medium",
      "category": "sagemaker",
      "aws_services": [
        "sagemaker",
        "s3"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:50:01.837429",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 53
    },
    {
      "question": "A financial technology company wants to integrate bias detection into their machine learning workflow using SageMaker Clarify. Their primary goal is to ensure compliance with global financial regulations requiring bias audits on AI models used for loan approvals. The company's models process highly sensitive Personally Identifiable Information (PII), and they must ensure data security during processing. Additionally, the company needs to minimize operational costs while still meeting a strict Service Level Agreement (SLA) of completing bias detection within 4 hours for datasets up to 500 GB. They are using SageMaker Processing Jobs to run the bias detection tasks. Which solution BEST meets these requirements?",
      "options": {
        "A": "Use SageMaker Clarify Processing Jobs with an encrypted Amazon S3 bucket for input/output, enable KMS encryption for all data transfers, configure the `DataConfig` parameter with `features_attributes` to correctly identify PII columns, and set the `InstanceType` to `ml.m5.4xlarge` for sufficient performance under the SLA.",
        "B": "Use SageMaker Clarify Processing Jobs with an encrypted Amazon S3 bucket for input/output, enable KMS encryption for all data transfers, configure the `DataConfig` parameter with `features_attributes` to correctly identify PII columns, and set the `InstanceType` to `ml.c5.4xlarge` to reduce costs while meeting the SLA.",
        "C": "Use SageMaker Clarify Processing Jobs with an encrypted Amazon S3 bucket for input/output, configure the `DataConfig` parameter with `features_attributes` to correctly identify PII columns, disable KMS encryption to reduce costs, and set the `InstanceType` to `ml.m5.4xlarge` for performance within the SLA.",
        "D": "Use SageMaker Clarify Processing Jobs with an encrypted Amazon S3 bucket for input/output, enable KMS encryption for all data transfers, configure the `DataConfig` parameter with `features_attributes` to identify categorical columns instead of PII, and set the `InstanceType` to `ml.m5.2xlarge` to minimize costs while meeting the SLA."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because it ensures compliance by identifying PII columns accurately using `features_attributes`, uses encryption for data security, and selects an `InstanceType` (`ml.m5.4xlarge`) that balances cost and performance to meet the SLA for processing 500 GB datasets. Option B fails because the `ml.c5.4xlarge` instance type prioritizes compute optimization but lacks sufficient memory for large datasets, risking SLA failure. Option C fails because disabling KMS encryption compromises data security, which is critical for handling PII in financial applications. Option D fails because using `features_attributes` to identify categorical columns does not fulfill the requirement to identify PII, leading to improper bias detection results.",
      "topic": "SageMaker Clarify bias detection",
      "difficulty": "hard",
      "category": "sagemaker",
      "aws_services": [
        "SageMaker",
        "Amazon S3",
        "AWS KMS"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-configure-processing-jobs.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:50:21.457809",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 54
    },
    {
      "question": "A healthcare analytics company needs to preprocess large volumes of patient data (10 TB daily) for machine learning model training on Amazon SageMaker. The preprocessing involves complex transformations, including feature engineering and normalization, requiring Python-based scripts. The company must ensure compliance with HIPAA regulations, optimize processing costs, and meet a processing time SLA of 2 hours per batch. The solution should also minimize the risk of data exposure during transit or storage. Which solution BEST meets these requirements?",
      "options": {
        "A": "Use SageMaker Processing jobs with an encrypted Amazon S3 bucket for input/output, configure the instance type to ml.m5.4xlarge for cost efficiency, and enable VPC-only access with private subnets and security groups to prevent data from leaving the VPC. Ensure all data is encrypted in transit using HTTPS and in storage using SSE-S3.",
        "B": "Use SageMaker Processing jobs with an encrypted Amazon S3 bucket for input/output, configure the instance type to ml.m5.8xlarge for faster processing, and enable VPC-only access with private subnets and security groups to prevent data from leaving the VPC. Ensure all data is encrypted in transit using HTTPS and in storage using SSE-S3.",
        "C": "Use SageMaker Processing jobs with an encrypted Amazon S3 bucket for input/output, configure the instance type to ml.m5.xlarge for cost savings, and enable VPC-only access with public subnets and security groups. Ensure all data is encrypted using SSE-KMS managed keys for compliance.",
        "D": "Use SageMaker Processing jobs with an encrypted Amazon S3 bucket for input/output, configure the instance type to ml.m5.4xlarge for cost efficiency, and enable VPC-only access with private subnets and security groups to prevent data from leaving the VPC. Ensure data encryption in storage using SSE-S3 but skip encryption in transit for performance."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because it balances cost efficiency (ml.m5.4xlarge instances), compliance (SSE-S3 encryption for storage and HTTPS for transit), and security (private subnets and VPC-only access). Option B fails because while it uses a faster instance type (ml.m5.8xlarge), it sacrifices cost optimization without meeting the SLA requirements more effectively. Option C fails because using public subnets increases the risk of data exposure, violating HIPAA compliance. Option D fails because skipping encryption in transit does not meet compliance requirements for securing data during transmission.",
      "topic": "Processing jobs for data preprocessing",
      "difficulty": "medium",
      "category": "sagemaker",
      "aws_services": [
        "SageMaker",
        "Amazon S3",
        "VPC"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-processing.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:50:35.120068",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 55
    },
    {
      "question": "A pharmaceutical research company is using Amazon SageMaker to train a deep learning model for drug discovery. The model has over 1 billion parameters and requires a distributed training strategy due to memory constraints on individual GPUs. The company must meet the following requirements: \n\n1. Minimize training time while staying within a fixed budget. \n2. Ensure compliance with strict data residency regulations, ensuring all training data remains within a specific AWS Region (e.g., eu-central-1). \n3. Use a secure training pipeline to avoid exposing sensitive data in transit or at rest. \n4. Optimize GPU utilization to avoid idle resources during training.\n\nWhich solution BEST meets these requirements?",
      "options": {
        "A": "Use SageMaker model parallelism with MPI for distributed training. Configure the training job to use the `s3://` URI of a bucket in the eu-central-1 Region for input data. Enable SageMaker distributed training security features like inter-node encryption and provide a custom `sagemaker.distr_config.` file to define the model partitioning strategy.",
        "B": "Use SageMaker data parallelism with the SageMaker Distributed Data Parallel library. Configure the training job with the `input_mode='Pipe'` parameter to stream data from an S3 bucket in the eu-central-1 Region. Enable inter-container traffic encryption and use the `optimized_dp` configuration for scaling across multiple GPUs.",
        "C": "Use SageMaker model parallelism with DeepSpeed for distributed training. Store the training data in an encrypted S3 bucket in eu-central-1 and configure the training instance to use `instance_type='ml.p4d.24xlarge'`. Enable inter-node encryption and provide a custom model parallel configuration script.",
        "D": "Use SageMaker data parallelism with MPI for distributed training. Store training data in an S3 bucket in eu-central-1. Set `input_mode='FastFile'` to improve data throughput. Enable inter-node encryption and use a custom `sagemaker.distr_config.` file for advanced data sharding."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because SageMaker model parallelism is necessary for handling the memory requirements of a model with over 1 billion parameters. MPI is the recommended communication backend for model parallelism in SageMaker, and the custom `sagemaker.distr_config.` file allows fine-grained control over model partitioning. Storing data in eu-central-1 and enabling inter-node encryption ensures compliance with the data residency and security requirements.\n\nOption B fails because SageMaker data parallelism is not suitable for training models with extreme memory constraints, as it duplicates the model on all GPUs rather than partitioning it. Additionally, the `optimized_dp` configuration is specific to SageMaker Data Parallel, which is not the correct strategy here.\n\nOption C fails because while DeepSpeed supports model parallelism, SageMaker's native model parallelism implementation is better integrated into the environment and provides more direct support for compliance features like inter-node encryption.\n\nOption D fails because MPI with SageMaker data parallelism is an incorrect combination for this use case. Data parallelism duplicates model weights across GPUs, which would exceed memory constraints for a model of this size. Also, `input_mode='FastFile'` is not suitable for streaming large datasets efficiently in this scenario.",
      "topic": "Distributed training - data parallel vs model parallel",
      "difficulty": "hard",
      "category": "sagemaker",
      "aws_services": [
        "Amazon SageMaker",
        "Amazon S3"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-training.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:50:57.977358",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 56
    },
    {
      "question": "A healthcare company is building a machine learning model to classify medical imaging data (e.g., X-rays) into multiple categories. To achieve this, they need to label a large dataset of sensitive patient images using Amazon SageMaker Ground Truth. Due to strict HIPAA compliance requirements, all data must remain encrypted at rest and in transit. The labeling process must minimize human involvement to reduce costs, while still ensuring high accuracy for edge cases. The companys budget is constrained, so they want to avoid unnecessary compute costs but still need to meet compliance requirements. Additionally, they require the labeling workflow to support multiple iterations, enabling them to refine their model over time. Which solution BEST meets these requirements?",
      "options": {
        "A": "Use Amazon SageMaker Ground Truth with a private workforce. Enable Amazon S3 server-side encryption (SSE-KMS) for data storage, configure a bounding box labeling task type, and set up automated labeling for active learning. Use the 'auto-labeling-only' mode for edge-case refinement in subsequent iterations.",
        "B": "Use Amazon SageMaker Ground Truth with a public workforce. Enable Amazon S3 server-side encryption (SSE-S3) for data storage, configure an image classification task type, and rely solely on automated labeling to minimize costs. Perform manual reviews for edge cases outside of automated labeling confidence thresholds.",
        "C": "Use Amazon SageMaker Ground Truth with a private workforce. Enable Amazon S3 server-side encryption (SSE-KMS) for data storage, configure an image classification task type, and set up automated labeling for active learning. Utilize the 'manual-labeling-first' mode to refine edge cases in subsequent iterations.",
        "D": "Use Amazon SageMaker Ground Truth with a vendor-managed workforce. Enable Amazon S3 server-side encryption (SSE-C) for data storage, configure a semantic segmentation task type, and set up automated labeling for active learning. Rely on the vendor-managed workforce to manually label edge cases as needed."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because it uses a private workforce for compliance with HIPAA, ensures data encryption with SSE-KMS, supports cost optimization with automated labeling, and uses 'auto-labeling-only' mode for subsequent iteration refinement to minimize human involvement. Option B fails because using a public workforce does not meet HIPAA compliance requirements, and SSE-S3 does not provide the same level of control as SSE-KMS. Option C fails because 'manual-labeling-first' mode increases costs and human involvement, which contradicts the requirement to minimize these. Option D fails because SSE-C requires the company to manage encryption keys, which can add operational complexity and increase risks in a highly regulated environment.",
      "topic": "Ground Truth labeling workflows",
      "difficulty": "medium",
      "category": "sagemaker",
      "aws_services": [
        "Amazon SageMaker Ground Truth",
        "Amazon S3"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/sagemaker/latest/dg/sms-workflows.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:51:14.703889",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 57
    },
    {
      "question": "A global e-commerce company needs to deploy a machine learning model for fraud detection. The model must handle unpredictable traffic spikes during sales events, requiring low-latency predictions (under 100ms). Compliance regulations mandate that customer data processed in the system must remain encrypted at rest and in transit. Additionally, the team must minimize operational overhead and only pay for inference when requests are made. To accommodate potential future use cases, the solution should also support batch processing for large historical datasets. Which solution BEST meets these requirements?",
      "options": {
        "A": "Use Amazon SageMaker Serverless Inference with an encrypted Amazon S3 bucket for model artifacts. Configure a SageMaker endpoint with model data encrypted using an AWS Key Management Service (KMS) key. For batch processing, use SageMaker Batch Transform jobs with the same encrypted S3 bucket.",
        "B": "Use Amazon SageMaker Real-Time Inference with an auto-scaling endpoint. Store model artifacts in an S3 bucket encrypted with a KMS key. For batch processing, use SageMaker Processing jobs with the same S3 bucket.",
        "C": "Use Amazon SageMaker Real-Time Inference with an encrypted Amazon S3 bucket for model artifacts and a KMS key for encryption. Deploy a provisioned endpoint instance with auto-scaling enabled. For batch processing, use SageMaker Batch Transform jobs with the same encrypted S3 bucket.",
        "D": "Use Amazon SageMaker Serverless Inference with an encrypted Amazon S3 bucket for model artifacts. Configure a SageMaker serverless endpoint with model data encryption using an AWS KMS key. For batch processing, use SageMaker Processing jobs with the same encrypted S3 bucket."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because SageMaker Serverless Inference minimizes operational overhead and costs by scaling to zero when idle, making it ideal for handling unpredictable traffic. It also supports data encryption via KMS and integrates with Batch Transform for historical data processing. Option B fails because Real-Time Inference incurs costs when endpoints are idle, violating the cost-efficiency requirement. Option C is incorrect because provisioning instances for Real-Time Inference increases operational overhead and cost, which is unnecessary for unpredictable traffic. Option D fails because SageMaker Processing jobs are not suitable for batch inference; Batch Transform should be used for inference on large datasets.",
      "topic": "Inference endpoints - real-time vs serverless vs async",
      "difficulty": "hard",
      "category": "sagemaker",
      "aws_services": [
        "Amazon SageMaker",
        "Amazon S3",
        "AWS Key Management Service (KMS)"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:51:33.023293",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 58
    },
    {
      "question": "A financial services company needs to preprocess large volumes of customer transaction data for machine learning model training. The data preprocessing pipeline must handle CSV files ranging from 100 GB to 300 GB stored in Amazon S3. The company requires high performance with minimal cost, while ensuring compliance with industry regulations like PCI DSS. Data must remain encrypted during processing, and they need to track resource utilization and job completion metrics for auditing purposes. Additionally, the preprocessing job must scale automatically based on the input size and avoid manual intervention. Which solution BEST meets these requirements?",
      "options": {
        "A": "Use Amazon SageMaker Processing Jobs with an encrypted Amazon S3 bucket as the input and output data store. Specify a custom Docker container with PCI DSS-compliant libraries for preprocessing and configure the instance type to ml.m5.4xlarge for balanced cost and performance. Enable CloudWatch logging and metrics for auditing, and use SageMaker-managed encryption for data security.",
        "B": "Use Amazon SageMaker Processing Jobs with an encrypted Amazon S3 bucket as the input and output data store. Specify a custom Docker container with PCI DSS-compliant libraries for preprocessing and configure the instance type to ml.m5.12xlarge for maximum performance. Enable CloudWatch logging and metrics for auditing, and use KMS-managed encryption for data security.",
        "C": "Use Amazon SageMaker Processing Jobs with an encrypted Amazon S3 bucket as the input data store and Amazon EFS for output storage to support frequent access during preprocessing. Specify a custom Docker container with PCI DSS-compliant libraries and configure the instance type to ml.m5.4xlarge. Enable CloudWatch logging but use EFS-managed encryption instead of SageMaker-managed encryption for compliance.",
        "D": "Use Amazon SageMaker Processing Jobs with an encrypted Amazon S3 bucket as the input and output data store. Use the built-in scikit-learn image for preprocessing and configure the instance type to ml.m5.2xlarge for cost efficiency. Enable CloudWatch logging and metrics for auditing, and use SageMaker-managed encryption for data security."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because it balances cost and performance by using ml.m5.4xlarge instances and leverages SageMaker-managed encryption, which simplifies compliance with PCI DSS. It also tracks metrics and logs using CloudWatch, fulfilling auditing requirements. Option B is incorrect because it uses ml.m5.12xlarge instances, which are unnecessarily expensive for preprocessing tasks of this scale. Additionally, KMS-managed encryption is not explicitly required for this scenario. Option C is incorrect because using Amazon EFS for output storage introduces higher complexity and cost without a clear benefit. EFS-managed encryption does not simplify compliance compared to SageMaker's managed encryption. Option D is incorrect because the built-in scikit-learn image lacks the PCI DSS-compliant libraries required, and ml.m5.2xlarge does not provide sufficient performance for processing 300 GB files efficiently.",
      "topic": "Processing jobs for data preprocessing",
      "difficulty": "medium",
      "category": "sagemaker",
      "aws_services": [
        "Amazon S3",
        "Amazon SageMaker Processing Jobs",
        "AWS CloudWatch",
        "AWS Key Management Service"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/sagemaker/latest/dg/processing-job.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:51:55.187105",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 59
    },
    {
      "question": "A healthcare analytics company wants to deploy multiple machine learning models for real-time patient risk prediction using Amazon SageMaker. Due to strict compliance requirements, all data must be encrypted in transit and at rest, and the solution must meet HIPAA compliance standards. The company needs to optimize costs since the models have varying traffic patterns where some models are idle for long periods while others see sporadic spikes in usage. The models are diverse in size, ranging from 50 MB to 2 GB, and latency must not exceed 200 ms for 95% of requests. The company also needs the ability to monitor model-specific metrics like invocation counts and latency. Which solution BEST meets these requirements?",
      "options": {
        "A": "Use Amazon SageMaker multi-model endpoints with an instance type like ml.m5.xlarge. Configure a custom inference pipeline to support data encryption in transit using TLS. Enable Amazon CloudWatch metrics for multi-model endpoints to monitor model-specific invocation counts and latency.",
        "B": "Use Amazon SageMaker single-model endpoints deployed on ml.m5.xlarge instances with Auto Scaling enabled. Encrypt data in transit using SageMaker's built-in HTTPS support and use Amazon CloudWatch Logs to track model performance metrics.",
        "C": "Use Amazon SageMaker multi-model endpoints with an instance type like ml.m5.xlarge. Enable Amazon CloudWatch Logs to monitor overall endpoint metrics, and configure model-specific encryption inside the model container.",
        "D": "Use Amazon SageMaker single-model endpoints with ml.m5.large instances to minimize costs. Use SageMaker's built-in HTTPS support for data encryption in transit and enable Auto Scaling for instances to handle traffic spikes."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because SageMaker multi-model endpoints are designed to host multiple models on a single endpoint, reducing costs significantly for workloads with varying traffic patterns. It also allows encryption in transit using TLS and enables model-specific metric tracking via Amazon CloudWatch. Option B is incorrect because single-model endpoints are less cost-effective for workloads with idle models or sporadic spikes, and while they allow HTTPS encryption, they do not support the cost optimization needed. Option C is incorrect because while it uses multi-model endpoints, model-specific encryption inside the container does not meet the requirement for encryption in transit. Additionally, it does not enable proper monitoring of per-model metrics. Option D is incorrect because single-model endpoints do not optimize costs for multiple models and may lead to over-provisioning during low traffic periods.",
      "topic": "Multi-model endpoints cost optimization",
      "difficulty": "hard",
      "category": "sagemaker",
      "aws_services": [
        "Amazon SageMaker",
        "Amazon CloudWatch"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-endpoints.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:52:09.454090",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 60
    },
    {
      "question": "A global e-commerce company needs to implement a machine learning model for personalized product recommendations. The model must process large datasets of user behavior logs, which amount to approximately 10 TB daily, and must generate recommendations in near real-time (under 150 ms latency). The company has strict compliance requirements to encrypt all data at rest and in transit, with regional data residency constraints (e.g., EU data must remain in the EU). Cost optimization is a priority, and the solution should minimize operational overhead and utilize serverless technologies where possible. The architecture must also support model training every 24 hours using updated datasets. Which solution BEST meets these requirements?",
      "options": {
        "A": "Use Amazon SageMaker to train the model and deploy it as a serverless endpoint using SageMaker Asynchronous Inference. Configure input data storage in Amazon S3 with server-side encryption using SSE-KMS and bucket policies for regional access control. Use Amazon SQS to batch inference requests and ensure compliance with data residency using VPC endpoints.",
        "B": "Use Amazon SageMaker to train the model and deploy it as a real-time endpoint using SageMaker Real-Time Inference. Store input data in Amazon S3 with server-side encryption using SSE-S3 and use bucket policies for regional access control. Batch inference requests using Amazon EventBridge and ensure compliance with data residency using VPC endpoints.",
        "C": "Use Amazon SageMaker to train the model and deploy it as a serverless endpoint using SageMaker Asynchronous Inference. Store input data in Amazon DynamoDB with regional replication enabled for data residency compliance. Batch inference requests using Amazon SNS and ensure encryption in transit using TLS 1.2.",
        "D": "Use Amazon SageMaker to train the model and deploy it as a real-time endpoint using SageMaker Real-Time Inference. Configure input data storage in Amazon S3 with server-side encryption using SSE-KMS and bucket policies for regional access control. Batch inference requests using AWS Step Functions and ensure compliance with regional data residency using VPC endpoints."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because SageMaker Asynchronous Inference minimizes cost by using serverless endpoints, which are ideal for batch processing large datasets without requiring a fully provisioned endpoint. Amazon S3 with SSE-KMS provides compliance-grade encryption, and SQS is suitable for batching inference requests while ensuring near real-time processing. Option B fails because SageMaker Real-Time Inference is designed for low-latency, on-demand predictions, which would increase costs unnecessarily for batch processing. Additionally, SSE-S3 encryption does not meet compliance requirements as strictly as SSE-KMS. Option C fails because DynamoDB is not optimized for storing large datasets like user behavior logs (10 TB daily), leading to operational inefficiencies and higher costs. SNS lacks the queuing mechanics needed for batch processing at scale. Option D fails because AWS Step Functions introduces unnecessary orchestration complexity for batching inference requests, which is more efficiently handled by SQS in this scenario.",
      "topic": "Cost optimization - model selection and batching",
      "difficulty": "hard",
      "category": "architecture",
      "aws_services": [
        "Amazon SageMaker",
        "Amazon S3",
        "Amazon SQS",
        "AWS Key Management Service (KMS)",
        "VPC Endpoints"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/sagemaker/latest/dg/inference-options.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:52:27.319001",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 61
    },
    {
      "question": "A global e-commerce company is implementing a Retrieval-Augmented Generation (RAG) architecture to power its intelligent product search and recommendation system. The system must retrieve relevant product data from a vast dataset stored in Amazon OpenSearch Service and re-rank the results using a custom ML model hosted on Amazon SageMaker. The company has the following requirements:\n\n1. The retrieval must support vector search for product embeddings with low latency (<50ms per query) across 10 million items.\n2. The re-ranking model must handle 10,000 queries per second with minimal inference latency (<10ms per query).\n3. The architecture must minimize operational overhead while staying within a budget of $5,000/month.\n4. All data must be encrypted in transit and at rest to comply with GDPR and PCI DSS standards.\n\nWhich solution BEST meets these requirements?",
      "options": {
        "A": "Use OpenSearch Serverless with the HNSW algorithm for vector search (k=10) and cosinesimil distance metric. Deploy the re-ranking model on Amazon SageMaker Real-Time Endpoint with Multi-Model Endpoint (MME) enabled, using ml.m5.4xlarge instances. Configure VPC endpoints for secure communication and enable AWS Key Management Service (KMS) for encryption.",
        "B": "Use OpenSearch Serverless with the IVF algorithm for vector search (k=10) and cosinesimil distance metric. Deploy the re-ranking model on Amazon SageMaker Real-Time Endpoint with Multi-Model Endpoint (MME) enabled, using ml.m5.xlarge instances. Configure VPC endpoints for secure communication and enable AWS Key Management Service (KMS) for encryption.",
        "C": "Use OpenSearch Serverless with the HNSW algorithm for vector search (k=10) and l2 distance metric. Deploy the re-ranking model on Amazon SageMaker Real-Time Endpoint with Multi-Model Endpoint (MME) enabled, using ml.m5.4xlarge instances. Configure VPC endpoints for secure communication and enable AWS Key Management Service (KMS) for encryption.",
        "D": "Use OpenSearch Serverless with the HNSW algorithm for vector search (k=25) and cosinesimil distance metric. Deploy the re-ranking model on Amazon SageMaker Real-Time Endpoint with Single-Model Endpoint configuration, using ml.m5.4xlarge instances. Configure VPC endpoints for secure communication and enable AWS Key Management Service (KMS) for encryption."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because the HNSW algorithm is optimized for high-performance vector search with low latency, and cosinesimil is the appropriate distance metric for embedding similarity in this use case. SageMaker Real-Time Endpoint with Multi-Model Endpoint (MME) on ml.m5.4xlarge instances balances cost and performance for 10,000 queries per second, staying within the budget. VPC endpoints and KMS ensure compliance with GDPR and PCI DSS.\n\nOption B fails because the IVF algorithm is less efficient for high recall and low latency in large-scale vector searches compared to HNSW.\n\nOption C fails because the l2 distance metric is not suitable for cosine similarity-based embeddings, leading to incorrect search results.\n\nOption D fails because using k=25 increases query latency beyond 50ms, and Single-Model Endpoint configuration results in higher operational costs compared to Multi-Model Endpoint for this workload.\n",
      "topic": "RAG architecture - query routing and re-ranking",
      "difficulty": "hard",
      "category": "architecture",
      "aws_services": [
        "Amazon OpenSearch Service",
        "Amazon SageMaker",
        "AWS Key Management Service (KMS)",
        "VPC endpoints"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/opensearch-service/latest/developerguide/vector-search.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:52:49.877429",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 62
    },
    {
      "question": "A legal tech company processes highly sensitive client contracts and must extract key clauses from thousands of multi-page PDF documents daily. The system must meet the following requirements: \n\n1. Handle up to 10,000 documents per day, with each document averaging 500 pages. The solution must scale automatically to varying workloads. \n2. Extract text using Amazon Textract and apply custom parsing logic. \n3. Ensure compliance with regulatory requirements such as encryption in transit and at rest (e.g., using KMS). \n4. Minimize costs while maintaining high throughput. \n5. Ensure that the processing pipeline is resilient to partial failures and can retry failed tasks without reprocessing the entire document. \n6. Provide detailed monitoring and tracing for debugging and operational visibility. \n\nWhich solution BEST meets these requirements?",
      "options": {
        "A": "Use Step Functions with a Map state to chunk each document into pages and invoke an AWS Lambda function for text extraction. Use Amazon Textract's DetectDocumentText API. Store intermediate results in Amazon S3 with bucket-level KMS encryption. Configure Step Functions with an exponential retry policy for failed tasks and use Amazon CloudWatch for monitoring.",
        "B": "Use Step Functions with a Parallel state to process multiple pages simultaneously. Invoke an AWS Lambda function that calls Amazon Textract's AnalyzeDocument API for key-value pair extraction. Store intermediate results in Amazon DynamoDB with KMS encryption enabled. Configure Step Functions with an exponential retry policy for failed tasks and use X-Ray for tracing.",
        "C": "Use AWS Lambda with an S3 event trigger for each uploaded document. Split the document into pages within the Lambda function and call Amazon Textract's DetectDocumentText API for text extraction. Store intermediate results in Amazon DynamoDB with encryption enabled. Use Amazon CloudWatch Logs for monitoring and retry failed invocations using Lambda's built-in retry mechanism.",
        "D": "Use Step Functions with a Map state to process each document page-by-page. Invoke an AWS Lambda function for each page that calls Amazon Textract's AnalyzeDocument API. Store intermediate results in Amazon S3 with KMS encryption enabled. Configure Step Functions with an error catching and retry policy for failed tasks, and use X-Ray for detailed tracing."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because Step Functions with a Map state efficiently handles chunking by processing each page as a separate item. Amazon Textract's DetectDocumentText API is appropriate for extracting raw text from large documents, which aligns with the requirement to extract key clauses downstream. Storing intermediate results in Amazon S3 with KMS encryption meets regulatory compliance, and the exponential retry policy ensures resilience to partial failures. Amazon CloudWatch provides detailed monitoring.\n\nOption B fails because the AnalyzeDocument API is optimized for key-value pair extraction rather than raw text extraction, which is unnecessary for the initial text parsing requirement. Additionally, storing intermediate results in DynamoDB is costlier and less suited for bulk page-level storage compared to S3.\n\nOption C fails because Lambda alone does not provide the orchestration required for complex workflows with retries and partial failure handling. Moreover, splitting the document into pages within the Lambda function can lead to memory and timeout issues for large documents.\n\nOption D fails because using the AnalyzeDocument API for each page is computationally expensive and unnecessary for the initial text extraction. While X-Ray is useful for tracing, it does not justify the cost and complexity introduced by this approach.",
      "topic": "Chunking pipeline architecture - Lambda vs Step Functions for document processing",
      "difficulty": "hard",
      "category": "architecture",
      "aws_services": [
        "AWS Step Functions",
        "AWS Lambda",
        "Amazon Textract",
        "Amazon S3",
        "AWS X-Ray"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/step-functions/latest/dg/concepts-amazons3.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:53:14.696779",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 63
    },
    {
      "question": "A global financial services company is building a hybrid AI/ML architecture for real-time fraud detection. They want to use Amazon Bedrock for generating embeddings from transaction text data via foundation models and Amazon SageMaker for custom fraud detection model training and endpoint hosting. The company must adhere to strict compliance regulations, ensuring data residency within specific AWS Regions (e.g., us-east-1 and eu-west-1) and avoid any sensitive customer data leaving these regions. Additionally, the architecture must minimize latency for real-time predictions while managing costs effectively. The fraud detection system must scale to handle peak transaction volumes of 10,000 requests per second globally. Which solution BEST meets these requirements?",
      "options": {
        "A": "Use Amazon Bedrock with a foundation model like Amazon Titan to generate embeddings via the `invoke_model()` API in the us-east-1 and eu-west-1 Regions. Pass the embeddings to SageMaker endpoints hosted in respective Regions for inference. Use SageMaker Multi-Model Endpoints to minimize costs and Amazon CloudFront for global content delivery.",
        "B": "Use Amazon Bedrock with a foundation model like Amazon Titan to generate embeddings via the `invoke_model()` API in us-east-1 only. Pass the embeddings to SageMaker endpoints hosted in us-east-1 for inference, using SageMaker Multi-Model Endpoints for cost savings and Amazon CloudFront for low-latency delivery worldwide.",
        "C": "Use Amazon Bedrock with a foundation model like Amazon Titan hosted in us-east-1 to generate embeddings via the `invoke_model()` API. Use SageMaker endpoints in us-east-1 and eu-west-1 for inference with SageMaker Asynchronous Endpoints to handle peak traffic, ensuring compliance by replicating embeddings to required Regions via Amazon S3 Cross-Region Replication.",
        "D": "Use Amazon Bedrock with a foundation model like Amazon Titan to generate embeddings through the `invoke_model()` API in us-east-1. Use SageMaker endpoints in us-east-1 and eu-west-1 for inference, leveraging SageMaker Serverless Inference for scalability and Amazon API Gateway for low-latency global access."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because it ensures compliance with data residency regulations by processing embeddings in-region and routing them to SageMaker endpoints in the same Region. The use of SageMaker Multi-Model Endpoints reduces costs by sharing model containers, and CloudFront provides low-latency access globally. Option B fails because it routes traffic to us-east-1 only, violating data residency policies for eu-west-1. Option C fails because SageMaker Asynchronous Endpoints are not ideal for real-time, low-latency workloads, and replicating embeddings via S3 Cross-Region Replication introduces latency and compliance risks. Option D fails because SageMaker Serverless Inference is not well-suited for sustained high-throughput, real-time use cases like fraud detection at 10,000 requests per second, leading to performance bottlenecks.",
      "topic": "Hybrid Bedrock-SageMaker architectures",
      "difficulty": "hard",
      "category": "architecture",
      "aws_services": [
        "Amazon Bedrock",
        "Amazon SageMaker",
        "Amazon CloudFront",
        "Amazon S3"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-endpoints.html, https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:53:50.026342",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 64
    },
    {
      "question": "An e-commerce company is implementing a Retrieval-Augmented Generation (RAG) solution to recommend products based on user queries. Their architecture uses Amazon SageMaker for embedding generation and OpenSearch Serverless for vector search. The company wants to optimize latency for real-time recommendations. They need to cache embeddings and retrieval results to reduce the frequency of expensive queries to SageMaker and OpenSearch. Constraints include: minimizing costs while maintaining sub-100ms response times, complying with GDPR (ensuring user data is encrypted at rest and in transit), and supporting up to 50 million queries per day. Which solution BEST meets these requirements?",
      "options": {
        "A": "Use Amazon ElastiCache for Redis with data encryption enabled to cache embeddings and retrieval results. Configure Redis with cluster mode enabled, set the maxmemory-policy to 'volatile-lru', and use Redis TTLs to expire entries after 10 minutes. Integrate AWS KMS for encryption and ensure data at rest uses AES-256 encryption.",
        "B": "Use Amazon ElastiCache for Redis with data encryption enabled to cache embeddings and retrieval results. Configure Redis with cluster mode enabled, set the maxmemory-policy to 'allkeys-lru', and use Redis TTLs to expire entries after 30 minutes. Integrate AWS KMS for encryption and ensure data at rest uses AES-256 encryption.",
        "C": "Use Amazon DynamoDB with on-demand mode to store embeddings and retrieval results. Configure DynamoDB with TTL enabled for items to expire after 10 minutes and encrypt data at rest using AWS KMS. Use DynamoDB Accelerator (DAX) to reduce latency for read operations.",
        "D": "Use Amazon MemoryDB for Redis with data encryption enabled to cache embeddings and retrieval results. Configure Redis with cluster mode enabled, set the maxmemory-policy to 'volatile-ttl', and use Redis TTLs to expire entries after 10 minutes. Integrate AWS KMS for encryption and ensure data at rest uses AES-256 encryption."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because it uses Amazon ElastiCache for Redis with cluster mode enabled, 'volatile-lru' eviction policy (ideal for caching frequently accessed data), and a 10-minute TTL to balance cost and performance under high traffic. It ensures compliance with GDPR through encryption in transit and at rest using AWS KMS. Option B fails because 'allkeys-lru' eviction policy is less optimal for caching embeddings and retrieval results, as it evicts data indiscriminately rather than focusing on least-used entries. Option C fails because DynamoDB with DAX improves read latency but lacks the native caching capabilities of Redis and may incur higher costs for 50 million queries/day. Option D fails because 'volatile-ttl' eviction policy is less optimal than 'volatile-lru' for this workload, as it prioritizes TTL over usage patterns.",
      "topic": "RAG latency optimization - caching embeddings and retrieval results",
      "difficulty": "hard",
      "category": "architecture",
      "aws_services": [
        "Amazon ElastiCache for Redis",
        "AWS KMS",
        "Amazon DynamoDB",
        "Amazon MemoryDB for Redis"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/Introduction.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:54:06.615216",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 65
    },
    {
      "question": "A healthcare analytics company is building a real-time patient monitoring system that uses Amazon Bedrock for generating AI-driven healthcare insights. They need to preprocess incoming patient data (from IoT devices) before sending it to Bedrock's generative AI models and perform post-processing to format the output for downstream systems. Due to strict HIPAA compliance requirements, all data must remain encrypted during transit and storage, and Lambda functions should not retain state. The company also needs to optimize costs while ensuring low-latency responses for critical patient alerts. Which solution BEST meets these requirements?",
      "options": {
        "A": "Use AWS Lambda with Amazon KMS to encrypt incoming data, and configure Lambda layers for preprocessing. Send the data to Amazon Bedrock using the InvokeModel API. For post-processing, use another Lambda function with Amazon S3 for encrypted temporary storage. Ensure Lambda concurrency limits are optimized for low-latency processing, and use Provisioned Concurrency for critical functions.",
        "B": "Use AWS Lambda with Amazon KMS for encryption and configure Lambda layers for preprocessing. Send the data to Amazon Bedrock using the InvokeModel API. For post-processing, use another Lambda function with Amazon DynamoDB for encrypted temporary storage and enable on-demand scaling for DynamoDB tables to optimize costs and latency.",
        "C": "Use AWS Lambda with Amazon KMS to encrypt incoming data, and configure Lambda layers for preprocessing. Send the data to Amazon Bedrock using the InvokeModel API. For post-processing, use another Lambda function with Amazon S3 for temporary storage without encryption. Ensure Lambda concurrency limits are optimized for low-latency processing, and use Auto Scaling for critical functions.",
        "D": "Use AWS Lambda with AWS Secrets Manager for encryption keys and configure Lambda layers for preprocessing. Send the data to Amazon Bedrock using the InvokeModel API. For post-processing, use another Lambda function with Amazon S3 for encrypted temporary storage. Ensure Lambda concurrency limits are optimized for low-latency processing, and use Provisioned Concurrency for critical functions."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because it uses Amazon KMS for HIPAA-compliant encryption, Lambda layers for efficient preprocessing, and Provisioned Concurrency to ensure low-latency responses while adhering to compliance requirements. Option B fails because Amazon DynamoDB is not HIPAA-eligible for temporary storage of patient data. Option C fails because it does not encrypt data stored in Amazon S3, violating HIPAA compliance. Option D fails because AWS Secrets Manager is not intended for encrypting incoming data; it is designed for managing secrets such as credentials.",
      "topic": "Lambda integration for Bedrock pre/post processing",
      "difficulty": "medium",
      "category": "architecture",
      "aws_services": [
        "AWS Lambda",
        "Amazon Bedrock",
        "Amazon KMS",
        "Amazon S3"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/lambda/latest/dg/intro-core-concepts.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:54:18.645986",
      "critique": {
        "overall_score": 9.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [
          "The explanation for Option D could be slightly misleading. While AWS Secrets Manager is not typically used for encrypting incoming data, it can manage encryption keys that could theoretically be used for this purpose. However, this is not a best practice for the described scenario."
        ],
        "verification_status": "APPROVED",
        "confidence": 0.95
      },
      "verification_status": "APPROVED",
      "id": 66
    },
    {
      "question": "A global e-commerce company wants to implement an AI-driven product recommendation system using Step Functions to orchestrate requests to Amazon Bedrock. The system must process user events in real time, generate personalized recommendations using a foundation model in Bedrock, and return results within 500 milliseconds. The solution must comply with GDPR requirements, ensuring that no personally identifiable information (PII) is stored or logged in any service. Additionally, the architecture must minimize costs and ensure high availability across multiple AWS Regions. Which solution BEST meets these requirements?",
      "options": {
        "A": "Use a Step Functions Express Workflow with a Lambda function to preprocess user events and invoke Bedrock's `InvokeModel` API. Use the `X-Amz-Client-Context` header for request tracing and configure Step Functions to log to an encrypted CloudWatch Logs group with GDPR-compliant log filtering. Deploy the solution using a multi-Region architecture with Lambda@Edge for low latency.",
        "B": "Use a Step Functions Standard Workflow with a Lambda function to preprocess user events and invoke Bedrock's `InvokeModel` API. Encrypt the Lambda environment variables using KMS for GDPR compliance and configure Step Functions to log to an encrypted CloudWatch Logs group. Deploy the solution in a single Region with provisioned concurrency for Lambda to meet latency requirements.",
        "C": "Use a Step Functions Express Workflow with a Lambda function to preprocess user events and invoke Bedrock's `InvokeModel` API. Enable VPC endpoints for Lambda and Bedrock to ensure security, and use CloudTrail to log API usage for compliance. Deploy the solution in a single Region with Lambda@Edge for low latency.",
        "D": "Use a Step Functions Standard Workflow with a Lambda function to preprocess user events and invoke Bedrock's `InvokeModel` API. Use Step Functions' built-in retry policies to handle transient errors, configure S3 for log storage with GDPR-compliant encryption, and deploy the solution using a multi-Region architecture with AWS Global Accelerator."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because Step Functions Express Workflows are optimized for high-performance, low-latency use cases, meeting the 500ms requirement. Lambda@Edge ensures low latency by running the preprocessing logic closer to users. Using the `X-Amz-Client-Context` header enables request tracing while maintaining GDPR compliance through encrypted CloudWatch log filtering. Multi-Region deployment ensures high availability. \n\nOption B fails because Step Functions Standard Workflows introduce higher latency than Express Workflows, making it less suitable for real-time use cases with strict latency requirements. Additionally, deploying in a single Region does not meet the high availability requirement.\n\nOption C fails because while enabling VPC endpoints enhances security, deploying in a single Region with Lambda@Edge does not align with the high availability requirement. Additionally, CloudTrail logs are not suitable for real-time GDPR-compliant log filtering.\n\nOption D fails because Step Functions Standard Workflows are not optimal for low-latency use cases, and S3 log storage introduces additional latency for logging. AWS Global Accelerator is unnecessary when Lambda@Edge can provide latency optimization.",
      "topic": "Step Functions with Bedrock orchestration",
      "difficulty": "hard",
      "category": "architecture",
      "aws_services": [
        "AWS Step Functions",
        "Amazon Bedrock",
        "AWS Lambda",
        "CloudWatch Logs",
        "Lambda@Edge",
        "AWS KMS"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/step-functions/latest/dg/concepts.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:54:38.523640",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 67
    },
    {
      "question": "A global e-commerce company is implementing a Retrieval-Augmented Generation (RAG) architecture to enhance its customer support chatbot with real-time product recommendations. To ensure sub-100ms latency, they need to optimize the retrieval and embedding storage layer. The chatbot generates embeddings using Amazon SageMaker and retrieves recommendations via vector similarity search in OpenSearch Serverless. These embeddings and retrieval results must be cached to minimize repetitive queries and reduce costs. The company has the following constraints: \n\n1. The solution must comply with GDPR regulations, ensuring that embeddings linked to user data are purged upon request.\n2. The caching layer must scale dynamically with unpredictable traffic patterns during sales events.\n3. The architecture must optimize for cost while maintaining high availability and sub-100ms latency.\n4. Security is critical: encryption at rest and in transit is required.\n\nWhich solution BEST meets these requirements?",
      "options": {
        "A": "Use Amazon DynamoDB with TTL-based expiration for caching embeddings and retrieval results. Enable DynamoDB Accelerator (DAX) for <10ms read latency. Configure encryption at rest with AWS KMS and purge user-specific embeddings using batch write operations. Integrate DynamoDB Streams to trigger cleanup processes when users request data deletion.",
        "B": "Use Amazon ElastiCache for Redis with eviction policies set to 'allkeys-lru' to cache embeddings and retrieval results. Enable Redis cluster mode for horizontal scaling. Configure encryption at rest and in transit using AWS KMS. Purge user-specific embeddings by directly deleting keys associated with user IDs.",
        "C": "Use Amazon MemoryDB for Redis with eviction policies set to 'volatile-lru' for caching embeddings and retrieval results. Enable single AZ deployment for cost optimization. Configure encryption in transit using AWS KMS and purge user-specific embeddings by deleting keys. Scale dynamically using MemoryDB's auto-scaling capabilities.",
        "D": "Use Amazon Aurora with PostgreSQL and pgvector extension for caching embeddings and retrieval results. Store embeddings in a dedicated table and retrieval results in another. Enable encryption at rest with AWS KMS and configure read replicas in multiple AZs for availability. Purge user-specific embeddings using SQL DELETE statements and configure Aurora Auto Scaling for traffic spikes."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because it uses DynamoDB with TTL for automatic expiration of cached embeddings and retrieval results, ensuring compliance with GDPR. DAX enhances read performance to meet latency requirements, and DynamoDB Streams enable efficient cleanup workflows for user data deletion. Encryption at rest and in transit ensures security. Option B fails because 'allkeys-lru' eviction in Redis does not guarantee GDPR compliance, as it relies on eviction rather than proactive deletion. Option C fails because single AZ deployment compromises availability during traffic spikes, and 'volatile-lru' eviction policy does not guarantee compliance with GDPR. Option D fails because while Aurora with pgvector supports vector search, its caching mechanism lacks the dynamic scalability and low-latency benefits of DynamoDB with DAX.",
      "topic": "RAG latency optimization - caching embeddings and retrieval results",
      "difficulty": "hard",
      "category": "architecture",
      "aws_services": [
        "Amazon DynamoDB",
        "Amazon OpenSearch Serverless",
        "Amazon SageMaker",
        "Amazon ElastiCache",
        "Amazon MemoryDB",
        "Amazon Aurora"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/dynamodb/latest/developerguide/Introduction.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:55:23.557316",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 68
    },
    {
      "question": "A large healthcare company is building an AI-driven diagnostic recommendation system using Amazon SageMaker. The company must ensure the system meets regulatory compliance standards (e.g., HIPAA) and minimizes bias in predictions to avoid discriminatory outcomes. The ML models need to be tested for bias, and human oversight must be integrated into the workflow for sensitive decisions. The solution must also optimize for cost while maintaining high performance. The company requires detailed explainability for every prediction, and all data must remain encrypted both in transit and at rest. The organization has a strict policy of using managed services to reduce operational overhead. Which solution BEST meets these requirements?",
      "options": {
        "A": "Use Amazon SageMaker Clarify for bias detection during training and inference, enable Data Encryption at Rest using AWS KMS keys, integrate Amazon A2I (Augmented AI) for human oversight on low-confidence predictions, and configure SageMaker endpoints with multi-model endpoints to optimize cost.",
        "B": "Use Amazon SageMaker Clarify for bias detection during training and inference, enable Data Encryption at Rest using AWS KMS keys, integrate Amazon Rekognition for human oversight on low-confidence predictions, and configure SageMaker endpoints with multi-model endpoints to optimize cost.",
        "C": "Use Amazon SageMaker Clarify for bias detection during training and inference, enable Data Encryption at Rest using AWS KMS keys, integrate Amazon A2I (Augmented AI) for human oversight on low-confidence predictions, and deploy SageMaker endpoints with individual endpoints for each model to ensure high performance.",
        "D": "Use Amazon SageMaker Clarify for bias detection during training only, enable Data Encryption at Rest using AWS KMS keys, integrate Amazon A2I (Augmented AI) for human oversight on all predictions, and configure SageMaker endpoints with multi-model endpoints to optimize cost."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because it uses Amazon SageMaker Clarify for comprehensive bias detection during both training and inference, integrates Amazon A2I for human oversight (which is purpose-built for such tasks), and optimizes costs by using multi-model endpoints. Option B is incorrect because Amazon Rekognition is not suitable for human oversight in this context; Amazon A2I is the correct service. Option C is incorrect because deploying individual endpoints for each model would significantly increase costs, which conflicts with the company's cost optimization requirement. Option D is incorrect because it only performs bias detection during training, missing potential bias during inference, and applies human oversight to all predictions, which can unnecessarily increase costs and operational overhead.",
      "topic": "Responsible AI - bias testing and human oversight",
      "difficulty": "hard",
      "category": "architecture",
      "aws_services": [
        "Amazon SageMaker",
        "Amazon SageMaker Clarify",
        "Amazon A2I (Augmented AI)",
        "AWS KMS"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-bias-detection.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:55:49.196724",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 69
    },
    {
      "question": "A financial services company is building a high-security payment processing system on AWS. The system must comply with PCI DSS standards, which require encryption of sensitive data both in transit and at rest. The architecture must isolate workloads by environment (e.g., dev, stage, prod) to ensure no cross-environment access. Additionally, the latency for inter-service communication must not exceed 10ms to meet real-time processing requirements. The company wants to minimize costs while ensuring compliance and security. They plan to use Amazon RDS for MySQL, and sensitive data must be encrypted with keys managed by the company. Which solution BEST meets these requirements?",
      "options": {
        "A": "Use AWS Key Management Service (KMS) with customer-managed CMKs for encrypting RDS data at rest. Configure VPCs for dev, stage, and prod with VPC Peering between them for inter-environment communication. Use TLS 1.2 for in-transit encryption and enforce IAM policies to control access to KMS keys.",
        "B": "Use AWS Key Management Service (KMS) with AWS-managed CMKs for encrypting RDS data at rest. Configure VPCs for dev, stage, and prod with AWS Transit Gateway for inter-environment communication. Use TLS 1.2 for in-transit encryption and enforce IAM policies to control access to KMS keys.",
        "C": "Use AWS Key Management Service (KMS) with customer-managed CMKs for encrypting RDS data at rest. Configure VPCs for dev, stage, and prod with AWS Transit Gateway for inter-environment communication. Use TLS 1.3 for in-transit encryption and enforce IAM policies to control access to KMS keys.",
        "D": "Use AWS Key Management Service (KMS) with customer-managed CMKs for encrypting RDS data at rest. Configure VPCs for dev, stage, and prod with VPC Peering between them for inter-environment communication. Use TLS 1.3 for in-transit encryption and enforce IAM policies to control access to KMS keys."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because it meets all the requirements: using customer-managed CMKs ensures compliance with the requirement for company-managed encryption keys, VPC Peering minimizes latency for inter-environment communication (lower than Transit Gateway), and TLS 1.2 is sufficient for PCI DSS compliance. Option B fails because AWS-managed CMKs do not meet the requirement for company-managed keys. Option C fails because while customer-managed CMKs are used correctly, TLS 1.3 is not strictly required for PCI DSS compliance and may introduce unnecessary complexity. Option D fails because TLS 1.3 is unnecessary, and while VPC Peering is correct for low-latency communication, this option does not provide any additional benefits over Option A.",
      "topic": "Security architecture - encryption and VPC isolation",
      "difficulty": "hard",
      "category": "architecture",
      "aws_services": [
        "Amazon RDS",
        "AWS KMS",
        "VPC",
        "VPC Peering",
        "AWS Transit Gateway"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/pci-dss/latest/guide/requirements.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:56:02.994447",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 70
    },
    {
      "question": "A global e-commerce company wants to implement a Retrieval-Augmented Generation (RAG) architecture to enhance its customer support chatbot. The objective is to retrieve relevant documents from a knowledge base and rerank them using a cross-encoder model before providing the top result to the Bedrock-hosted generative AI model for final response generation. The system must meet the following requirements:\n\n1. Latency must be under 200ms for document reranking and retrieval, as the chatbot operates in real-time.\n2. Minimize costs, especially for inference, as the system will handle over 2 million queries per day.\n3. Ensure compliance with GDPR by processing all customer data within the EU region.\n4. Security must follow AWS best practices, including encryption of data in transit and at rest.\n\nWhich solution BEST meets these requirements?",
      "options": {
        "A": "Use Amazon OpenSearch Service in the Frankfurt region with the kNN plugin for document retrieval, integrated with a Bedrock-hosted cross-encoder model using a custom Lambda function for reranking. Configure the cross-encoder in Bedrock to use a GPU-optimized model endpoint with auto-scaling enabled and ensure all data is encrypted using AWS KMS-managed keys.",
        "B": "Use Amazon OpenSearch Service in the Frankfurt region with the kNN plugin for document retrieval and integrate it with a Bedrock-hosted cross-encoder model. Configure the cross-encoder in Bedrock to use a CPU-optimized model endpoint with auto-scaling enabled and ensure data is encrypted using a customer-managed key (CMK).",
        "C": "Use Amazon OpenSearch Serverless in the Frankfurt region with HNSW vector search for document retrieval, integrated with a Bedrock-hosted cross-encoder model using a custom Lambda function for reranking. Configure the cross-encoder in Bedrock to use a GPU-optimized model endpoint with auto-scaling enabled and ensure all data is encrypted using AWS KMS-managed keys.",
        "D": "Use Amazon OpenSearch Serverless in the Frankfurt region with IVF vector search for document retrieval, integrated with a Bedrock-hosted cross-encoder model using a custom Lambda function for reranking. Configure the cross-encoder in Bedrock to use a GPU-optimized model endpoint with auto-scaling enabled and ensure data is encrypted using AWS KMS-managed keys."
      },
      "correct_answer": "C",
      "explanation": "Option C is correct because it uses Amazon OpenSearch Serverless with HNSW vector search, which is optimized for high-performance nearest neighbor retrieval and meets the latency requirement. The integration with Bedrock's GPU-optimized cross-encoder ensures fast inference times under heavy load, and auto-scaling helps manage cost efficiency. Additionally, the solution ensures compliance with GDPR by running in the Frankfurt region and uses AWS KMS-managed keys for encryption.\n\nOption A fails because Amazon OpenSearch Service (non-serverless) may introduce unnecessary overhead and latency compared to OpenSearch Serverless. Option B fails because using a CPU-optimized endpoint for the cross-encoder would significantly increase latency, violating the 200ms requirement. Option D fails because the IVF vector search algorithm is less efficient in terms of retrieval latency compared to HNSW, making it unsuitable for real-time use cases.",
      "topic": "RAG reranking strategies - cross-encoder integration with Bedrock",
      "difficulty": "hard",
      "category": "architecture",
      "aws_services": [
        "Amazon OpenSearch Serverless",
        "Amazon Bedrock",
        "AWS Lambda"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/bedrock/latest/userguide/what-is.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:56:29.823053",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 71
    },
    {
      "question": "A fintech company is building a real-time customer support chatbot powered by a GenAI model. The chatbot must process customer queries and provide responses while ensuring low latency (sub-50ms response time), compliance with PCI DSS for processing sensitive financial data, and cost efficiency. The company uses EventBridge for event-driven architecture and needs to integrate the GenAI model with their existing services. The model is hosted on Amazon Bedrock, and sensitive customer data must not leave the AWS environment. Logs must be securely stored for compliance reasons and encrypted using AWS-managed encryption keys (SSE-KMS). The solution must also scale automatically to handle unpredictable query volumes without impacting performance or compliance. Which solution BEST meets these requirements?",
      "options": {
        "A": "Use EventBridge to route customer queries to an AWS Lambda function. The Lambda function invokes the GenAI model hosted on Amazon Bedrock using the InvokeModel API, passing sensitive data as encrypted payloads. Configure EventBridge archive with KMS encryption for event logs, and enable Lambda reserved concurrency to ensure consistent low-latency performance.",
        "B": "Use EventBridge to route customer queries to an AWS Step Functions workflow. The workflow invokes the GenAI model hosted on Amazon Bedrock using the InvokeModel API, passing sensitive data directly in plaintext. Configure EventBridge archive with SSE-KMS for logs and use Step Functions' default retry mechanism for resilience.",
        "C": "Use EventBridge to route customer queries to an Amazon SQS queue. Configure an AWS Lambda function to poll SQS and invoke the GenAI model hosted on Amazon Bedrock using the InvokeModel API. Encrypt EventBridge logs using customer-managed KMS keys and use Lambda provisioned concurrency to manage consistent low-latency performance.",
        "D": "Use EventBridge to route customer queries to Amazon SNS, which triggers an AWS Lambda function to invoke the GenAI model on Amazon Bedrock using the InvokeModel API. Configure EventBridge archive with KMS encryption and use Lambda's default concurrency settings to handle scaling automatically."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because it ensures low-latency performance with Lambda reserved concurrency, encrypts sensitive data in transit to Amazon Bedrock, and securely archives logs with SSE-KMS. Option B fails because sensitive data is passed in plaintext, violating compliance requirements. Option C fails because SQS introduces additional latency due to polling, potentially exceeding the 50ms response time requirement. Option D fails because Lambda's default concurrency settings may not guarantee consistent low-latency performance under unpredictable query volumes.",
      "topic": "EventBridge event-driven GenAI patterns",
      "difficulty": "medium",
      "category": "architecture",
      "aws_services": [
        "EventBridge",
        "Amazon Bedrock",
        "AWS Lambda",
        "AWS KMS"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/eventbridge/latest/userguide/what-is-amazon-eventbridge.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:56:53.132378",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 72
    },
    {
      "question": "A global e-commerce company is building a hybrid retrieval system to power its product search functionality. The system must support both dense vector embeddings (for semantic relevance) and sparse vector retrieval (for keyword-based relevance). The company wants to implement vector fusion scoring to combine these approaches dynamically at query time. Due to strict regional data residency regulations, user data and search indices must remain within their respective AWS Regions. Security is critical, requiring encryption in transit and at rest. The company expects 100 million products indexed globally and 50,000 queries per second during peak seasons. Cost optimization is also a priority. Which solution BEST meets these requirements?",
      "options": {
        "A": "Use Amazon OpenSearch Service with UltraWarm storage for sparse indices and k-NN plugin with HNSW algorithm for dense vector indices. Use the Rescore API to combine sparse and dense retrieval scores dynamically during query execution. Ensure data residency by deploying OpenSearch domains in each region with VPC access, and encrypt data using AWS KMS-managed keys.",
        "B": "Use Amazon OpenSearch Service with UltraWarm storage for sparse indices and k-NN plugin with IVF algorithm for dense vector indices. Implement the Rescore API for hybrid scoring. Deploy to multiple regions for compliance, enable VPC access for security, and encrypt using AWS KMS-managed keys.",
        "C": "Use Amazon OpenSearch Service with UltraWarm storage for sparse indices and k-NN plugin with HNSW algorithm for dense vector indices. Use a custom Lambda function to compute hybrid scores instead of the Rescore API. Deploy separate OpenSearch domains per region and encrypt with AWS KMS-managed keys.",
        "D": "Use Amazon OpenSearch Service with standard storage for sparse indices and k-NN plugin with HNSW algorithm for dense vector indices. Implement the Rescore API for dynamic fusion scoring. Deploy a single global OpenSearch domain with cross-region replication for compliance, and encrypt using AWS KMS-managed keys."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because it uses Amazon OpenSearch Service with UltraWarm for cost optimization, HNSW for efficient dense vector retrieval, and the Rescore API for hybrid scoring at query time. It also ensures data residency by deploying separate domains per region and secures data with VPC access and AWS KMS-managed keys. Option B is incorrect because the IVF algorithm is less suitable for high-performance dense vector retrieval in OpenSearch compared to HNSW. Option C is incorrect because using a custom Lambda for hybrid scoring is less efficient and introduces unnecessary complexity compared to the Rescore API. Option D is incorrect because a single global OpenSearch domain with cross-region replication violates the data residency requirement and increases latency.",
      "topic": "Hybrid retrieval architecture - dense + sparse vector fusion scoring",
      "difficulty": "hard",
      "category": "architecture",
      "aws_services": [
        "Amazon OpenSearch Service",
        "AWS KMS"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/opensearch-service/latest/developerguide/knn.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:57:14.194877",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 73
    },
    {
      "question": "A global e-commerce company needs to implement a Retrieval-Augmented Generation (RAG) pipeline to improve search relevance and personalized recommendations for their platform. They plan to use Amazon Bedrock for generative AI model inference and integrate a cross-encoder reranking model to enhance the quality of retrieved documents before passing them to the generative model. The solution must meet the following constraints: \n\n1. Low latency for real-time user queries (under 200ms for the RAG pipeline).\n2. Compliance with GDPR for user data, requiring encryption in transit and at rest.\n3. Minimized operational costs while scaling to handle peak traffic of 50,000 RPS (requests per second).\n4. Flexibility to support multiple generative models from different providers available in Bedrock.\n\nWhich solution BEST meets these requirements?",
      "options": {
        "A": "Deploy an Amazon SageMaker Endpoint for the cross-encoder reranking model with ml.g5.4xlarge instance type and integrate it with Amazon Bedrock using the `InvokeModel` API. Use AWS KMS for data encryption and implement Amazon CloudFront with caching for frequently accessed queries to reduce latency and costs.",
        "B": "Deploy an Amazon SageMaker Endpoint for the cross-encoder reranking model with ml.p4d.24xlarge instance type and integrate it with Amazon Bedrock using the `InvokeModel` API. Use AWS KMS for data encryption but rely on VPC Endpoints for network security without implementing caching to reduce operational complexity.",
        "C": "Deploy the cross-encoder reranking model on Amazon ECS with Fargate Spot tasks and integrate it with Amazon Bedrock using the `InvokeModel` API. Use AWS KMS for data encryption and Amazon CloudFront for caching frequent queries to improve latency and reduce costs.",
        "D": "Deploy the cross-encoder reranking model on an EC2 Auto Scaling group using g4dn.xlarge instances and integrate it with Amazon Bedrock using the `InvokeModel` API. Enable AWS KMS for data encryption and utilize Amazon ElastiCache for caching to optimize costs and improve latency."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because it balances low latency, cost-efficiency, and compliance. The ml.g5.4xlarge instance type is optimized for inference workloads and provides sufficient performance for real-time queries while avoiding the higher costs of p4d.24xlarge instances. Integration with Amazon Bedrock using the `InvokeModel` API ensures flexibility to use multiple generative models. AWS KMS ensures encryption compliance with GDPR, and Amazon CloudFront caching reduces latency and operational costs at high query volumes.\n\nOption B fails because the ml.p4d.24xlarge instance is unnecessarily expensive for this workload, and the lack of caching results in higher latency and costs during peak traffic. \n\nOption C fails because Fargate Spot tasks are not ideal for real-time, low-latency inference due to potential delays in task startup times, which can breach the 200ms latency requirement.\n\nOption D fails because g4dn.xlarge instances may not provide sufficient GPU resources for high-throughput cross-encoder inference at 50,000 RPS. Additionally, while Amazon ElastiCache is a suitable caching solution, its cost and performance trade-offs are less optimal compared to Amazon CloudFront for this use case.",
      "topic": "RAG reranking strategies - cross-encoder integration with Bedrock",
      "difficulty": "hard",
      "category": "architecture",
      "aws_services": [
        "Amazon SageMaker",
        "Amazon Bedrock",
        "AWS KMS",
        "Amazon CloudFront"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:57:42.242436",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 74
    },
    {
      "question": "A healthcare analytics company is building a hybrid AI/ML architecture to process sensitive patient data for clinical predictions. The company uses Amazon Bedrock for foundation model inference and Amazon SageMaker for custom model training and hosting. They must comply with HIPAA regulations, ensure low latency predictions for real-time clinical decision-making, and optimize costs. The company operates in a hybrid environment with on-premises systems managing raw data ingestion and pre-processing, which is then sent to AWS for further processing. The architecture must securely integrate on-premises systems with AWS, and predictions from Bedrock and SageMaker must be unified into a single API endpoint for the application layer. Additionally, the company needs to monitor and enforce fine-grained IAM policies for all AI/ML operations. Which solution BEST meets these requirements?",
      "options": {
        "A": "Use AWS PrivateLink to securely connect on-premises systems to an Amazon VPC. Configure Amazon Bedrock with a custom VPC endpoint and SageMaker endpoints for respective models. Use Amazon API Gateway to unify predictions into a single API endpoint. Enable AWS CloudTrail for auditing and enforce IAM policies using AWS Identity Center.",
        "B": "Use AWS Direct Connect to link on-premises systems to AWS. Configure Amazon Bedrock with a public endpoint and use SageMaker multi-model endpoints for hosting custom models. Use Amazon API Gateway to manage a unified API endpoint. Enable AWS Config for compliance monitoring and enforce IAM policies using Identity and Access Management (IAM) roles.",
        "C": "Use AWS Transit Gateway to connect on-premises systems with AWS. Deploy Amazon Bedrock within a secure Amazon VPC and configure SageMaker hosting endpoints. Use AWS App Runner to unify predictions into a single API endpoint. Enable AWS CloudTrail for auditing and configure IAM permissions using AWS Organizations SCPs.",
        "D": "Use AWS Site-to-Site VPN for connecting on-premises systems with AWS. Configure Amazon Bedrock with a custom VPC endpoint and SageMaker endpoints for respective models. Use Amazon Elastic Load Balancing (ALB) to unify predictions into a single API endpoint. Enable AWS Config for compliance monitoring and enforce IAM policies using AWS Identity Center."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because it uses AWS PrivateLink, which provides secure, private connectivity to AWS services and supports HIPAA compliance. Bedrock and SageMaker endpoints are configured within the same VPC, ensuring low-latency predictions and compliance. Amazon API Gateway unifies predictions into a single API endpoint, and AWS CloudTrail provides auditing capabilities while IAM policies are enforced through AWS Identity Center.\n\nOption B is incorrect because using public endpoints for Amazon Bedrock violates the security requirements for HIPAA compliance. Additionally, AWS Config is not the optimal choice for auditing AI/ML operations, and IAM roles alone do not provide fine-grained policy enforcement.\n\nOption C is incorrect because AWS Transit Gateway introduces unnecessary complexity and cost for this use case. AWS App Runner is not suitable for unifying predictions into a single API endpoint, as it is designed for running containerized applications rather than managing APIs.\n\nOption D is incorrect because AWS Site-to-Site VPN does not offer the same level of scalability or performance as AWS PrivateLink or Direct Connect. Using Elastic Load Balancing (ALB) for unifying predictions is not ideal for this architecture, as ALB is typically used for load balancing traffic rather than API management.",
      "topic": "Hybrid Bedrock-SageMaker architectures",
      "difficulty": "hard",
      "category": "architecture",
      "aws_services": [
        "Amazon Bedrock",
        "Amazon SageMaker",
        "AWS PrivateLink",
        "Amazon API Gateway",
        "AWS CloudTrail",
        "AWS Identity Center"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/sagemaker/latest/dg/what-is.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:58:06.828341",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 75
    },
    {
      "question": "A financial services company needs to implement OpenSearch Serverless to store and query sensitive customer financial data. Due to regulatory compliance (e.g., PCI DSS), all data must be encrypted both at rest and in transit. The company also requires fine-grained access controls to ensure that only specific IAM roles can perform read or write operations on certain collections. Additionally, they want to minimize costs while maintaining high query performance for real-time analytics. Which solution BEST meets these requirements?",
      "options": {
        "A": "Use OpenSearch Serverless with AWS KMS-managed encryption for data at rest and enable HTTPS for encryption in transit. Configure resource-based access policies with granular permissions to specify IAM roles for access to collections. Optimize costs by choosing collections with auto-scaling enabled for high query performance.",
        "B": "Use OpenSearch Serverless with AWS KMS-managed encryption for data at rest and enable TLS 1.2 for encryption in transit. Configure fine-grained access control using OpenSearch RBAC (role-based access control), assigning permissions directly to user groups. Optimize costs by enabling read replicas for collections to improve performance.",
        "C": "Use OpenSearch Serverless with server-side encryption using AES-256 for data at rest and enable HTTPS for encryption in transit. Configure resource-based access policies at the domain level to specify IAM roles for access. Optimize costs by manually provisioning throughput capacity for collections based on expected workloads.",
        "D": "Use OpenSearch Serverless with AWS KMS-managed encryption for data at rest and enable HTTPS for encryption in transit. Configure fine-grained access control using OpenSearch RBAC (role-based access control), assigning permissions to individual users. Optimize costs by configuring collections with fixed capacity to reduce auto-scaling costs."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because AWS KMS-managed encryption satisfies PCI DSS requirements for encryption at rest, and HTTPS ensures encryption in transit. Resource-based access policies are the recommended way to configure granular permissions for IAM roles in OpenSearch Serverless. Auto-scaling collections balance cost-efficiency with performance for real-time analytics. Option B is incorrect because OpenSearch RBAC is not supported in OpenSearch Serverless; access policies should be used instead. Option C is incorrect because AES-256 encryption is not an option for OpenSearch Serverless; KMS-managed encryption must be used. Additionally, manual provisioning of throughput capacity might not optimize costs effectively compared to auto-scaling. Option D is incorrect because assigning permissions to individual users via RBAC is not supported in OpenSearch Serverless, and fixed capacity could lead to performance bottlenecks in real-time analytics scenarios.",
      "topic": "OpenSearch Serverless security - encryption and access policies",
      "difficulty": "medium",
      "category": "opensearch",
      "aws_services": [
        "OpenSearch Serverless",
        "AWS KMS"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/opensearch-service/latest/developerguide/security.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:58:24.782945",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 76
    },
    {
      "question": "A global e-commerce company is designing a search solution for its product catalog. The catalog data exceeds 50 TB and includes structured and unstructured data. The company requires millisecond-scale query responses during peak traffic, which can exceed 500,000 requests per second. They must ensure compliance with GDPR and HIPAA regulations, including encryption at rest and fine-grained access control. Budget constraints require cost optimization, but the solution must scale elastically to handle unpredictable traffic spikes and maintain high availability across multiple AWS regions. Which solution BEST meets these requirements?",
      "options": {
        "A": "Use OpenSearch Serverless with three collections configured for different workloads (catalog search, analytics, and autocomplete). Encrypt data at rest using AWS KMS and configure fine-grained IAM roles for access control. Deploy collections across multiple regions using a multi-region active-active architecture.",
        "B": "Use a provisioned OpenSearch domain with UltraWarm nodes for cost-effective storage of historical catalog data and enabled zone awareness across two Availability Zones. Configure encryption at rest using AWS KMS and VPC endpoint access for additional security compliance. Scale the domain horizontally during traffic peaks.",
        "C": "Use OpenSearch Serverless with a single collection for all workloads, configured with auto-scaling enabled. Utilize AWS KMS for encryption at rest and enable fine-grained access control via resource-based policies. Deploy in a single primary region with cross-region replication enabled for disaster recovery.",
        "D": "Use a provisioned OpenSearch domain with Cold Storage for archived catalog data and index scaling adjusted based on traffic patterns. Encrypt data using AWS KMS and enforce compliance via service-linked S3 bucket policies. Enable cross-region replication for high availability."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because OpenSearch Serverless provides the required elasticity for unpredictable traffic spikes while eliminating the need for manual provisioning. The multi-collection design separates workloads, optimizing query performance for different use cases. Active-active multi-region architecture ensures high availability. AWS KMS encryption and IAM role-based access control meet GDPR and HIPAA requirements. Option B fails because provisioned domains require manual scaling and do not provide the same elasticity as Serverless, making it less ideal for unpredictable traffic patterns. Option C fails as it uses a single collection for all workloads, which can lead to performance bottlenecks, and disaster recovery is less robust compared to active-active multi-region deployment. Option D fails as Cold Storage is optimized for infrequent access and archival, making it unsuitable for millisecond-scale query responses required for the product catalog search.",
      "topic": "OpenSearch Serverless vs provisioned domains - when to use each",
      "difficulty": "hard",
      "category": "opensearch",
      "aws_services": [
        "Amazon OpenSearch Serverless",
        "Amazon OpenSearch Service"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/opensearch-service/latest/developerguide/serverless.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:58:43.861116",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 77
    },
    {
      "question": "A rapidly growing e-commerce company is building a product recommendation engine using OpenSearch to power personalized search capabilities. The engine relies on vector embeddings for user preferences, processed using a machine learning model. They estimate handling 2 million unique vectors with a minimum search latency of 50ms for queries and require high availability with automatic scaling for peak traffic during promotional campaigns. The company operates under strict compliance requirements, including data residency within the EU and encryption at rest. Budget constraints demand cost optimization, but performance must remain uncompromised during peak loads. Which solution BEST meets these requirements?",
      "options": {
        "A": "Deploy OpenSearch Service with UltraWarm storage for vector embeddings and configure a multi-AZ domain in the EU region. Use the HNSW algorithm for approximate nearest neighbor (ANN) search and cosinesimil distance metric. Enable node-to-node encryption and ensure encryption at rest using AWS KMS with a CMK in the EU region. Provision 50 OCUs for the vector workload based on peak traffic estimates.",
        "B": "Deploy OpenSearch Serverless with UltraWarm storage for vector embeddings and configure a single-AZ domain in the EU region. Use the HNSW algorithm for approximate nearest neighbor (ANN) search and cosinesimil distance metric. Enable node-to-node encryption and ensure encryption at rest using AWS KMS with a CMK in the EU region. Provision 50 OCUs for the vector workload based on peak traffic estimates.",
        "C": "Deploy OpenSearch Service with UltraWarm storage for vector embeddings and configure a multi-AZ domain in the EU region. Use the IVF algorithm for approximate nearest neighbor (ANN) search and cosinesimil distance metric. Enable node-to-node encryption and ensure encryption at rest using AWS KMS with a CMK in the EU region. Provision 50 OCUs for the vector workload based on peak traffic estimates.",
        "D": "Deploy OpenSearch Service with UltraWarm storage for vector embeddings and configure a multi-AZ domain in the EU region. Use the HNSW algorithm for approximate nearest neighbor (ANN) search and l2 distance metric. Enable node-to-node encryption and ensure encryption at rest using AWS KMS with a CMK in the EU region. Provision 50 OCUs for the vector workload based on peak traffic estimates."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because it uses OpenSearch Service with UltraWarm storage for cost optimization, a multi-AZ deployment for high availability, and the HNSW algorithm with cosinesimil distance metric, which is ideal for vector workloads in recommendation systems. It also satisfies compliance requirements with EU data residency and encryption at rest using AWS KMS. Option B fails because OpenSearch Serverless does not currently support UltraWarm storage, leading to higher storage costs for large-scale vector embeddings. Option C fails because the IVF algorithm is less performant for high-dimensional vector workloads compared to HNSW. Option D fails because the l2 distance metric is less suitable for similarity-based recommendation systems compared to cosinesimil, leading to suboptimal results.",
      "topic": "OpenSearch OCU capacity planning for vector workloads",
      "difficulty": "hard",
      "category": "opensearch",
      "aws_services": [
        "Amazon OpenSearch Service",
        "AWS KMS"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/opensearch-service/latest/developerguide/sizing.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:59:01.353645",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [
          "The question assumes UltraWarm storage is relevant for vector embeddings, but UltraWarm is typically used for infrequently accessed data, such as logs, and may not be optimal for high-performance vector search workloads. However, this does not invalidate the solution since the focus is on cost optimization.",
          "The explanation could clarify that OpenSearch Serverless does not currently support UltraWarm storage, which is why Option B is invalid."
        ],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 78
    },
    {
      "question": "A global e-commerce company is building a personalized product recommendation system. They need to integrate Amazon Bedrock's generative AI capabilities with an OpenSearch cluster for real-time vector-based search. The OpenSearch cluster will store product embeddings generated by Bedrock's foundation models, enabling similarity searches for recommendations. The company requires the solution to handle at least 100,000 search queries per second globally with low latency (<50ms), comply with GDPR data residency rules by keeping European customer data in the EU, and ensure encryption in transit and at rest. Cost optimization is a priority, but it must not compromise compliance or performance. Which solution BEST meets these requirements?",
      "options": {
        "A": "Deploy an OpenSearch Service domain with UltraWarm nodes for cost-efficient storage and enable k-NN vector search using the HNSW algorithm with a `cosinesimil` distance metric. Configure the OpenSearch domain to use VPC endpoints for secure communication. Use AWS Bedrock in the same AWS Region to generate embeddings and store them in OpenSearch, ensuring that data from European customers is processed and stored in an EU AWS Region. Enable fine-grained access control and encrypt data using AWS KMS.",
        "B": "Deploy an OpenSearch Service domain with UltraWarm nodes to reduce costs and enable k-NN vector search using the IVF algorithm with a `cosinesimil` distance metric. Use AWS Bedrock to generate embeddings and send them to OpenSearch over a public endpoint. Ensure data residency compliance by restricting data processing and storage to EU Regions for European users. Encrypt all data using AWS KMS and enforce fine-grained access control.",
        "C": "Deploy an OpenSearch Serverless domain to minimize operational costs and enable k-NN vector search using the HNSW algorithm with an `l2` distance metric. Connect OpenSearch to AWS Bedrock using a Lambda function to ingest embeddings. Use a multi-region setup with replication to handle GDPR compliance, ensuring all European data is stored in an EU AWS Region. Encrypt data in transit and at rest using AWS KMS.",
        "D": "Deploy an OpenSearch Service domain with UltraWarm nodes to optimize costs and enable k-NN vector search using the HNSW algorithm with a `cosinesimil` distance metric. Use AWS Bedrock to generate embeddings in a centralized US Region and ingest them into OpenSearch. Ensure European data complies with GDPR by encrypting it with AWS KMS and using fine-grained access control to restrict access to authorized users."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because it uses the HNSW algorithm with `cosinesimil` distance, which is optimized for similarity searches in OpenSearch, and complies with GDPR by keeping EU data within an EU AWS Region. It also ensures cost optimization with UltraWarm nodes and secures the data with VPC endpoints, fine-grained access control, and AWS KMS encryption. Option B fails because the IVF algorithm is less performant for high-dimensional vector searches compared to HNSW. Option C fails because the `l2` distance metric is less suitable for similarity searches in this context, and OpenSearch Serverless may not meet the query throughput requirement of 100,000 queries per second. Option D fails because processing embeddings in a centralized US Region violates GDPR data residency requirements for European users, even if encryption is applied.",
      "topic": "OpenSearch integration with Bedrock Knowledge Bases",
      "difficulty": "hard",
      "category": "opensearch",
      "aws_services": [
        "Amazon OpenSearch Service",
        "Amazon Bedrock",
        "AWS KMS"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/opensearch-service/latest/developerguide/knn-vector-search.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:59:28.270418",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 79
    },
    {
      "question": "A retail analytics company needs to implement a high-performance product recommendation engine using OpenSearch Service with the k-NN plugin. The dataset consists of 100 million product vectors, each with 512 dimensions. The solution must prioritize low query latency (<50ms per query) for real-time recommendations while minimizing indexing cost and adhering to strict compliance requirements for data encryption at rest and in transit. The company also expects the system to scale dynamically based on unpredictable traffic spikes during seasonal sales events. Which solution BEST meets these requirements?",
      "options": {
        "A": "Use OpenSearch Service with the k-NN plugin configured to use the HNSW algorithm, `space_type=cosinesimil`, and enable dynamic scaling with UltraWarm nodes for cost-effective storage. Apply AWS Key Management Service (KMS) for encryption while ensuring cluster-level HTTPS for data in transit.",
        "B": "Use OpenSearch Service with the k-NN plugin configured to use the IVF algorithm, `space_type=cosinesimil`, and provision dedicated instances with EBS volumes optimized for high IOPS. Apply AWS Key Management Service (KMS) for encryption while ensuring cluster-level HTTPS for data in transit.",
        "C": "Use OpenSearch Service with the k-NN plugin configured to use the HNSW algorithm, `space_type=l2`, and enable dynamic scaling with UltraWarm nodes for cost-effective storage. Apply AWS Key Management Service (KMS) for encryption while ensuring cluster-level HTTPS for data in transit.",
        "D": "Use OpenSearch Service with the k-NN plugin configured to use the IVF algorithm, `space_type=l2`, and provision dedicated instances with EBS volumes optimized for high IOPS. Apply AWS Key Management Service (KMS) for encryption while ensuring cluster-level HTTPS for data in transit."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because the HNSW algorithm is optimized for low-latency nearest neighbor searches in high-dimensional spaces, making it ideal for real-time recommendations. The `space_type=cosinesimil` is appropriate for similarity-based searches in vector embeddings, and UltraWarm nodes provide cost-effective storage for large datasets. Encryption requirements are met with AWS KMS and HTTPS. Option B fails because the IVF algorithm is not as performant as HNSW for low-latency queries in high-dimensional datasets. Option C fails because the `space_type=l2` is less suitable for similarity-based searches in embeddings compared to `cosinesimil`. Option D fails because both the IVF algorithm and `space_type=l2` are suboptimal for the specified use case and performance requirements.",
      "topic": "OpenSearch k-NN plugin - HNSW vs IVF algorithm selection",
      "difficulty": "hard",
      "category": "opensearch",
      "aws_services": [
        "OpenSearch Service",
        "AWS KMS",
        "UltraWarm"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/opensearch-service/latest/developerguide/knn.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T22:59:44.496603",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 80
    },
    {
      "question": "A financial analytics company needs to implement a search solution for their newly launched risk assessment platform. The platform must allow users to perform two types of queries: (1) keyword-based searches to retrieve documents containing specific terms and (2) similarity-based searches to find documents closely matching user-provided vector embeddings. The system must support high query concurrency during trading hours, comply with data-at-rest encryption requirements, and minimize costs. The company is considering AWS OpenSearch Serverless collections but is unsure how to configure them for optimal performance and compliance. Which solution BEST meets these requirements?",
      "options": {
        "A": "Create two OpenSearch Serverless collections: one with the 'search' collection type for keyword-based searches and another with the 'vector-search' collection type using the HNSW algorithm and cosinesimil distance metric. Enable encryption with AWS KMS for both collections and configure auto-scaling policies to handle peak concurrency.",
        "B": "Create a single OpenSearch Serverless collection with the 'vector-search' collection type using the HNSW algorithm and cosinesimil distance metric. Use the same collection for both keyword-based and vector similarity searches. Enable encryption with AWS KMS and configure auto-scaling policies for peak loads.",
        "C": "Create two OpenSearch Serverless collections: one with the 'search' collection type for keyword-based searches and another with the 'vector-search' collection type using the IVF algorithm and l2 distance metric. Enable encryption with AWS KMS for both collections and configure fixed concurrency limits for cost control.",
        "D": "Create two OpenSearch Serverless collections: one with the 'search' collection type for keyword-based searches and another with the 'vector-search' collection type using the HNSW algorithm and l2 distance metric. Enable encryption with AWS KMS for both collections and configure auto-scaling policies to handle peak concurrency."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because it uses two collections, one optimized for keyword search ('search' collection type) and another for similarity-based searches ('vector-search' collection type) with the appropriate HNSW algorithm and cosinesimil distance metric. This configuration ensures the best performance for both query types while complying with encryption and scaling requirements. \nOption B fails because 'vector-search' collection type cannot efficiently handle keyword-based searches, leading to poor performance for those queries. \nOption C is incorrect because the IVF algorithm and l2 distance metric are suboptimal for the use case, as HNSW and cosinesimil are better suited for high-accuracy vector similarity searches. \nOption D fails because it uses the l2 distance metric, which is less suitable for the similarity-based searches in this scenario compared to the cosinesimil distance metric.",
      "topic": "OpenSearch Serverless collections - Search vs Vector search types",
      "difficulty": "medium",
      "category": "opensearch",
      "aws_services": [
        "OpenSearch Serverless",
        "AWS KMS"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/opensearch-service/latest/developerguide/serverless.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T23:00:06.892942",
      "critique": {
        "overall_score": 9.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [],
        "verification_status": "APPROVED",
        "confidence": 0.95
      },
      "verification_status": "APPROVED",
      "id": 81
    },
    {
      "question": "A media streaming company wants to implement a recommendation engine for its users based on search and vector similarity. The company has a catalog of 5 million video titles, each with metadata (title, genre, description) as well as feature embeddings generated by a machine learning model. Users should be able to search by text (e.g., 'science fiction movies') and receive results ranked by relevance. Additionally, users should get recommendations for 'similar' videos based on the embedding of a selected video. To comply with internal cost constraints, the solution must be serverless and scale automatically during peak traffic. Security is critical, and the solution must ensure encryption in transit for all API calls. Which solution BEST meets these requirements?",
      "options": {
        "A": "Create two OpenSearch Serverless collections: one with the 'search' collection type for text-based searches and another with the 'vector search' collection type for similarity recommendations. Use the CreateCollection API to set the 'collectionType' parameter to 'search' and 'vector-search' respectively, ensuring encryption in transit with HTTPS endpoints.",
        "B": "Create a single OpenSearch Serverless collection of type 'search', and store both metadata and video embeddings in the same index. Use the CreateCollection API with 'collectionType' set to 'search'. Implement vector similarity by leveraging the k-Nearest Neighbors (k-NN) plugin on the existing collection.",
        "C": "Create a single OpenSearch Serverless collection of type 'vector-search' to handle both text-based searches and similarity recommendations. Use the CreateCollection API with 'collectionType' set to 'vector-search' and query the collection using cosinesimil distance for vector similarity and full-text queries for text search.",
        "D": "Create two OpenSearch Serverless collections: one with the 'search' collection type for text-based searches and another with the 'vector search' collection type for similarity recommendations. Use the CreateCollection API to set the collectionType parameter to 'search' and 'vector-search' respectively, but configure the 'vector-search' collection with l2 distance metric for similarity recommendations."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because it follows AWS best practices by creating separate OpenSearch Serverless collections for text-based and vector similarity searches, leveraging the specific capabilities of 'search' and 'vector-search' collection types. The HTTPS endpoint ensures encryption in transit, meeting the security requirement. Option B fails because a single 'search' collection cannot effectively handle vector similarity queries, and the k-NN plugin is not supported in OpenSearch Serverless. Option C fails because the 'vector-search' collection type is not optimized for full-text search, which would lead to suboptimal performance for text queries. Option D fails because the l2 distance metric is not suitable for comparing feature embeddings in recommendation scenarios; cosinesimil is the recommended metric.",
      "topic": "OpenSearch Serverless collections - Search vs Vector search types",
      "difficulty": "medium",
      "category": "opensearch",
      "aws_services": [
        "OpenSearch Serverless"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/opensearch-service/latest/developerguide/serverless.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T23:00:34.380101",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 82
    },
    {
      "question": "A financial analytics company is building a real-time recommendation engine that uses OpenSearch Service to process vector workloads for personalized investment strategies. The workload involves storing and querying millions of 512-dimensional vectors, requiring low latency for real-time responses. The company must minimize costs while maintaining high performance and meeting compliance requirements for GDPR and financial data encryption. Additionally, the solution must scale seamlessly to handle peak traffic during trading hours. Which solution BEST meets these requirements?",
      "options": {
        "A": "Use OpenSearch Service with UltraWarm nodes for historical data and dedicated data nodes with 6 vCPUs and 32 GiB RAM for vector workloads. Configure HNSW indexing with `EF_CONSTRUCTION=512` and `EF_SEARCH=64` for optimal vector search performance. Enable AWS KMS encryption at rest and enforce fine-grained access controls to meet GDPR compliance. Use Auto-Tune to dynamically allocate resources during peak traffic.",
        "B": "Use OpenSearch Service with UltraWarm nodes for historical data and dedicated data nodes with 6 vCPUs and 24 GiB RAM for vector workloads. Configure HNSW indexing with `EF_CONSTRUCTION=256` and `EF_SEARCH=128` for vector search. Enable AWS KMS encryption at rest and enforce fine-grained access controls to meet GDPR compliance. Use Auto-Tune to dynamically allocate resources during peak traffic.",
        "C": "Use OpenSearch Service with UltraWarm nodes for historical data and dedicated data nodes with 4 vCPUs and 16 GiB RAM for vector workloads. Configure IVF indexing with `EF_CONSTRUCTION=512` and `EF_SEARCH=64` for vector search performance. Enable AWS KMS encryption at rest and enforce fine-grained access controls to meet GDPR compliance. Use Auto-Tune to dynamically allocate resources during peak traffic.",
        "D": "Use OpenSearch Service with UltraWarm nodes for historical data and dedicated data nodes with 6 vCPUs and 32 GiB RAM for vector workloads. Configure HNSW indexing with `EF_CONSTRUCTION=512` and `EF_SEARCH=128` for optimal vector search performance. Enable AWS KMS encryption at rest and enforce fine-grained access controls to meet GDPR compliance. Use manual scaling to handle peak traffic."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because it provides the optimal configuration for vector workloads using HNSW indexing parameters (`EF_CONSTRUCTION=512` and `EF_SEARCH=64`) to balance low latency and performance. It uses proper data node sizing (6 vCPUs and 32 GiB RAM) for handling high-dimensional vector data and leverages Auto-Tune for cost-efficient scaling during peak traffic. Option B fails because the lower `EF_CONSTRUCTION` value (256) reduces indexing quality, and smaller RAM (24 GiB) may lead to performance bottlenecks. Option C fails because IVF indexing is less effective for real-time vector search compared to HNSW, and smaller node sizes (4 vCPUs, 16 GiB RAM) would underperform for the workload. Option D fails because manual scaling does not provide the seamless scaling required during peak traffic, which is critical for minimizing latency and cost.",
      "topic": "OpenSearch OCU capacity planning for vector workloads",
      "difficulty": "hard",
      "category": "opensearch",
      "aws_services": [
        "OpenSearch Service",
        "AWS KMS",
        "Auto-Tune"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/opensearch-service/latest/developerguide/sizing.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T23:00:49.865497",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 83
    },
    {
      "question": "A global e-commerce company needs to enhance its product recommendation system by integrating OpenSearch with Amazon Bedrock to leverage generative AI for personalized search results. The company expects to store and query millions of product embeddings using OpenSearchs vector search capabilities while ensuring high performance and low latency. The solution must comply with GDPR and encrypt data at rest. Additionally, the team needs to optimize costs without compromising on scalability, as traffic is expected to peak during seasonal sales. Which solution BEST meets these requirements?",
      "options": {
        "A": "Deploy OpenSearch Service (managed cluster) with UltraWarm nodes for historical data and dedicated master nodes. Use the k-NN plugin with HNSW indexing and cosine similarity to store and query product embeddings. Leverage AWS KMS for encryption at rest and integrate Bedrock via its API to generate recommendations in real-time.",
        "B": "Use OpenSearch Serverless with the k-NN plugin configured for HNSW indexing and cosine similarity to handle vector embeddings. Enable AWS KMS encryption for compliance. Integrate with Bedrock using Lambda for real-time generative AI recommendations. Use auto-scaling to manage seasonal traffic spikes.",
        "C": "Deploy OpenSearch Service (managed cluster) with UltraWarm nodes for cost-efficient query storage and k-NN plugin configured for IVF indexing and euclidean distance. Enable default encryption at rest and integrate with Bedrock using its API for generative AI recommendations.",
        "D": "Use OpenSearch Serverless configured with k-NN plugin for HNSW indexing and l2 distance metric for vector embeddings. Enable AWS KMS encryption for GDPR compliance. Integrate with Bedrock using API Gateway and Lambda to process real-time recommendations during seasonal traffic spikes."
      },
      "correct_answer": "B",
      "explanation": "Option B is correct because OpenSearch Serverless eliminates the need for managing infrastructure, making it cost-effective while providing scalability to handle seasonal traffic spikes. The k-NN plugin configured for HNSW indexing and cosine similarity is optimal for vector search with high accuracy, and AWS KMS ensures compliance with GDPR. Using Lambda to integrate Bedrock ensures real-time processing without over-provisioning resources. Option A is incorrect because UltraWarm nodes are designed for historical data and not suitable for low-latency use cases like real-time vector search. Option C is incorrect because IVF indexing and euclidean distance are suboptimal for high-dimensional vector search compared to HNSW and cosine similarity. Option D is incorrect because l2 distance (euclidean metric) is not the best choice for vector embeddings in recommendation systems, which perform better with cosine similarity.",
      "topic": "OpenSearch integration with Bedrock Knowledge Bases",
      "difficulty": "hard",
      "category": "opensearch",
      "aws_services": [
        "OpenSearch Service",
        "Amazon Bedrock",
        "AWS Lambda",
        "AWS KMS",
        "API Gateway"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/opensearch-service/latest/developerguide/knn-vector-search.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T23:01:11.484822",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 84
    },
    {
      "question": "A financial services company needs to deploy OpenSearch Serverless to analyze customer transaction logs for fraud detection. The data contains sensitive Personally Identifiable Information (PII) and must meet strict compliance requirements under GDPR and PCI DSS. The company mandates encryption at rest and in transit, granular access control for different user roles, and integration with their central AWS Identity and Access Management (IAM) system. They are also cost-conscious and want to avoid overprovisioning resources. Which solution BEST meets these requirements?",
      "options": {
        "A": "Configure OpenSearch Serverless with an encryption policy leveraging AWS KMS for encryption at rest using a customer-managed key. Use IAM role-based policies to restrict access to specific collections and operations. Enable HTTPS for encryption in transit, and verify compliance using AWS Artifact PCI DSS and GDPR documentation.",
        "B": "Use OpenSearch Serverless with AWS-owned KMS keys for encryption at rest, default OpenSearch access policies for user roles, and enable HTTPS for encryption in transit. Ensure compliance by configuring Amazon OpenSearch Service audit logs for tracking access patterns.",
        "C": "Deploy OpenSearch Serverless with a custom KMS-managed key for encryption at rest. Use resource-based access policies specifying IP address whitelisting for access control, and enable TLS encryption using a custom CA certificate for compliance with encryption in transit.",
        "D": "Set up OpenSearch Serverless with AWS KMS using a customer-managed key for encryption at rest. Use fine-grained access control based on OpenSearch native user permissions and enable HTTPS for encryption in transit. Document compliance using AWS Security Hub findings."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because it uses AWS KMS with customer-managed keys, ensuring strong encryption at rest aligned with compliance requirements. IAM role-based policies provide granular access control, and HTTPS ensures encryption in transit. PCI DSS and GDPR compliance is verified using AWS Artifact documentation. Option B fails because AWS-owned KMS keys do not meet the customer's explicit preference for customer-managed encryption keys. Option C fails because resource-based IP whitelisting does not provide granular user or role-based access control, which is a stated requirement. Option D fails because OpenSearch native user permissions are less integrated with the company's central IAM system, making it harder to manage access control efficiently.",
      "topic": "OpenSearch Serverless security - encryption and access policies",
      "difficulty": "medium",
      "category": "opensearch",
      "aws_services": [
        "AWS OpenSearch Serverless",
        "AWS KMS",
        "IAM"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/opensearch-service/latest/developerguide/security.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T23:01:37.572822",
      "critique": {
        "overall_score": 8.5,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [
          "The explanation for Option D could be expanded to clarify why OpenSearch native user permissions are less suitable compared to IAM role-based policies, particularly in the context of central IAM integration."
        ],
        "verification_status": "APPROVED",
        "confidence": 0.95
      },
      "verification_status": "APPROVED",
      "id": 85
    },
    {
      "question": "A financial analytics company needs to implement a search solution for querying and analyzing large volumes of real-time transaction data. The solution must support low-latency queries for end-users across multiple regions, comply with PCI-DSS regulations for processing sensitive financial data, and scale automatically during peak transaction periods. Additionally, the company needs to minimize operational overhead while keeping costs predictable. Which solution BEST meets these requirements?",
      "options": {
        "A": "Use OpenSearch Serverless with fine-grained access control enabled through AWS Identity and Access Management (IAM) and encryption at rest configured with AWS Key Management Service (KMS). Configure resource policies to enforce PCI-DSS compliance, and enable auto-scaling to handle peak transaction loads.",
        "B": "Use provisioned OpenSearch domains with three dedicated master nodes and data nodes spread across three Availability Zones. Enable fine-grained access control via IAM and encryption at rest using KMS, and configure manual scaling to handle peak transaction loads.",
        "C": "Use OpenSearch Serverless with fine-grained access control enabled through IAM and encryption at rest configured with KMS. Configure resource policies to enforce compliance, but use static scaling configurations to control costs during peak loads.",
        "D": "Use provisioned OpenSearch domains with UltraWarm storage tier to optimize costs for historical data. Enable fine-grained access control via IAM and encryption at rest using KMS, and configure auto-scaling for peak transaction loads."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because OpenSearch Serverless is designed for use cases requiring automatic scaling and low operational overhead while ensuring compliance through IAM-based access control and KMS encryption. It also provides predictable pricing compared to manually scaling provisioned domains. Option B fails because while provisioned domains offer more granular control, they require manual scaling, which increases operational overhead and may lead to unpredictable costs during peak loads. Option C fails because static scaling configurations in OpenSearch Serverless contradict the requirement for automatic scaling during peak periods. Option D fails because UltraWarm storage is optimized for infrequent access to historical data, which does not align with the need for low-latency queries on real-time transaction data.",
      "topic": "OpenSearch Serverless vs provisioned domains - when to use each",
      "difficulty": "hard",
      "category": "opensearch",
      "aws_services": [
        "Amazon OpenSearch Service",
        "AWS Identity and Access Management",
        "AWS Key Management Service"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/opensearch-service/latest/developerguide/serverless.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T23:01:50.956016",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 86
    },
    {
      "question": "A healthcare company is using Amazon OpenSearch Serverless to store and query sensitive patient data. They are required to comply with HIPAA regulations, which mandate strict access controls and encryption. The company has deployed OpenSearch Serverless collections in a multi-account architecture, where the data is accessed by applications hosted in both AWS and on-premises environments. The company has the following requirements: 1) Minimize data transfer costs between AWS and on-premises systems, 2) Ensure that only authorized applications can query specific collections, 3) Use fine-grained access controls to restrict access to specific documents within the collections, 4) Comply with HIPAA by encrypting data in transit and at rest. Which solution BEST meets these requirements?",
      "options": {
        "A": "Configure an OpenSearch Serverless data access policy that uses resource-based policies to restrict access to specific collections by IAM roles and AWS accounts. Use Amazon VPC endpoints to securely connect on-premises systems to the OpenSearch Serverless endpoint, and enable TLS for encryption in transit.",
        "B": "Use OpenSearch Serverless network policies to allow access only from VPCs hosting the authorized applications. Define data access policies at the collection level to restrict access to specific documents, and use public endpoints with TLS for encryption in transit.",
        "C": "Implement OpenSearch Serverless data access policies to grant access to specific IAM roles. Use AWS PrivateLink with VPC endpoints for secure connectivity between on-premises systems and OpenSearch Serverless, and enable node-to-node encryption for compliance.",
        "D": "Create OpenSearch Serverless network policies to only allow traffic from specific CIDR blocks. Use data access policies to define collection-level permissions for authorized IAM users, and rely on AWS Direct Connect to minimize data transfer costs between on-premises systems and AWS."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because it uses a combination of OpenSearch Serverless data access policies for fine-grained access control and Amazon VPC endpoints for secure and cost-effective connectivity between AWS and on-premises systems. It also ensures HIPAA compliance by enabling TLS for encryption in transit. Option B fails because public endpoints, even with TLS, are not the most secure or cost-effective option for connecting on-premises systems. Option C fails because node-to-node encryption is irrelevant in this context; the requirement is to secure data in transit between systems, not between OpenSearch nodes. Option D fails because using CIDR blocks in network policies does not provide the fine-grained access control required, and AWS Direct Connect does not inherently enforce encryption for compliance.",
      "topic": "OpenSearch Serverless data access policies vs network policies",
      "difficulty": "hard",
      "category": "opensearch",
      "aws_services": [
        "Amazon OpenSearch Serverless",
        "Amazon VPC",
        "AWS PrivateLink"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/opensearch-service/latest/developerguide/security.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T23:02:08.722489",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 87
    },
    {
      "question": "A global e-commerce company is implementing a product recommendation engine using OpenSearch. They plan to use vector search with embeddings generated by a pre-trained machine learning model. The embeddings represent product features, and the goal is to find the most similar products to a given query embedding. The company has the following requirements:\n\n1. High search accuracy is critical to ensure relevant recommendations.\n2. Compute costs must be minimized, but not at the expense of accuracy.\n3. The vector search must scale to handle 10 million product embeddings with low-latency responses (<50 ms).\n4. The similarity metric should align with their embedding model's output, which normalizes vectors to unit length.\n\nThe company is using OpenSearch 2.9 with the k-Nearest Neighbor (k-NN) plugin. Which solution BEST meets these requirements?",
      "options": {
        "A": "Configure OpenSearch with the k-NN plugin, use the HNSW algorithm, and set the similarity metric to 'cosinesimil'. Ensure the 'ef_search' parameter is tuned to balance latency and recall, and use M5.large.search instances to optimize cost and performance.",
        "B": "Configure OpenSearch with the k-NN plugin, use the HNSW algorithm, and set the similarity metric to 'l2'. Ensure the 'ef_search' parameter is tuned to balance latency and recall, and use M5.large.search instances to optimize cost and performance.",
        "C": "Configure OpenSearch with the k-NN plugin, use the IVF algorithm, and set the similarity metric to 'cosinesimil'. Ensure the 'nlist' parameter is tuned based on the dataset size, and use M5.large.search instances to optimize cost and performance.",
        "D": "Configure OpenSearch with the k-NN plugin, use the HNSW algorithm, and set the similarity metric to 'cosinesimil'. Use general-purpose T3.medium instances to minimize cost and rely on default parameter settings for latency and recall."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because the companys embeddings are normalized to unit length, making the 'cosinesimil' similarity metric the most appropriate for accurate search results. The HNSW algorithm is well-suited for high-accuracy, low-latency vector searches at scale, and tuning the 'ef_search' parameter allows balancing recall and latency while keeping compute costs under control. Using M5.large.search instances ensures sufficient compute and memory for the workload.\n\nOption B is incorrect because the 'l2' similarity metric does not account for the normalization of the embeddings, leading to suboptimal accuracy. Option C fails because the IVF algorithm is better suited for large datasets where approximate search is acceptable, but it sacrifices accuracy compared to HNSW, which is a better fit for the companys high-accuracy requirement. Option D is incorrect because T3.medium instances are not designed for compute-intensive workloads like vector search, and relying on default parameters would likely result in subpar performance and higher latency.",
      "topic": "OpenSearch vector search - similarity metrics selection (L2 vs cosine)",
      "difficulty": "hard",
      "category": "opensearch",
      "aws_services": [
        "Amazon OpenSearch Service"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/opensearch-service/latest/developerguide/knn-vector-search.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T23:02:27.624351",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 88
    },
    {
      "question": "A large e-commerce company is implementing a product recommendation engine using Amazon OpenSearch Service with vector search. They need to store high-dimensional embeddings (512 dimensions) of product features and perform similarity searches to recommend similar products in real-time. The system must handle a query volume of 10,000 queries per second and prioritize low-latency responses under 50ms. Compliance with GDPR requires strict data governance, and they want to avoid managing infrastructure by using OpenSearch Serverless. The company also needs to optimize costs while ensuring the search results provide high relevance for their recommendation algorithm. Which solution BEST meets these requirements?",
      "options": {
        "A": "Use OpenSearch Serverless with the HNSW algorithm and the cosinesimil similarity metric for vector search. Configure the vector field with 'dimension': 512 and set 'engine.kvstore.memory_limit_mb' to optimize memory usage. Use fine-grained IAM policies to enforce data access controls for GDPR compliance.",
        "B": "Use OpenSearch Serverless with the HNSW algorithm and the L2 similarity metric for vector search. Configure the vector field with 'dimension': 512 and enable 'engine.kvstore.memory_limit_mb' for cost optimization. Use fine-grained IAM policies for GDPR-compliant access control.",
        "C": "Use OpenSearch Serverless with the IVF algorithm and the cosinesimil similarity metric for vector search. Configure 'dimension': 512 in the vector field and set 'engine.kvstore.memory_limit_mb' to optimize memory usage. Apply fine-grained IAM policies to meet GDPR compliance requirements.",
        "D": "Use OpenSearch Serverless with the HNSW algorithm and the cosinesimil similarity metric for vector search. Configure the vector field with 'dimension': 512 and use default memory settings for the key-value store. Apply resource-based policies for GDPR-compliant data access."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because the HNSW algorithm is optimized for real-time, low-latency vector searches, which is critical for handling 10,000 queries per second with under 50ms latency. The cosinesimil similarity metric is appropriate for high-dimensional embeddings typically used in recommendation systems since it measures angular distance, which aligns better with how embeddings are structured. Configuring 'engine.kvstore.memory_limit_mb' helps manage costs while ensuring performance. Fine-grained IAM policies provide precise control over data access, aligning with GDPR compliance.\n\nOption B fails because it uses the L2 similarity metric, which is less suitable for high-dimensional embeddings, leading to potentially lower relevance in search results. Option C is incorrect because the IVF algorithm is not supported in OpenSearch Serverless, and it is not optimal for real-time, low-latency search use cases. Option D is partially correct but lacks the explicit optimization of memory settings ('engine.kvstore.memory_limit_mb'), which could lead to higher costs or degraded performance under the given query load.",
      "topic": "OpenSearch vector search - similarity metrics selection (L2 vs cosine)",
      "difficulty": "hard",
      "category": "opensearch",
      "aws_services": [
        "Amazon OpenSearch Service",
        "AWS IAM"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/opensearch-service/latest/developerguide/knn-vector-search.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T23:02:43.559948",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 89
    },
    {
      "question": "A financial analytics company needs to deploy an OpenSearch Serverless collection for real-time log analysis with strict security and compliance requirements. The company must ensure that sensitive data is encrypted at rest and in transit, while also restricting access to specific IAM roles within their AWS account. Additionally, they must adhere to PCI DSS compliance and maintain cost efficiency. The solution must support high query performance without compromising security. Which solution BEST meets these requirements?",
      "options": {
        "A": "Use OpenSearch Serverless with a collection-level encryption policy configured using AWS KMS keys for encryption at rest. Create a resource-based access policy that restricts access to specific IAM roles using condition keys (aws:PrincipalArn). Enable HTTPS for data in transit by enforcing TLS 1.2 in the network settings. Monitor costs by optimizing shard allocation and using cold storage for infrequent data.",
        "B": "Use OpenSearch Serverless with a collection-level encryption policy using default AWS-managed KMS keys for encryption at rest. Configure an access policy that allows all IAM roles within the AWS account. Enable HTTPS for data in transit by enforcing TLS 1.2. Optimize costs by enabling automatic scaling and using warm storage tiers.",
        "C": "Use OpenSearch Serverless with a collection-level encryption policy configured using AWS KMS keys for encryption at rest. Create a resource-based access policy allowing access to all IAM roles and external accounts for flexibility. Enable HTTPS for data in transit by enforcing TLS 1.3. Monitor costs by minimizing shard allocation and using cold storage tiers.",
        "D": "Use OpenSearch Serverless with a collection-level encryption policy using customer-managed KMS keys for encryption at rest. Configure strict IAM-based access policies using aws:SourceIP condition keys to restrict access to specific IP ranges. Enable HTTPS for data in transit by enforcing TLS 1.2. Optimize costs by enabling automatic scaling and restricting query concurrency."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because it uses customer-managed AWS KMS keys for encryption at rest, enforces TLS 1.2 for compliance with PCI DSS, and employs resource-based access policies with specific IAM role restrictions, meeting both security and compliance requirements while optimizing costs. Option B fails because it uses default AWS-managed KMS keys, which do not provide the level of control required for PCI DSS compliance and allows overly permissive access policies. Option C fails because it permits access to external accounts and enforces TLS 1.3, which is not explicitly required under PCI DSS and may introduce compatibility issues. Option D fails because it uses aws:SourceIP condition keys instead of aws:PrincipalArn for IAM role restrictions, making it less precise and potentially less secure in this context.",
      "topic": "OpenSearch Serverless security - encryption and access policies",
      "difficulty": "medium",
      "category": "opensearch",
      "aws_services": [
        "OpenSearch Serverless",
        "AWS KMS",
        "IAM"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/opensearch-service/latest/developerguide/security.html",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T23:02:58.026568",
      "critique": {
        "overall_score": 8.5,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [
          "The explanation for Option D mentions aws:SourceIP condition keys as less precise than aws:PrincipalArn, which is accurate but could be elaborated further to clarify that aws:SourceIP is typically used for network-level restrictions rather than IAM role-based access control."
        ],
        "verification_status": "APPROVED",
        "confidence": 0.95
      },
      "verification_status": "APPROVED",
      "id": 90
    },
    {
      "question": "A global e-commerce company needs to preprocess large volumes of multilingual customer feedback for sentiment analysis and key phrase extraction. The feedback data is stored in Amazon S3, and the company must comply with GDPR requirements to ensure no personally identifiable information (PII) is processed or stored in an unredacted form. The preprocessing workflow must be cost-effective, scalable to handle spikes during seasonal sales, and integrate seamlessly with downstream analytics in Amazon Redshift. Additionally, the company requires a solution that minimizes operational overhead and provides detailed audit logs for compliance purposes. Which solution BEST meets these requirements?",
      "options": {
        "A": "Use Amazon Comprehend with the DetectEntities API to identify PII in the S3 data and enable the 'redact' feature to remove PII. Perform sentiment analysis and key phrase extraction using the BatchDetectSentiment and BatchDetectKeyPhrases APIs. Use AWS Lambda with an EventBridge rule to orchestrate the preprocessing pipeline, and enable AWS CloudTrail for audit logging.",
        "B": "Use Amazon Comprehend with the DetectEntities API to identify PII in the S3 data and enable the 'mask' feature to replace PII with placeholder text. Perform sentiment analysis and key phrase extraction using the BatchDetectSentiment and BatchDetectKeyPhrases APIs. Use Amazon Glue workflows to orchestrate the pipeline, and enable AWS CloudTrail for compliance logging.",
        "C": "Use Amazon Comprehend with the DetectPiiEntities API to identify and redact PII directly from the S3 data. Then, use the BatchDetectSentiment and BatchDetectKeyPhrases APIs for sentiment and key phrase analysis. Orchestrate the pipeline with AWS Step Functions and enable Amazon S3 server access logging for compliance.",
        "D": "Use Amazon Comprehend with the DetectEntities API to identify and redact PII in the S3 data. Perform sentiment analysis and key phrase extraction using the DetectSentiment and DetectKeyPhrases APIs in real-time. Orchestrate the workflow using Amazon SQS and enable AWS Config for audit logging."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because it uses the DetectEntities API with the 'redact' feature to meet GDPR requirements by removing PII, and it uses the batch APIs (BatchDetectSentiment and BatchDetectKeyPhrases) for cost-effective and scalable processing of large datasets. AWS Lambda with EventBridge minimizes operational overhead, and CloudTrail provides detailed compliance audit logs. \nOption B fails because the 'mask' feature replaces PII with placeholder text, which may still violate GDPR requirements if the placeholders can be inferred to contain sensitive information. \nOption C fails because the DetectPiiEntities API is specifically designed for PII detection but does not support direct redaction at the API level; manual handling would be required, increasing operational complexity. \nOption D fails because the DetectEntities API is not optimized for PII detection, and using real-time APIs (DetectSentiment and DetectKeyPhrases) instead of batch processing increases costs and may not handle spikes as efficiently.",
      "topic": "Amazon Comprehend for NLP preprocessing",
      "difficulty": "medium",
      "category": "ai-services",
      "aws_services": [
        "Amazon Comprehend",
        "AWS Lambda",
        "Amazon S3",
        "AWS CloudTrail",
        "EventBridge"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/comprehend/latest/dg/",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T23:03:22.694105",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 91
    },
    {
      "question": "A retail company specializing in online fashion wants to implement an AI-driven search feature for their website. The feature must allow customers to upload images of clothing items and find visually similar products in the store's catalog. Their catalog contains 1 million images stored in S3. The company needs high accuracy for visual similarity matching but also wants to incorporate text-based metadata (e.g., color, brand, material) into the search results for better customer experience. Compliance with GDPR is critical, requiring data residency in the EU region. Cost optimization is important but secondary to performance. Which solution BEST meets these requirements?",
      "options": {
        "A": "Use Amazon Rekognition's DetectLabels API to extract visual features from customer-uploaded images and store metadata in Amazon OpenSearch Service. Implement a multi-modal search using OpenSearch's k-nearest neighbors (k-NN) plugin with HNSW index and cosine similarity metric. Configure both Rekognition and OpenSearch in an EU region.",
        "B": "Use Amazon Rekognition's DetectLabels API to extract visual features and Amazon Personalize for metadata-based recommendations. Configure Personalize to use the customer's uploaded image data and catalog metadata for training. Store both image data and metadata in the EU region for GDPR compliance.",
        "C": "Use Amazon Rekognition's CompareFaces API to identify visually similar items by comparing uploaded images against the catalog. Store text-based metadata in Amazon DynamoDB and combine results programmatically. Ensure all services are deployed in an EU region for GDPR compliance.",
        "D": "Use Amazon SageMaker with a pre-trained multi-modal model to process both image and text metadata in a single pipeline. Store catalog images in S3 and metadata in Amazon RDS. Deploy the SageMaker endpoint in the EU region for compliance and implement a custom API for search functionality."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because Amazon Rekognition's DetectLabels API is specifically designed for extracting visual features, and Amazon OpenSearch Service with its k-NN plugin supports efficient multi-modal searches using HNSW index and cosine similarity metric. Both services can be deployed in the EU region to meet GDPR compliance requirements. Option B fails because Amazon Personalize is optimized for personalized recommendations, not multi-modal search functionality combining visual and text-based features. Option C fails because CompareFaces is not suitable for catalog-wide visual similarity matching; it is designed for face comparison only. Option D fails because SageMaker would require significant effort to fine-tune a multi-modal model, leading to higher costs and complexity compared to Rekognition and OpenSearch's ready-to-use solutions.",
      "topic": "Rekognition vs multi-modal models trade-offs",
      "difficulty": "medium",
      "category": "ai-services",
      "aws_services": [
        "Amazon Rekognition",
        "Amazon OpenSearch Service",
        "Amazon S3"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/rekognition/latest/dg/",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T23:03:39.334637",
      "critique": {
        "overall_score": 8.5,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [
          "The explanation could elaborate on why OpenSearch's k-NN plugin with HNSW index and cosine similarity is particularly suited for this use case, emphasizing its scalability for a catalog of 1 million images.",
          "Option D mentions SageMaker with a pre-trained multi-modal model, but it does not specify which model would be used or its availability in SageMaker. This lack of detail weakens the option but does not make it factually incorrect."
        ],
        "verification_status": "APPROVED",
        "confidence": 0.95
      },
      "verification_status": "APPROVED",
      "id": 92
    },
    {
      "question": "A retail analytics company wants to analyze customer sentiment and behavior in real-time across 500 stores using in-store cameras. They aim to identify emotional responses (e.g., happy, frustrated) and detect customer demographics (e.g., age group, gender) to improve product placement strategies. The solution must integrate with their existing AWS infrastructure and provide a secure API endpoint for querying results. Key constraints include: maintaining compliance with GDPR, keeping inference costs under $5,000/month, and ensuring latency below 300ms for real-time processing. Additionally, the customer wants to explore the possibility of using multi-modal AI models to correlate visual and text data (e.g., in-store signage sentiment analysis) for deeper insights. Which solution BEST meets these requirements?",
      "options": {
        "A": "Use Amazon Rekognition for real-time video analysis with the DetectFaces and DetectModerationLabels APIs for sentiment and demographics detection. Implement AWS Lambda for API integration and set up data residency controls in an EU region for GDPR compliance. Use Amazon Comprehend for signage sentiment analysis via custom entity recognition.",
        "B": "Use Amazon Rekognition for emotion and demographic detection via the RecognizeCelebrities and DetectFaces APIs. Implement AWS Lambda for API integration and utilize Amazon S3 Object Lock for security compliance. Use Amazon SageMaker multi-modal models to perform signage sentiment analysis and visual correlation.",
        "C": "Use Amazon Rekognitions DetectFaces API paired with the DetectText API for extracting both demographics and signage sentiment. Deploy AWS AppSync for real-time API integration. Configure Amazon GuardDuty for security monitoring to ensure GDPR compliance.",
        "D": "Use multi-modal AI models deployed on Amazon SageMaker to analyze both video and signage sentiment data in one unified pipeline. Integrate AWS Step Functions for orchestration and use Amazon Rekognition for supplemental emotion detection. Configure IAM policies for GDPR-compliant access controls."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because it leverages Amazon Rekognitions specialized APIs (DetectFaces and DetectModerationLabels) for real-time video analysis with low latency and cost efficiency. It also ensures GDPR compliance by storing data in an EU region and integrates seamlessly with Amazon Comprehend for signage sentiment analysis. Option B fails because RecognizeCelebrities is not appropriate for general demographics detection, and SageMaker multi-modal models would exceed the $5,000/month budget for real-time inference. Option C fails because DetectText is designed for text extraction, not sentiment analysis, making it unsuitable for the signage use case. Option D fails because while SageMaker multi-modal models provide deeper insights, their cost and complexity exceed the budget and latency constraints, and Rekognition APIs are better optimized for real-time emotion detection.",
      "topic": "Rekognition vs multi-modal models trade-offs",
      "difficulty": "medium",
      "category": "ai-services",
      "aws_services": [
        "Amazon Rekognition",
        "Amazon Comprehend",
        "Amazon SageMaker"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/rekognition/latest/dg/",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T23:04:04.434642",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [
          "The explanation for Option D could be expanded to clarify why SageMaker multi-modal models are not cost-effective for this use case, given the $5,000/month budget constraint.",
          "The explanation for Option B could better emphasize why RecognizeCelebrities is not suitable for general demographic detection, as this may not be immediately clear to all candidates."
        ],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 93
    },
    {
      "question": "A healthcare company needs to transcribe patient consultation recordings into text and process the transcriptions using a fine-tuned Large Language Model (LLM) hosted on Amazon SageMaker. The solution must comply with HIPAA regulations, ensure data is encrypted at rest and in transit, and minimize processing latency to deliver near real-time responses for patient queries. The company also requires cost optimization since the number of recordings processed varies significantly throughout the day. Which solution BEST meets these requirements?",
      "options": {
        "A": "Use Amazon Transcribe with the `Medical Transcription` API, specifying `PHI Content Identification` to tag sensitive data. Store transcriptions in an Amazon S3 bucket with server-side encryption (SSE-S3) enabled. Use an Amazon SageMaker endpoint with Elastic Inference to process the transcriptions using the LLM. Ensure VPC endpoints are configured for S3 and SageMaker to comply with HIPAA requirements.",
        "B": "Use Amazon Transcribe with the `Standard Transcription` API and enable an Amazon KMS key for encryption. Store transcriptions in an Amazon S3 bucket with server-side encryption (SSE-KMS) enabled. Use an Amazon SageMaker endpoint with GPU-based instances to process the transcriptions using the LLM. Enable VPC endpoints for S3 and SageMaker to meet compliance requirements.",
        "C": "Use Amazon Transcribe with the `Medical Transcription` API and specify `Language Filtering` to remove sensitive content. Store transcriptions in an Amazon S3 bucket with server-side encryption (SSE-S3) enabled. Use an Amazon SageMaker endpoint with Elastic Inference to process the transcriptions using the LLM. Configure VPC endpoints for S3 and SageMaker to comply with HIPAA regulations.",
        "D": "Use Amazon Transcribe with the `Standard Transcription` API and enable `PHI Content Identification`. Store transcriptions in an Amazon S3 bucket with server-side encryption (SSE-KMS) enabled. Use an Amazon SageMaker endpoint with GPU-based instances to process the transcriptions using the LLM. Configure VPC endpoints for S3 and SageMaker to comply with HIPAA requirements."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because it uses the `Medical Transcription` API, which is designed for healthcare use cases, and enables `PHI Content Identification` to tag sensitive data, a requirement for HIPAA compliance. Elastic Inference optimizes cost while meeting the performance requirements. Additionally, VPC endpoints ensure data security. Option B fails because `Standard Transcription` does not support PHI identification, which is crucial for HIPAA compliance. Option C fails because `Language Filtering` removes sensitive content rather than tagging it, which may lead to loss of critical PHI data. Option D fails because it uses the `Standard Transcription` API, which is not designed for healthcare-specific scenarios, and GPU-based instances are less cost-efficient compared to Elastic Inference for variable workloads.",
      "topic": "Amazon Transcribe to LLM pipelines",
      "difficulty": "medium",
      "category": "ai-services",
      "aws_services": [
        "Amazon Transcribe",
        "Amazon SageMaker",
        "Amazon S3",
        "Elastic Inference",
        "VPC endpoints"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/transcribe/latest/dg/",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T23:04:24.474724",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 94
    },
    {
      "question": "A multinational financial services company needs to automate the extraction of data from millions of scanned documents, such as invoices and receipts, and generate summaries in natural language to aid compliance reporting. The solution must: 1) Ensure compliance with GDPR by keeping all data within the EU, 2) Minimize costs while maintaining high accuracy, and 3) Support scaling to process bursts of up to 10 million pages per month. The company also requires the ability to fine-tune the summarization model based on their specific document formats using their proprietary data. Which solution BEST meets these requirements?",
      "options": {
        "A": "Use Amazon Textract for document data extraction with asynchronous StartDocumentAnalysis API in the eu-central-1 region, and integrate it with a Bedrock-hosted fine-tuned Falcon-40B model for summarization. Fine-tune the model using the Bedrock Fine-Tuning API and store all data in S3 with default encryption using AWS KMS.",
        "B": "Use Amazon Textract for document data extraction with synchronous AnalyzeDocument API in the eu-west-1 region, and integrate it with a Bedrock-hosted fine-tuned Claude-2 model for summarization. Fine-tune the model using the Bedrock Fine-Tuning API and store all data in S3 with default encryption using AWS KMS.",
        "C": "Use Amazon Textract for document data extraction with asynchronous StartDocumentTextDetection API in the eu-central-1 region, and integrate it with a Bedrock-hosted fine-tuned Falcon-7B model for summarization. Fine-tune the model using SageMaker and store all data in S3 with default encryption using AWS KMS.",
        "D": "Use Amazon Textract for document data extraction with asynchronous StartDocumentAnalysis API in the eu-central-1 region, and integrate it with a Bedrock-hosted fine-tuned Claude-2 model for summarization. Fine-tune the model using SageMaker and store all data in S3 with default encryption using AWS KMS."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because it uses the asynchronous StartDocumentAnalysis API, which is optimized for handling complex document structures like invoices and receipts at scale, and ensures GDPR compliance by keeping operations in the eu-central-1 region. The Falcon-40B model on Bedrock provides high accuracy for summarization and supports fine-tuning using Bedrock's native Fine-Tuning API, which is more cost-efficient and integrated compared to SageMaker for this use case. Option B fails because the synchronous AnalyzeDocument API is less suited for large-scale operations and the selection of eu-west-1 does not align with the GDPR compliance requirement to operate in eu-central-1. Option C incorrectly uses the StartDocumentTextDetection API, which is designed for plain text extraction and not suitable for structured data like invoices. It also selects the less accurate Falcon-7B model and uses SageMaker for fine-tuning, which is unnecessary when Bedrock's Fine-Tuning API is available. Option D fails because while it uses the correct Textract API and region, it uses SageMaker for fine-tuning instead of Bedrock's native Fine-Tuning API, increasing complexity and cost.",
      "topic": "Amazon Textract integration with Bedrock",
      "difficulty": "medium",
      "category": "ai-services",
      "aws_services": [
        "Amazon Textract",
        "Amazon Bedrock",
        "Amazon S3",
        "AWS KMS"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/textract/latest/dg/",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T23:04:53.436907",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [
          "The question assumes Falcon-40B is available on Bedrock and supports fine-tuning, but the provided AWS facts do not list Falcon-40B as a supported model. This could cause confusion for examinees.",
          "The explanation does not explicitly clarify why Claude-2 is not a valid option, even though it is mentioned in the incorrect options. This could lead to ambiguity."
        ],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 95
    },
    {
      "question": "A financial services company needs to process thousands of scanned documents daily, extracting tabular data and performing sentiment analysis on specific textual fields within those documents. Due to strict compliance requirements, they must encrypt all data at rest and in transit using customer-managed keys (CMKs) in AWS Key Management Service (KMS). Additionally, the company wants to use a generative AI model from Amazon Bedrock to summarize the extracted text into concise financial insights. The solution must minimize latency while staying cost-effective, as the document processing workload varies significantly throughout the month. Which solution BEST meets these requirements?",
      "options": {
        "A": "Use Amazon Textract's AnalyzeDocument API to extract text and tables, configure it to use an Amazon S3 bucket with default encryption using a CMK for intermediate storage. Invoke a Bedrock model (like Claude or Titan) via the InvokeModel API using a Lambda function, ensuring that the Lambda function is configured with an AWS KMS CMK for environment variable encryption.",
        "B": "Use Amazon Textract's AnalyzeExpense API to extract text and tables, configure it to use an Amazon S3 bucket with default encryption using an AWS-managed key for intermediate storage. Invoke a Bedrock model (like Claude or Titan) via the InvokeModel API using a Lambda function, ensuring that the Lambda function uses the default encryption key for its environment variables.",
        "C": "Use Amazon Textract's AnalyzeDocument API to extract text and tables, configure it to use an Amazon S3 bucket with default encryption using an AWS-managed key for intermediate storage. Invoke a Bedrock model (like Claude or Titan) via the InvokeModel API using an Amazon SageMaker endpoint, ensuring that the SageMaker endpoint is configured with VPC endpoints for security.",
        "D": "Use Amazon Textract's StartDocumentAnalysis API to extract text and tables, configure it to use an Amazon S3 bucket with default encryption using a CMK for intermediate storage. Invoke a Bedrock model (like Claude or Titan) via the InvokeModel API using a Lambda function, ensuring the Lambda function is configured with an AWS KMS CMK for environment variable encryption."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because it uses the appropriate Textract API (AnalyzeDocument) for extracting both text and tables, ensures compliance by encrypting data at rest in S3 with a CMK, and secures Lambda environment variables using a CMK. Additionally, it uses the Bedrock InvokeModel API to call a generative AI model, meeting the requirement for summarization with low latency. \n\nOption B fails because it uses the AnalyzeExpense API, which is designed for processing receipts and invoices, not general tabular data, and it uses an AWS-managed key instead of a CMK, violating compliance requirements. \n\nOption C fails because it uses an AWS-managed key for S3 encryption, which does not meet the compliance requirements, and unnecessarily adds complexity and cost by leveraging an Amazon SageMaker endpoint instead of directly invoking the Bedrock model. \n\nOption D fails because the StartDocumentAnalysis API is asynchronous and designed for large-scale, asynchronous operations, which can introduce unnecessary latency for this use case. While it uses a CMK for S3 encryption and Lambda environment variables, it does not optimize for the low-latency requirement.",
      "topic": "Amazon Textract integration with Bedrock",
      "difficulty": "medium",
      "category": "ai-services",
      "aws_services": [
        "Amazon Textract",
        "Amazon Bedrock",
        "AWS KMS",
        "Amazon S3",
        "AWS Lambda"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/textract/latest/dg/",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T23:05:12.694473",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 96
    },
    {
      "question": "A financial services company needs to implement a smart search solution for its internal knowledge base containing millions of documents that include PDFs, Word files, and spreadsheets. The company requires semantic search capabilities to allow employees to query using natural language and receive relevant results. Key constraints include: maintaining compliance with SOC 2 standards, minimizing costs while scaling to support up to 50,000 queries per day, and ensuring that the solution integrates with their existing AWS Lambda-based microservices architecture for preprocessing user queries. Additionally, the solution must provide fine-grained access control to ensure sensitive data is restricted based on employee roles. Which solution BEST meets these requirements?",
      "options": {
        "A": "Use Amazon Kendra with a custom data source connector configured via the CreateDataSource API, enable document-level access control using AWS Identity and Access Management (IAM), and integrate with AWS Lambda for preprocessing queries. Choose the Enterprise Edition to meet SOC 2 compliance and scale for 50,000 queries per day.",
        "B": "Use Amazon Kendra with a custom data source connector configured via the CreateDataSource API, enable document-level access control using AWS Secrets Manager for sensitive data, and integrate with AWS Lambda for preprocessing queries. Choose the Standard Edition to reduce costs while scaling for 50,000 queries per day.",
        "C": "Deploy an Elasticsearch cluster with natural language processing capabilities, configure fine-grained access control using AWS Cognito, and preprocess queries using AWS Lambda. Scale the cluster for 50,000 queries per day and ensure compliance by encrypting data at rest using AWS KMS.",
        "D": "Use Amazon Kendra with a custom data source connector configured via the CreateDataSource API, enable document-level access control using a combination of AWS IAM and resource-based policies, and preprocess queries with AWS Lambda. Choose the Developer Edition to minimize costs while scaling for 50,000 queries per day."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because Amazon Kendra's Enterprise Edition meets SOC 2 compliance requirements, supports the necessary query scale, and allows integration with AWS Lambda via its APIs. It also provides document-level access control using IAM, which ensures fine-grained permissions based on employee roles. Option B fails because the Standard Edition does not meet the compliance requirements for SOC 2 standards, and AWS Secrets Manager is not the appropriate tool for document-level access control. Option C fails because Elasticsearch does not natively support semantic search with the same ease as Amazon Kendra, and configuring fine-grained access control via Cognito is less effective for this use case. Option D fails because the Developer Edition is designed for small-scale, non-production environments and cannot handle the required query volume or compliance standards.",
      "topic": "Amazon Kendra vs Knowledge Bases comparison",
      "difficulty": "hard",
      "category": "ai-services",
      "aws_services": [
        "Amazon Kendra",
        "AWS Lambda",
        "IAM",
        "AWS Secrets Manager",
        "Elasticsearch"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/kendra/latest/dg/",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T23:05:28.673678",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 97
    },
    {
      "question": "A retail company wants to implement an AI-driven solution to analyze customer sentiment and behavior in their stores using video footage from existing security cameras. They need to detect and classify emotions, recognize specific gestures, and identify objects (e.g., products, carts) in real-time. The solution must comply with regional data protection regulations, which require that all data processing occurs within their country (no cross-border data transfers). They are budget-conscious and need to minimize operational costs while achieving high accuracy. Which solution BEST meets these requirements?",
      "options": {
        "A": "Use Amazon Rekognition's DetectFaces and DetectLabels APIs for video analysis, combined with AWS Lambda for real-time processing within the same region. Enable encryption at rest for all data stored in Amazon S3 using AWS Key Management Service (KMS). Configure Rekognition to process data exclusively within the local region by specifying the region in the API calls.",
        "B": "Use Amazon Rekognition's DetectFaces API for emotion detection and AWS SageMaker multi-modal models for gesture and object recognition. Deploy SageMaker models locally using SageMaker Edge Manager to ensure compliance with regional restrictions. Store video data in Amazon S3 with server-side encryption using S3-managed keys (SSE-S3).",
        "C": "Use Amazon Rekognition's DetectFaces and DetectModerationLabels APIs for analysis, combined with AWS Lambda for event-driven processing. Store video data in Amazon S3 with default encryption enabled, and configure Rekognition to process data in the nearest region with lower latency.",
        "D": "Use AWS SageMaker multi-modal models pre-trained for object and emotion recognition, deployed to an Amazon EC2 instance within the local region. Store video data in Amazon S3 with encryption enabled using AWS KMS keys. Configure the EC2 instance with enhanced networking (Elastic Network Adapter) for low-latency data processing."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because it uses Amazon Rekognition APIs that are explicitly designed for emotion, gesture, and object recognition, ensuring real-time processing with AWS Lambda and regional compliance by specifying the region in API calls. Video data is securely stored in S3 with KMS encryption, meeting compliance and security requirements. Option B fails because SageMaker multi-modal models, while powerful, require additional customization to achieve the same level of accuracy for emotions and gestures, and server-side encryption with SSE-S3 does not provide the fine-grained control of KMS keys. Option C fails because DetectModerationLabels is not suitable for gesture recognition, and processing data in the nearest region violates the compliance requirement for local-region processing. Option D fails because SageMaker multi-modal models deployed on EC2 require complex setup and tuning for this use case, and while encryption is enabled, EC2-based processing introduces higher operational costs compared to Rekognition APIs and Lambda.",
      "topic": "Rekognition vs multi-modal models trade-offs",
      "difficulty": "medium",
      "category": "ai-services",
      "aws_services": [
        "Amazon Rekognition",
        "AWS Lambda",
        "Amazon S3",
        "AWS Key Management Service (KMS)",
        "AWS SageMaker"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/rekognition/latest/dg/",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T23:05:47.305201",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [
          "The explanation for Option B could be expanded to clarify that SageMaker Edge Manager is designed for edge deployments, but it may not be the most cost-effective or straightforward solution for this use case compared to Rekognition APIs.",
          "The explanation for Option D could better emphasize that deploying SageMaker multi-modal models on EC2 introduces higher operational complexity and costs, which may not align with the budget-conscious requirement."
        ],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 98
    },
    {
      "question": "A financial services company needs to process thousands of scanned loan application documents daily to extract key information such as applicant names, loan amounts, and dates. The extracted data must be used to generate summaries and insights using a generative AI model for internal review. The company has the following constraints: \n\n1. Compliance with strict regulatory requirements, including ensuring that no sensitive customer data leaves their AWS region.\n2. Cost-efficiency, as the company expects high document throughput. \n3. Low latency is critical for processing documents and generating summaries in near real-time.\n4. The generative AI model must be fine-tuned to understand the company's specific terminology (e.g., financial jargon). \n\nWhich solution BEST meets these requirements?",
      "options": {
        "A": "Use Amazon Textract's asynchronous StartDocumentTextDetection API to extract document data and store the results in an Amazon S3 bucket with default encryption. Use Amazon Bedrock to deploy a fine-tuned foundation model in-region, ensuring all data remains within the AWS region for compliance.",
        "B": "Use Amazon Textract's synchronous AnalyzeDocument API with the FORMS feature to extract key-value pairs. Store the results in Amazon S3 with a custom AWS KMS key. Use Amazon Bedrock with a fine-tuned foundation model deployed in-region, and enable real-time inference.",
        "C": "Use Amazon Textract's asynchronous StartDocumentAnalysis API with the TABLES and FORMS features enabled. Store the extracted data in Amazon S3 with default encryption. Use Amazon Bedrock with a pre-trained foundation model deployed in the nearest AWS region for summary generation.",
        "D": "Use Amazon Textract's synchronous DetectDocumentText API to extract text data. Store the output in Amazon DynamoDB for low-latency access. Use Amazon Bedrock with a fine-tuned foundation model deployed in-region, ensuring all sensitive data remains compliant with regional regulations."
      },
      "correct_answer": "B",
      "explanation": "Option B is correct because it uses the AnalyzeDocument API with the FORMS feature to extract structured key-value pairs, which is the most efficient method for extracting relevant fields in loan applications. It also ensures compliance by using an AWS KMS key for encryption and utilizes a fine-tuned Bedrock model deployed in-region for low-latency inference. \n\nOption A fails because the StartDocumentTextDetection API only extracts raw text and does not handle structured data like forms. This would require additional processing, increasing latency and costs. \n\nOption C fails because using a pre-trained model instead of a fine-tuned model may not accurately handle the company's specific financial terminology. Additionally, deploying the model in the nearest region could violate compliance requirements since sensitive data might leave the primary region. \n\nOption D fails because the DetectDocumentText API only provides raw text extraction without structure, which is inadequate for extracting key information such as names or loan amounts. Storing data in DynamoDB is also unnecessary for this use case and adds unnecessary cost and complexity.",
      "topic": "Amazon Textract integration with Bedrock",
      "difficulty": "medium",
      "category": "ai-services",
      "aws_services": [
        "Amazon Textract",
        "Amazon Bedrock",
        "Amazon S3",
        "AWS KMS"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/textract/latest/dg/",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T23:06:13.617135",
      "critique": {
        "overall_score": 8.5,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [
          "The explanation for Option B could be more explicit about why the FORMS feature is particularly suited for extracting structured data like key-value pairs in loan applications.",
          "The explanation for Option D could clarify that while DynamoDB provides low-latency access, it is not necessary for this use case, as S3 is sufficient for storing extracted data."
        ],
        "verification_status": "APPROVED",
        "confidence": 0.95
      },
      "verification_status": "APPROVED",
      "id": 99
    },
    {
      "question": "A global e-commerce company wants to analyze customer reviews to extract key insights such as sentiment, key phrases, and named entities in multiple languages. The company processes over 10 million reviews per month and requires near real-time insights for new reviews as they are submitted. They need to meet the following constraints: \n1. Minimize costs while ensuring high throughput and low latency for processing. \n2. Adhere to strict data residency regulations requiring all data to remain in the same AWS Region. \n3. Ensure that the pipeline can scale automatically during peak traffic periods and provide detailed logging for auditing purposes. \nWhich solution BEST meets these requirements?",
      "options": {
        "A": "Use Amazon Comprehend's real-time APIs (DetectSentiment, DetectEntities, and DetectKeyPhrases) with an Application Load Balancer and auto-scaling Amazon ECS Fargate tasks running a custom Lambda function to orchestrate API calls. Enable CloudWatch Logs for auditing and ensure all resources are deployed in a single AWS Region.",
        "B": "Use Amazon Comprehend asynchronous batch processing APIs to process reviews in batches of up to 25 documents per request. Deploy the solution using AWS Lambda functions triggered by Amazon SQS and store the output in Amazon S3. Enable CloudTrail for auditing and ensure resources are deployed in a single AWS Region.",
        "C": "Use Amazon Comprehend's real-time APIs (DetectSentiment, DetectEntities, and DetectKeyPhrases) with an Amazon API Gateway frontend, Lambda functions for API orchestration, and auto-scaling Amazon EC2 instances for processing. Enable CloudWatch Logs and deploy all resources in a single AWS Region.",
        "D": "Use Amazon Comprehend asynchronous batch processing APIs with Amazon Glue ETL jobs to process reviews and store the output in Amazon S3. Schedule Glue jobs using AWS Step Functions and enable logging to CloudWatch and CloudTrail. Deploy all resources in a single AWS Region."
      },
      "correct_answer": "A",
      "explanation": "Option A is correct because it uses Amazon Comprehend's real-time APIs for near real-time processing, which is required for new reviews. The use of ECS Fargate ensures scalability and cost-effectiveness, while CloudWatch Logs provide detailed auditing. All resources are explicitly deployed in a single AWS Region to meet data residency requirements. \nOption B fails because asynchronous batch APIs introduce significant latency, which is unsuitable for near real-time needs. Additionally, asynchronous APIs process up to 25 documents per request, making it less efficient for high-throughput scenarios. \nOption C fails because Amazon EC2 instances, while scalable, may not be as cost-efficient or fully managed as ECS Fargate for this use case. \nOption D fails because Amazon Glue ETL jobs are not designed for near real-time processing and introduce unnecessary complexity. Asynchronous batch APIs also do not meet the real-time requirement.",
      "topic": "Amazon Comprehend for NLP preprocessing",
      "difficulty": "medium",
      "category": "ai-services",
      "aws_services": [
        "Amazon Comprehend",
        "AWS Lambda",
        "Amazon ECS",
        "CloudWatch"
      ],
      "documentation_reference": "https://docs.aws.amazon.com/comprehend/latest/dg/",
      "generated_by": "gpt-4o",
      "generated_at": "2026-02-03T23:06:32.731976",
      "critique": {
        "overall_score": 8.0,
        "is_correct_answer_valid": true,
        "alternative_could_be_correct": false,
        "factual_errors": [],
        "potential_issues": [],
        "verification_status": "APPROVED",
        "confidence": 0.9
      },
      "verification_status": "APPROVED",
      "id": 100
    }
  ]
}